teval(mean(10^6))
teval<-function(...){
gc()
start<-proc.time()
result<-eval(...)
finish<-proc.time()
return(list(Duration=finish-start,Result=result))
}
#install.packages("parallel")
#install.packages("snow")
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-383\\data\\oct20.RData")
library(parallel)
library(snow)
library(tictoc)
detectCores()
cl <- makeCluster(3, type = "SOCK")
cl
apply(computo[,15:17],2, mean)
clusterApply(cl, computo[,15:17], mean)
parSapply(cl, computo[,15:17], mean)
mclapply(computo[,15:17],FUN=mean,mc.cores = 1)#Windows no soporta >1
teval(apply(computo[,15:17],2, mean))
teval(apply(computo[,15:17],2, mean))
teval(clusterApply(cl, computo[,15:17], mean))
teval(parSapply(cl, computo[,15:17], mean))
teval(mclapply(computo[,15:17],FUN=mean,mc.cores = 1))
#matrix enorme
mm<-matrix(rnorm(10000000*15),nrow = 10000000,ncol=15)
teval(m0<-apply(mm,2,mean))
mm<-as.data.frame(mm)
teval(m0<-apply(mm,2,mean))
teval(m1<-clusterApply(cl, mm, mean))
teval(m2<-parSapply(cl, mm, mean))
View(cl)
cl
#install.packages("parallel")
#install.packages("snow")
library(bigmemory)
mm<-matrix(rnorm(10000000*15),nrow = 10000000,ncol=15)
mm<-as.data.frame(mm)
mm<-as.big.matrix(mm)
?ff::as.data.frame.ffdf()
mclapply(X=list(A,B,C),FUN=mean)
?mclapply(X=list(A,B,C),FUN=mean)
library(parallel)
library(snow)
?mclapply(X=list(A,B,C),FUN=mean)
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
system("java -version")
#Habilitando la librería
library(sparklyr)
#versiones disponibles
spark_available_versions()
#versiones instaladas
spark_installed_versions()
#Habilitando la librería
library(sparklyr)
#versiones instaladas
spark_installed_versions()
#versiones instaladas
spark_installed_versions()
#inicia la sesión
sc <- spark_connect(master = "local", version = "2.4.3")
spark_disconnect(sc)
spark_disconnect(sc)
#configuración
conf <- spark_config()
spark_disconnect(sc)
#configuración
conf <- spark_config()
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
2*0.8
spark_web(sc)
spark_web(sc)
spark_web(sc)
#Habilitando la librería
library(sparklyr)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#Schema
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\importaciones1993.csv",sep=",", nrows = 5)
spec_with_r <- sapply(top_rows, class)
spec_with_r
# esquema
sp_importaciones<-spark_read_csv(sc,
name="importaciones",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\",
columns = spec_with_r)
#importaciones
dimnames(sp_importaciones)
sp_t1<-sp_importaciones %>% count(GESTION) %>% compute("t1")
library(dplyr)
sp_t1<-sp_importaciones %>% count(GESTION) %>% compute("t1")
sp_importaciones %>% count(GESTION) %>% compute("t1")
sp_t1<-sp_importaciones %>% count(GESTION) %>% compute("t1")
sp_t1
aa<-sp_importaciones %>% count(GESTION,DESDEPTO) %>% collect()
aa
View(aa)
sp_importaciones %>% count(GESTION) %>% collect()
aa<-sp_importaciones %>% count(GESTION) %>% collect()
View(aa)
sp_importaciones %>% tally
sp_importaciones %>% tally
ww<-sp_importaciones %>% group_by(GESTION,DESDEPTO,NANDINA) %>% summarise(total=sum(FOB,na.rm=T)) %>% collect()
View(ww)
table(aa$GESTION)
#apagar conexión
spark_disconnect(sc)
#apagar conexión
spark_disconnect(sc)
#apagar conexión
spark_disconnect(sc)
rm(list=ls())
rm(list=ls())
setwd("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\excel")
##
library(readxl)
for(i in 2012:2020){
bd<-read_excel(paste0("Importaciones ",i,".xlsx"))
write.csv(bd,file=paste0("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\importaciones",i,".csv"))
}
rm(list=ls())
#Habilitando la librería
library(sparklyr)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#Schema
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\importaciones1993.csv",sep=",", nrows = 5)
spec_with_r <- sapply(top_rows, class)
spec_with_r
# esquema
sp_importaciones<-spark_read_csv(sc,
name="importaciones",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\",
columns = spec_with_r)
sp_importaciones %>% tally
#importaciones
dimnames(sp_importaciones)
sp_importaciones %>%  dbplot_bar(GESTION)
library(dbplot)
sp_importaciones %>%  dbplot_bar(GESTION)
#apagar conexión
spark_disconnect(sc)
#apagar conexión
spark_disconnect(sc)
spark_disconnect_all()
#apagar conexión
spark_disconnect(sc)
system("java -version")
#Habilitando la librería
library(sparklyr)
#Habilitando la librería
library(help=sparklyr)
#versiones disponibles
spark_available_versions()
#versiones instaladas
spark_installed_versions()
system("java -version")
#versiones instaladas
spark_installed_versions()
#inicia la sesión
sc <- spark_connect(master = "local", version = "2.4.3")
spark_web(sc)
#configuración
conf <- spark_config()
conf
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#config$sparklyr.gateway.port <- 8890
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#instalando spark desde R
?spark_install()
?spark_home_dir()
#versiones instaladas
spark_installed_versions()
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
#config$sparklyr.gateway.port <- 9090
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
config$sparklyr.gateway.port <- 9090
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
spark_disconnect(sc)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
conf$sparklyr.gateway.port <- 9090
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#instalando spark desde R
spark_install("2.3")
spark_disconnect(sc)
spark_disconnect(sc)
#configuración
conf <- spark_config()
conf$`sparklyr.shell.driver-memory` <- "2G"
conf$spark.memory.fraction <- 0.8
#spark_home="C:\\Users\\ALVARO\\AppData\\Local\\spark\\spark-2.4.3-bin-hadoop2.7"
#conf$sparklyr.gateway.port <- 9090
#conección
sc <- spark_connect(master = "local", version = "2.4.3",config = conf)
#Schema
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\importaciones1993.csv",sep=",", nrows = 5)
top_rows
spec_with_r <- sapply(top_rows, class)
spec_with_r
# esquema
sp_importaciones<-spark_read_csv(sc,
name="importaciones",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\csv\\",
columns = spec_with_r)
#spark SQL
library(DBI)
top10 <- dbGetQuery(sc, "Select * from importaciones limit 10")
top10
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-383\\data\\oct20.RData")
object.size(computo)/10^6
oct20<-copy_to(sc,computo)
rm(computo)
sp_covid2<-spark_read_csv(sc,
name="covid2",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bigdata\\200614COVID19MEXICO.csv")
library(dplyr)
sp_importaciones %>% tally
sp_t1<-sp_importaciones %>% count(GESTION) %>% compute("t1")
sp_t1
sp_covid2 %>% count(SEXO,NEUMONIA)
#apagar conexión
spark_disconnect(sc)
#apagar conexión
spark_disconnect(sc)
#apagar conexión
spark_disconnect(sc)
#Habilitando la librería
library(sparklyr)
spark_disconnect(sc)
rm(list=ls())
rm(list=ls())
setwd("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_excel")
dir()
##
library(readxl)
##
library(readxl)
for(i in 1992:1994){
bd<-read_excel(paste0("Importaciones ",i,".xlsx"))
write.csv(bd,file=paste0("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv\\",i,".csv"))
rm(bd)
}
#Habilitando la librería
library(sparklyr)
#inicia la sesión
sc <- spark_connect(master = "local", version = "2.4.3")
spark_web(sc)
rm(i)
#csv/importaciones
sp_importaciones<-spark_read_csv(sc,
name="importaciones",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv")
###########
#esquema
###########
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv\\importaciones1993.csv",sep=",", nrows = 5)
###########
#esquema
###########
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv\\1993.csv",sep=",", nrows = 5)
spec_with_r <- sapply(top_rows, class)
spec_with_r
###########
#esquema
###########
top_rows <- read.csv("C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv\\1993.csv",sep=",", nrows = 5)
spec_with_r <- sapply(top_rows, class)
spec_with_r
spec_with_r[6]
spec_with_r[6]<-"factor"
spec_with_r
sp_importaciones2<-spark_read_csv(sc,
name="importaciones2",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\importaciones\\importaciones_csv",
columns = spec_with_r)
spec_with_r
# memoria
object.size(sp_importaciones)
m1<-data.frame(matrix(rnorm(10^6),ncol=20))
m1<-data.frame(matrix(rnorm(10^7),ncol=20))
object.size(m1)/10^6
object.size(m1)/(10^6)
sp_m1<-copy_to(sc,m1)
rm(m1)
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-383\\data\\oct20.RData")
object.size(computo)/10^6
sp_computo<-copy_to(sc,computo)
rm(computo)
sp_covid<-spark_read_csv(sc,
name="covid",
path = "C:\\Users\\ALVARO\\Desktop\\db_bolivia\\bigdata\\200614COVID19MEXICO.csv")
library(dplyr)
db_drop_table(sc,importaciones2)
db_drop_table(sc,sp_importaciones2)
db_drop_table(sc,importaciones2)
db_drop_table(sc,"importaciones2")
db_drop_table(sc,"m1")
load("C:\\Users\\ALVARO\\Documents\\GitHub\\EST-383\\data\\oct20.RData")
head(computo)
aux<-computo[computo$País=="Bolivia",]
aux2<-computo %>% filter(País=="Bolivia")
spark_disconnect(sc)
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
library(dplyr)
load(url("https://github.com/AlvaroLimber/EST-383/raw/master/data/oct20.RData"))
names(computo)
names(computo)[1]
names(computo)[1]<-"pais"
### Columnas -> variables
#cambio de nombre de las variables
computo<-computo %>% rename(mas=`MAS - IPSP`,cc=CC)
names(computo)
computo<-computo %>% rename(idep=`Número departamento`,imun=5)
names(computo)
# Seleccion de variables
bd1<-computo %>% select(pais,cc,mas)
head(bd1)
head(bd1)
# Crear nuevas variables
bd1<-computo %>% mutate(a=1,s_mas_cc=cc+mas,raiz_mas=sqrt(mas),lcc=log(cc))
head(bd1)
# Selección de variables
bd1<-computo %>% select(pais,cc,mas)
head(bd1)
# Crear nuevas variables
bd1<-bd1 %>% mutate(a=1,s_mas_cc=cc+mas,raiz_mas=sqrt(mas),lcc=log(cc))
head(bd1)
bd1<-bd1 %>% mutate(mas100=mas>100,mean_mas=mas>mean(mas))
head(bd1)
# ordenando las variables
bd1<-bd1 %>% relocate(lcc,after=cc)
head(bd1)
?relocate
# ordenando las variables
bd1<-bd1 %>% relocate(lcc,after=cc)
head(bd1)
# ordenando las variables
bd1<-bd1 %>% relocate(lcc,before=cc)
head(bd1)
# ordenando las variables
bd1<-bd1 %>% relocate(lcc:mas,after=a)
head(bd1)
# ordenando las variables
bd1<-bd1 %>% relocate(lcc:mas,before=a)
head(bd1)
bd1[,c("cc","pais","a")]
##############################
### Filas-> Observaciones
##############################
#filtrado
table(computo$Elección)
pv<-computo %>% filter(Elección=="Presidente y Vicepresidente")
pvbol<-computo %>% filter(Elección=="Presidente y Vicepresidente" & pais=="Bolivia")
##############################
### Resumen estadístico
##############################
pvbol %>% summarise(mean(mas),mean(cc),max(mas),max(cc))
##############################
### Resumen estadístico
##############################
pvbol %>% summarise(media_mas=mean(mas),media_cc=mean(cc),max_mas=max(mas),max_cc=max(cc))
pvbol %>% summarise(pmas=mean(mas>100))
pvbol %>% summarise(p100mas=mean(mas>100),p100cc=mean(cc>100))
pvbol %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100)
names(pvbol)
# group_by
pvbol %>% group_by(Departamento) %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100)
pvbol %>% group_by(Departamento,Provincia) %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100)
pvbol %>% group_by(Departamento,Provincia) %>% count()
pvbol %>% group_by(Departamento,Provincia) %>% tally()
# encadenamiento masivo
pvbol %>% filter(Departamento=="Beni") %>% select(mas,cc) %>% plot()
# encadenamiento masivo
pvbol %>% filter(Departamento=="Beni") %>% select(mas,cc) %>% plot()
pvbol %>% filter(Departamento=="Beni") %>% select(mas,cc) %>% cor()
library(sparklyr)
sc <- spark_connect(master = "local", version = "2.4.3")
spark_web(sc)
# cargar el pvbol
sp_pvbol<-copy_to(sc,pvbol,name="elecciones")
object.size(pvbol)/10^6
rm(pvbol)
# interactuar con la base en spark
sp_pvbol %>% count()
sp_pvbol %>% group_by(Departamento) %>% count()
t1<-sp_pvbol %>% group_by(Departamento) %>% count()
t1
View(t1)
View(t1)
t1<-sp_pvbol %>% group_by(Departamento) %>% count()
t1<-sp_pvbol %>% group_by(Departamento) %>% count() %>%  collect()
t1
t1<-sp_pvbol %>% group_by(Departamento) %>% count()
t1$n
barplot(t1$n)
t1<-sp_pvbol %>% group_by(Departamento) %>% count() %>%  collect()
barplot(t1$n)
t0<-sp_pvbol %>% group_by(Departamento) %>% count()
#collect es para guardar la salida en la memoria de R
t1<-sp_pvbol %>% group_by(Departamento) %>% count() %>%  collect()
t0<-sp_pvbol %>% group_by(Departamento) %>% count()
t0
# compute es para guardar la salida en la memoria de spark
sp_t2<-sp_pvbol %>% group_by(Departamento) %>% count() %>% compute("t2")
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100)
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100)
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(p100mas=mean(mas>100)*100,p100cc=mean(cc>100)*100,na.rm=T)
sp_pvbol %>% group_by(Departamento,Provincia)
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(mean(mas))
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas))
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc))
sp_pvbol %>% group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(mas>100))
sp_pvbol %>% mutate(aux=mas>100) %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(aux))
sp_pvbol %>% mutate(aux=mas>100) %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(aux,na.rm=T))
sp_pvbol %>% mutate(aux=100) %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(aux))
sp_pvbol %>% mutate(aux=mas>100) %>% filter(is.na(aux)==F) %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(aux))
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(mas>100))
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(mas>100),na.rm=T)
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),p100mas=mean(mas>100,na.rm=T))
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc))
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),sd(mas))
fx<-function(x){
y<-sum(x)+3
return(y)
}
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),sd(mas),fx(mas))
sp_pvbol %>%  group_by(Departamento,Provincia) %>% summarise(media_mas=mean(mas),media_cc=mean(cc),sd(mas))
# SQL
sp_pvbol %>% group_by(Departamento) %>% count()
# SQL show_query
sp_pvbol %>% group_by(Departamento) %>% count() %>% show_query()
sp_pvbol %>% group_by(Departamento) %>% select(mas,cc) %>% summarise_all(mean)
sp_pvbol %>% group_by(Departamento) %>% select(mas,cc) %>% summarise_all(mean) %>% show_query()
# spark SQL
library(DBI)
dbGetQuery(sc,"Select * from elecciones limit 10")
e10<-dbGetQuery(sc,"Select * from elecciones limit 10")
# figuras
sp_pvbol %>% select(cc) %>% hist()
library(ggplot2)
ggplot(sp_pvbol,aes(cc))+geom_histogram()
ggplot(sp_pvbol,aes(cc,mas))+geom_point()
summarise(mean(mas,na.rm=T))
bookdown::clean_book(TRUE)
bookdown::render_book("index.Rmd", "bookdown::gitbook")
