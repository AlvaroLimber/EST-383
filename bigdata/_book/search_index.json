[
["index.html", "Big Data en R EST-383 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Big Data en R EST-383 Alvaro Chirino Gutierrez 2020-12-06 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Programación Estadística I, una materia optativa del pregrado de la carrera de Estadística de la Universidad Mayor de San Andres. Este documento representa un primer acercamiento a los estudiantes de estadistica al software R y al mundo del Big Data. Estructura del libro El libro inluye 5 capitulos, estos son: Introducción a R Scraping Web en R Introducción al Big Data Big Data en R R y Spark Software y acuerdos sessionInfo() ## R version 4.0.3 (2020-10-10) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19041) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 ## [2] LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods ## [7] base ## ## other attached packages: ## [1] dplyr_1.0.2 ggplot2_3.3.2 rvest_0.3.6 xml2_1.3.2 ## ## loaded via a namespace (and not attached): ## [1] highr_0.8 compiler_4.0.3 pillar_1.4.7 ## [4] tools_4.0.3 digest_0.6.27 packrat_0.5.0 ## [7] evaluate_0.14 lifecycle_0.2.0 tibble_3.0.4 ## [10] gtable_0.3.0 pkgconfig_2.0.3 rlang_0.4.9 ## [13] cli_2.2.0 rstudioapi_0.11 curl_4.3 ## [16] yaml_2.2.1 xfun_0.19 withr_2.2.0 ## [19] httr_1.4.2 stringr_1.4.0 knitr_1.30 ## [22] generics_0.0.2 vctrs_0.3.5 tidyselect_1.1.0 ## [25] grid_4.0.3 glue_1.4.2 R6_2.5.0 ## [28] fansi_0.4.1 rmarkdown_2.5 bookdown_0.18 ## [31] farver_2.0.3 purrr_0.3.4 selectr_0.4-2 ## [34] magrittr_2.0.1 scales_1.1.1 htmltools_0.5.0 ## [37] ellipsis_0.3.1 assertthat_0.2.1 xtable_1.8-4 ## [40] colorspace_1.4-1 labeling_0.3 utf8_1.1.4 ## [43] stringi_1.5.3 munsell_0.5.0 crayon_1.3.4 Bases de datos En este documento se emplearan 4 bases de datos del contecto Boliviano: Encuesta a Hogares 2019 y 2019. Vivienda y Personas Encuesta de Demografía y Salud 1989 - 2008 Encuesta de Niños, niñas y adolescentes 2016 Computo oficial de las elecciones del 20 de Octubre de 2019 Bases de datos de contagios, muertes y recuperados del COVID-19 del Johns Hopkins Institute. Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento Eponine… "],
["introR.html", "Chapter 1 Introducción a R 1.1 ¿Qué es R? 1.2 R básico 1.3 Tipos de estructuras 1.4 Loops y condiciones 1.5 Funciones 1.6 Importacion de datos 1.7 Dataframe y exploración 1.8 Estadística descriptiva 1.9 Muestreo e inferencia 1.10 Gráficos de origen 1.11 ggplot 1.12 R Markdown 1.13 Shiny 1.14 Ejercicios Propuestos", " Chapter 1 Introducción a R R es un software de libre distribución 1.1 ¿Qué es R? Comparación perfecta ### Algo de historia de R R es el hermano de S S es un lenguaje de programación estadística desarrollado por John Chambers de Bell Labs El objetivo de S era “convertir las ideas en el software, de forma rápida y fielmente” S fue creado en 1976 y se reinvento 1988 introduciendo muchos cambios En 1993, StatSci (fabricante de S-Plus) adquieren licencia exclusiva a S S-Plus integra S con una interfaz gráfica de usuario agradable y pleno apoyo al cliente R Fue creado por Ross Ihaka y Robert Gentleman de la University of Auckland, New Zealand 1.1.1 Acerca de R El proyecto R inicio en 1991 R apareció por primera vez en 1996 como un software de código abierto! Altamente personalizable a través de paquetes La comunidad R, se basa en el poder de la colaboración con miles de paquetes de libre disposición Existen muchas interfaces gráficas de usuario de R libres y comerciales (por ejemplo R Studio y Revolución) 1.1.2 ¿Qué es R? R es un conjunto integrado de servicios de software para la manipulación de datos, cálculo y representación gráfica. Incluye: instalación sencilla y un fácil almacenamiento de datos un conjunto de operadores para los cálculos en arrays, particularmente en las matrices facilidad en los gráficos y el análisis de datos y bien desarrollado, lenguaje de programación sencillo y eficaz que incluye condicionales, bucles, funciones recursivas definidos por el usuario. Altamente intuitivo A pesar de ser libre y de código abierto, R es ampliamente utilizado por los analistas de datos dentro de las empresas y el mundo académico. (R en the NY Times) Ver NY Times artículo. 1.1.3 Algunas referencias aRrgh: a newcomer’s (angry) guide to R by Tim Smith and Kevin Ushey Introductory Statistics with by Peter Dalgaard R tarjeta de comandos http://cran.r-project.org/doc/contrib/Short-refcard.pdf Tutorial de R http://www.cyclismo.org/tutorial/R/ R project and Bioconductor Mas avanzado: Hadley Wickham’s book 1.1.4 RStudio RStudio es un ambiente libre y abierto de desarrollo de código integrado. multiplataforma El resaltado de sintaxis, completado de código, y la sangría inteligente gestionar fácilmente múltiples directorios de trabajo Flexible para el manejo de gráficos Integrado con Knitr Integrado con Git 1.1.5 Instalación R-CRAN https://cran.r-project.org/ (elija el Sistema operativo, descargue y siguiente, siguiente…) R-Studio https://www.rstudio.com/ (elija el Sistema operativo, descargue y siguiente, siguiente…) Nota: Para actualizar ambos paquetes: descargue la nueva versión e instale (las librerías no sufren cambios). 1.2 R básico R es una calculadora demasiado grande 5*8 ## [1] 40 89+90 ## [1] 179 67/6 ## [1] 11.16667 45^7 ## [1] 373669453125 1.2.1 Lógica de los comandos en R Como entiende R los comandos comando(argumentos, argumentos, …) Advertencia: No es posible resumir un comando R distingue mayúscula de minúscula Siempre cerrar los paréntesis R entiende el orden de los argumentos o su nombre clave exp(90) # exponencial ## [1] 1.220403e+39 exp(0) ## [1] 1 exp(1) ## [1] 2.718282 log(10) # logarítmo ## [1] 2.302585 sqrt(25) # raíz cuadrada ## [1] 5 sqrt(x=25) ## [1] 5 Comando para pedir ayuda help(pi) # equivalent ?pi ?sqrt ?sin ?Special ?sqrt ?log Escribir varios comandos en una sola línea. log(20) ; exp(0) ; 5*6 ; 10^10 ## [1] 2.995732 ## [1] 1 ## [1] 30 ## [1] 1e+10 1.2.2 Palabras reservadas y simbolos especiales de R NA: datos perdidos NULL: datos nulos Inf -Inf: Infinito #: comentario TRUE (T), FALSE (F): valores lógicos NaN: not a number ?: Ayuda x, ,x + y, x - y ,x * y ,x / y ,x ^ y (**),x %% y (mod) ,x %/% y (div int) ! x, .x &amp; y ,x &amp;&amp; y ,x | y ,x || y 1.2.3 Símbolos Lógicos 5 == 6 # igualdad ## [1] FALSE 5^2 == 25 ## [1] TRUE 5 != 6 # desigualdad ## [1] TRUE 25 != 25 ## [1] FALSE !(5&gt;10) # negación ## [1] TRUE (5==5) &amp; (5&gt;6) # Y lógico ## [1] FALSE (5==5) &amp; (5&lt;6) # Y lógico ## [1] TRUE (5==5) | (5&gt;6) # ó lógico ## [1] TRUE (5==5) | (5&lt;6) # ó lógico ## [1] TRUE (5!=5) | (5&gt;6) # ó lógico ## [1] FALSE 1.2.4 Asignación o creación de objetos (estructuras, variables) x &lt;- 50 x1 = 4 4 -&gt; x2 xxxxcsdvsdvsdcs&lt;-4 x*(x1^x2) ## [1] 12800 Trate de usar nombres significativos! Miren esto: Hadley Wickham’s book Naming 1.2.5 Clases en R y1&lt;-50 y2&lt;-&quot;hola&quot; y3&lt;-(56&gt;60) y4&lt;-NA y5&lt;-NULL class(y1) ## [1] &quot;numeric&quot; class(y2) ## [1] &quot;character&quot; class(y3) ## [1] &quot;logical&quot; class(y4) ## [1] &quot;logical&quot; class(y5) ## [1] &quot;NULL&quot; 1.3 Tipos de estructuras 1.3.1 Homogéneas Scalar: Valores simples y1&lt;-50 y2&lt;-&quot;hola&quot; y3&lt;-(56&gt;60) y4&lt;-NA y5&lt;-NULL Vectores: Colección de valores simples. Los vectores en R son vectores columna v1 &lt;- c(4,2,6,7,12) v2 &lt;- c(&quot;hola&quot;,&quot;cómo&quot;,&quot;estás&quot;,&quot;adios&quot;) v3&lt;- c(4,2,6,7,12,&quot;hola&quot;) v4&lt;-(v1 &gt; 5) v5&lt;-c(-9,pi,3/7) Matrices Arrays 1.3.2 Heterogéneas Dataframes Listas rm(list=ls()) v&lt;-1:10 v2&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;2&quot;) as.character(v) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; as.numeric(v) ## [1] 1 2 3 4 5 6 7 8 9 10 x1&lt;-1 0/0 ## [1] NaN x2&lt;-NULL x2 ## NULL str(x1) ## num 1 typeof(x1) ## [1] &quot;double&quot; typeof(v2) ## [1] &quot;character&quot; # vectores con usos estadisticos rbinom(10,5,0.1) ## [1] 2 0 1 0 0 0 1 2 0 0 rnorm(10,5,10) ## [1] 5.5533725 11.7961284 -5.2427246 -3.2860607 -14.1599973 ## [6] -0.2493029 30.8414322 -21.6852936 -18.2778136 -4.9446358 runif(10,50,500) ## [1] 383.99544 459.28326 91.18398 470.53636 55.80825 304.97136 ## [7] 445.49476 189.94728 239.46643 243.76843 rchisq(10,2) ## [1] 0.7645659 1.0735339 2.8546968 1.4218133 0.3718119 1.5897724 ## [7] 0.1869642 1.4315652 1.2502356 2.4405328 rexp(10,5) ## [1] 0.35514471 0.02569001 0.23475282 0.06561624 0.26702076 ## [6] 0.36974346 0.05993730 0.20696524 0.52803083 0.02321273 x&lt;-rchisq(1000,3) hist(x) (sum((x-mean(x))**4)/1000)/sd(x)**4 ## [1] 7.231891 median(rexp(100000,3)) ## [1] 0.2315083 quantile(x,c(2,56)/100) ## 2% 56% ## 0.1943932 2.6728154 quantile(x,1:100/100) ## 1% 2% 3% 4% 5% 6% ## 0.1146055 0.1943932 0.2394775 0.2772172 0.3503580 0.3976528 ## 7% 8% 9% 10% 11% 12% ## 0.4437526 0.4681780 0.5119180 0.5494182 0.6038977 0.6355183 ## 13% 14% 15% 16% 17% 18% ## 0.6690467 0.7111336 0.7422105 0.7849838 0.8501244 0.9022602 ## 19% 20% 21% 22% 23% 24% ## 0.9365174 0.9823572 1.0228292 1.0687425 1.1275014 1.1513826 ## 25% 26% 27% 28% 29% 30% ## 1.1993169 1.2437948 1.2672055 1.3147240 1.3614517 1.4038494 ## 31% 32% 33% 34% 35% 36% ## 1.4497996 1.4833810 1.5448401 1.5918055 1.6157927 1.6451807 ## 37% 38% 39% 40% 41% 42% ## 1.6960245 1.7401633 1.7988360 1.8307613 1.8862457 1.9796700 ## 43% 44% 45% 46% 47% 48% ## 2.0390340 2.1075666 2.1584948 2.1834225 2.2223983 2.2690784 ## 49% 50% 51% 52% 53% 54% ## 2.3255925 2.3736661 2.4310089 2.4774296 2.5345954 2.5828468 ## 55% 56% 57% 58% 59% 60% ## 2.6215819 2.6728154 2.7490825 2.8040575 2.8798923 2.9470751 ## 61% 62% 63% 64% 65% 66% ## 2.9987330 3.0718292 3.1576824 3.2259442 3.2834047 3.3389540 ## 67% 68% 69% 70% 71% 72% ## 3.4238760 3.5078819 3.5841119 3.6566212 3.7310638 3.7955911 ## 73% 74% 75% 76% 77% 78% ## 3.8898840 4.0079524 4.0700629 4.1421953 4.2397658 4.3362970 ## 79% 80% 81% 82% 83% 84% ## 4.4653508 4.5832694 4.7817955 4.9049975 5.0720404 5.1548082 ## 85% 86% 87% 88% 89% 90% ## 5.3273130 5.4756769 5.6714234 5.8951621 6.0027664 6.1684467 ## 91% 92% 93% 94% 95% 96% ## 6.4730652 6.8505916 7.0119433 7.3750473 7.7669967 8.2079801 ## 97% 98% 99% 100% ## 8.8282866 9.5525724 11.6663516 17.3538946 a1&lt;-matrix(1:20,4,5) I&lt;-diag(rep(1,5)) b1&lt;-t(a1)%*%a1 b2&lt;-a1%*%t(a1) #solve(b1) #solve(b2) det(b1) ## [1] 2.791799e-37 det(b2) ## [1] 0 eigen(b1) ## eigen() decomposition ## $values ## [1] 2.864414e+03 5.585784e+00 -3.006427e-14 -5.964175e-14 ## [5] -1.744794e-13 ## ## $vectors ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.09654784 0.76855612 0.00000000 0.00000000 6.324555e-01 ## [2,] -0.24551564 0.48961420 0.08358281 0.54130760 -6.324555e-01 ## [3,] -0.39448345 0.21067228 0.29202311 -0.78404241 -3.162278e-01 ## [4,] -0.54345125 -0.06826963 -0.83479466 -0.05583796 1.609823e-15 ## [5,] -0.69241905 -0.34721155 0.45918874 0.29857278 3.162278e-01 dim(b1) ## [1] 5 5 #indexaci?n v2&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;2&quot;) v2[1] ## [1] &quot;a&quot; v2[3] ## [1] &quot;2&quot; v3&lt;-1:20 v3&lt;-v3/10 v3 ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 ## [16] 1.6 1.7 1.8 1.9 2.0 v3[c(3,19)] ## [1] 0.3 1.9 v3[v3&gt;0.5] ## [1] 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2.0 v3[v3==1] ## [1] 1 v3[v3!=0.9] ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0 1.1 1.2 1.3 1.4 1.5 1.6 ## [16] 1.7 1.8 1.9 2.0 v3[v3&gt;0.5 &amp; v3&lt;1] ## [1] 0.6 0.7 0.8 0.9 v3[v3&gt;0.5 || v3&lt;1] ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 ## [16] 1.6 1.7 1.8 1.9 2.0 a1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 a1[1,] ## [1] 1 5 9 13 17 a1[,1] ## [1] 1 2 3 4 a1[3,1] ## [1] 3 a1[c(1,4),] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 4 8 12 16 20 a1[,c(2,4)] ## [,1] [,2] ## [1,] 5 13 ## [2,] 6 14 ## [3,] 7 15 ## [4,] 8 16 a1[c(1,4),c(2,4)] ## [,1] [,2] ## [1,] 5 13 ## [2,] 8 16 a1[,2]&gt;6 ## [1] FALSE FALSE TRUE TRUE a1[a1[,2]&gt;6,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 7 11 15 19 ## [2,] 4 8 12 16 20 mean(a1) ## [1] 10.5 a2&lt;-as.data.frame(a1) a2 ## V1 V2 V3 V4 V5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 str(a2) ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ V1: int 1 2 3 4 ## $ V2: int 5 6 7 8 ## $ V3: int 9 10 11 12 ## $ V4: int 13 14 15 16 ## $ V5: int 17 18 19 20 mean(a2$V1) ## [1] 2.5 colMeans(a2) ## V1 V2 V3 V4 V5 ## 2.5 6.5 10.5 14.5 18.5 summary(a2) ## V1 V2 V3 V4 ## Min. :1.00 Min. :5.00 Min. : 9.00 Min. :13.00 ## 1st Qu.:1.75 1st Qu.:5.75 1st Qu.: 9.75 1st Qu.:13.75 ## Median :2.50 Median :6.50 Median :10.50 Median :14.50 ## Mean :2.50 Mean :6.50 Mean :10.50 Mean :14.50 ## 3rd Qu.:3.25 3rd Qu.:7.25 3rd Qu.:11.25 3rd Qu.:15.25 ## Max. :4.00 Max. :8.00 Max. :12.00 Max. :16.00 ## V5 ## Min. :17.00 ## 1st Qu.:17.75 ## Median :18.50 ## Mean :18.50 ## 3rd Qu.:19.25 ## Max. :20.00 cov(a2) ## V1 V2 V3 V4 V5 ## V1 1.666667 1.666667 1.666667 1.666667 1.666667 ## V2 1.666667 1.666667 1.666667 1.666667 1.666667 ## V3 1.666667 1.666667 1.666667 1.666667 1.666667 ## V4 1.666667 1.666667 1.666667 1.666667 1.666667 ## V5 1.666667 1.666667 1.666667 1.666667 1.666667 cor(a2) ## V1 V2 V3 V4 V5 ## V1 1 1 1 1 1 ## V2 1 1 1 1 1 ## V3 1 1 1 1 1 ## V4 1 1 1 1 1 ## V5 1 1 1 1 1 a2 ## V1 V2 V3 V4 V5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 1.4 Loops y condiciones rm(list = ls()) x&lt;-5 ## if if(x&gt;6){ hist(rnorm(100,3,2)) } ## if else if(x&gt;6){ hist(rnorm(100,3,2)) } else { boxplot(rnorm(100,3,2)) } ## if encadenado x&lt;-60 if(x&gt;6){ hist(rnorm(100,3,2)) mean(x) } else if(x&lt;6){ boxplot(rnorm(100,3,2)) } else if(typeof(x)==&quot;double&quot;){ print(&quot;hola&quot;) } else { print(&quot;hola hola&quot;) } ##for for(i in 1:100){ print(i) } for(i in c(2,7,9,15,19)){ print(i) } for(i in c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)){ print(i) } for(i in 1:5){ for(j in 6:10){ print(i*j) } } for(i in 1:5){ for(j in 6:10){ aux&lt;-i*j print(aux) if(aux==20){ break() print(&quot;hola&quot;) boxplot(rnorm(100,i,j)) } } } #while z&lt;-1 k&lt;-1 while(z&gt;0.0001){ print(k) z&lt;-1/k k&lt;-k+1 } 1.5 Funciones fx&lt;-function(x){ y&lt;-8*x**2 return(y) } fx(5) fx(1:10) curve(fx,xlim = c(-20,20),ylim=c(0,1000)) plot(fx) ##comando de estadisticas de tendencia central tendencia&lt;-function(x){ n&lt;-length(x) cat(&quot;media:&quot;,fill = T) print(sum(x)/n) cat(&quot;mediana:&quot;,fill = T) x&lt;-sort(x) if(n%%2==0){ me&lt;-(x[n/2]+x[n/2+1])/2 } else { me&lt;-x[ceiling(n/2)] } print(me) cat(&quot;moda:&quot;,fill = T) tt&lt;-table(x) mm&lt;-max(tt) print(names(tt)[(table(x)==mm)]) cat(&quot;media cuadrática:&quot;,fill = T) mc&lt;-sqrt(sum(x**2)/n) print(mc) cat(&quot;media armónica&quot;,fill = T) ma&lt;-n/sum(1/x) print(ma) cat(&quot;media geométrica:&quot;,fill = T) mg&lt;-prod(x)**(1/n) print(mg) } xx&lt;-scan() a&lt;-tendencia(xx) tendencia2&lt;-function(x){ n&lt;-length(x) media&lt;-sum(x)/n x&lt;-sort(x) if(n%%2==0){ me&lt;-(x[n/2]+x[n/2+1])/2 } else { me&lt;-x[ceiling(n/2)] } tt&lt;-table(x) mm&lt;-max(tt) mo&lt;-names(tt)[(table(x)==mm)] mc&lt;-sqrt(sum(x**2)/n) ma&lt;-n/sum(1/x) mg&lt;-prod(x)**(1/n) aux&lt;-list(media,me,mo,mc,ma,mg) return(aux) } a&lt;-tendencia2(xx) tendenciaF&lt;-function(x,f){ n&lt;-sum(f) media&lt;-sum(x*f)/n } 1.6 Importacion de datos #importando bases de datos getwd() setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) dir() ##SPSS ## activar o instalar librerias library(foreign) eh18d&lt;-read.spss(&quot;EH2018_Discriminacion.sav&quot;, to.data.frame = T,use.value.labels=F) names(eh18d) table(eh18d$s10a_01a) attributes(eh18d)$variable.labels[5] head(eh18d) ##CSV setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\CSV&quot;) dir() eh18d2&lt;-read.csv2(&quot;EH2018_Discriminacion.csv&quot;) aux&lt;-read.csv(&quot;EH2018_Discriminacion.csv&quot;,sep=&quot;;&quot;) table(eh18d2$s10a_01a) daux&lt;-c(&quot;Sexo&quot;,&quot;O. Sexual&quot;,&quot;Edad&quot;,&quot;piel&quot;,&quot;Pertenencia&quot;, &quot;idioma&quot;,&quot;vestimente&quot;,&quot;procedencia&quot;,&quot;discapacidad&quot;, &quot;religion&quot;,&quot;clase&quot;,&quot;otro&quot;) for(i in 5:16){ barplot(prop.table(table(eh18d2[,i]))*100,main=daux[i-4]) } 1.7 Dataframe y exploración rm(list=ls()) setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) library(foreign) dir() eh18&lt;-read.spss(&quot;EH2018_Persona.sav&quot;, to.data.frame = T,use.value.labels=F) ################################################# names(eh18)#nombres de las variable str(eh18) class(eh18) ##renombrando variables names(eh18)[c(5,6)]&lt;-c(&quot;sexo&quot;,&quot;edad&quot;) names(eh18) ##crear variable (mujer 1=mujer, 0 eoc) eh18$mujer&lt;-eh18$sexo==2 sum(eh18$mujer) mean(eh18$mujer) #exportación (&quot;write&quot;) write.dta(eh18,&quot;eh18p.dta&quot;) getwd() #excel library(readxl) install.packages(&quot;writexl&quot;) library(writexl) library(help=writexl) write_xlsx(eh18,&quot;eh18p.xlsx&quot;) apropos(&quot;read&quot;) apropos(&quot;write&quot;) install.packages(&quot;haven&quot;) library(haven) eh18$s03a_02e&lt;-substr(as.character(eh18$s03a_02e),1,120) eh18$s03a_02e&lt;-gsub(&quot; &quot;,&quot; &quot;,as.character(eh18$s03a_02e)) gsub(&quot;a&quot;,&quot;7&quot;,&quot;hola&quot;) gsub(&quot;`&quot;,&quot;&#39;&quot;,&quot;`s04a_04e`, `s04a_07ge`, `s04b_15e`, `s04b_16e`, `s04b_17e`, `s04b_18e`, `s04e_30c`, `s04f_35e`, `s05a_04`, `s05c_13_e`, `s05d_21e`, `s05d_22_e`, `s06a_06e`, `s06a_09e`, `s06a_10e`, `s06b_11a`, `s06b_11b`, `s06b_12a`, `s06b_12b`, `s06b_13`, `s06b_20e`, `s06e_34e`, `s06f_40a`, `s06f_40b`, `s06h_54e`, `s06h_55e`, `s06h_56e`, `s07a_01e1e`, `s07a_01e2e`, `s07a_02ce`, `s07b_05de`, `s07b_05ee`, `s07c_08e`, `s07c_09e`&quot;) nchar(eh18$s03a_02e) vv&lt;-c(&#39;s04a_04e&#39;, &#39;s04a_07ge&#39;, &#39;s04b_15e&#39;, &#39;s04b_16e&#39;, &#39;s04b_17e&#39;, &#39;s04b_18e&#39;, &#39;s04e_30c&#39;, &#39;s04f_35e&#39;, &#39;s05a_04&#39;, &#39;s05c_13_e&#39;, &#39;s05d_21e&#39;, &#39;s05d_22_e&#39;, &#39;s06a_06e&#39;, &#39;s06a_09e&#39;, &#39;s06a_10e&#39;, &#39;s06b_11a&#39;, &#39;s06b_11b&#39;, &#39;s06b_12a&#39;, &#39;s06b_12b&#39;, &#39;s06b_13&#39;, &#39;s06b_20e&#39;, &#39;s06e_34e&#39;, &#39;s06f_40a&#39;, &#39;s06f_40b&#39;, &#39;s06h_54e&#39;, &#39;s06h_55e&#39;, &#39;s06h_56e&#39;, &#39;s07a_01e1e&#39;, &#39;s07a_01e2e&#39;, &#39;s07a_02ce&#39;, &#39;s07b_05de&#39;, &#39;s07b_05ee&#39;, &#39;s07c_08e&#39;, &#39;s07c_09e&#39;) for(i in vv){ eh18[,i]&lt;-gsub(&quot; &quot;,&quot; &quot;,as.character(eh18[,i])) eh18[,i]&lt;-substr(eh18[,i],1,120) } write_sav(eh18,&quot;eh18p.sav&quot;) save(eh18,vv,file=&quot;eh18p.RData&quot;) rm(list=ls()) getwd() load(&quot;eh18p.RData&quot;) set.seed(123456)####semilla a&lt;-matrix(rnorm(20),5,4) apply(a, 1, sum) apply(a, 2, mean) apply(a, 1, sd) mc&lt;-function(x){ n&lt;-length(x) aux&lt;-sqrt(sum(x^2)/n) return(aux) } apply(a, 1, mc) apply(a, 2, mc) tapply(eh18$mujer,eh18$depto,mean) tapply(eh18$mujer,list(eh18$depto,eh18$area),mean) tapply(eh18$edad,list(eh18$depto,eh18$area),mean) tapply(eh18$edad&gt;60,list(eh18$depto,eh18$area),mean) tapply(eh18$edad&lt;5,list(eh18$depto,eh18$area),mean) tapply(eh18$p0,list(eh18$depto,eh18$area),mean,na.rm=T) aux&lt;-tapply(eh18$p0,list(eh18$depto,eh18$area,eh18$niv_ed_g),mean,na.rm=T) as.data.frame(aux) lapply() mapply() sapply() 1.8 Estadística descriptiva #Estadística descriptiva rm(list=ls()) setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) #encuesta a hogares 2018 load(&quot;eh18p.RData&quot;) data()#muestra las bases de datos disponibles en R ChickWeight View(ChickWeight) help(&quot;ChickWeight&quot;) ##Porcentajes y tablas de fracuencias t1&lt;-table(eh18$sexo)#tabla de frcuencias t2&lt;-table(eh18$depto) t3&lt;-table(eh18$edad) barplot(t1,col=c(&quot;blue&quot;,&quot;darkgreen&quot;))#diagramas de barra barplot(t3,horiz = T) barplot(t3,col=&quot;red&quot;) ldep&lt;-c(&quot;CH&quot;,&quot;LP&quot;,&quot;CB&quot;,&quot;OR&quot;,&quot;PT&quot;,&quot;TJ&quot;,&quot;SC&quot;,&quot;BN&quot;,&quot;PD&quot;) barplot(t2,col=&quot;black&quot;,names.arg = ldep) #llevando a % t2p&lt;-prop.table(t2)*100 barplot(t2p,col=&quot;black&quot;,names.arg = ldep,ylab=&quot;%&quot;) pie(t2p) t3&lt;-as.data.frame(t3) t3$F&lt;-cumsum(t3$Freq) t3$r&lt;-prop.table(t3$Freq) t3$R&lt;-cumsum(t3$r) head(t3) table(ChickWeight$Time) table(ChickWeight$Diet) dim(table(ChickWeight$Chick)) #tabla de contingencia aux&lt;-table(ChickWeight$Chick,ChickWeight$Diet) aux[aux!=0]&lt;-1 apply(aux,2,sum) table(ChickWeight$Time) table(ChickWeight$Diet[ChickWeight$Time==0]) ##medidas de tendencia central mean(eh18$edad) median(eh18$edad) mean(eh18$ylab,na.rm = T) median(eh18$ylab,na.rm = T) hist(eh18$ylab) plot(density(eh18$ylab,na.rm = T)) locator(1) summary(eh18$ylab) #medidas de dispersion var(eh18$ylab,na.rm = T) sd(eh18$ylab,na.rm = T) max(eh18$ylab,na.rm = T)-min(eh18$ylab,na.rm = T) sd(eh18$ylab,na.rm = T)/mean(eh18$ylab,na.rm = T) #medidas de forma quantile(eh18$ylab,c(0.1,0.44,0.99),na.rm=T) hist(eh18$edad) #Coeficiente de asimetría #EDAD x&lt;-eh18$edad N&lt;-length(x) as&lt;-(sum((x-mean(x))^3)/N)/sd(x)^3 as #iNGRESO LABORAL x&lt;-eh18$ylab x&lt;-x[is.na(x)==F] x&lt;-x[complete.cases(x)] N&lt;-length(x) as&lt;-(sum((x-mean(x))^3)/N)/sd(x)^3 as #kurtosis... plot(density(rnorm(1000,sd=40)),ylim=c(0,0.08)) points(density(rnorm(1000,sd=10)),type=&quot;l&quot;,col=&quot;red&quot;) points(density(rnorm(1000,sd=5)),type = &quot;l&quot;,col=&quot;blue&quot;) lm(ylab~edad,data=eh18,weights = f) lm(ylab~-1+edad,data=eh18) summary(lm(ylab~edad,data=eh18)) 1.9 Muestreo e inferencia rm(list=ls()) N&lt;-15;n&lt;-6 choose(N,n) set.seed(123) y1&lt;-rnorm(N,10,5) set.seed(234) y2&lt;-rexp(N,3) set.seed(345) y3&lt;-runif(N,500,3000) #par?metros theta1&lt;-mean(y1) theta2&lt;-sum(y2) # sum(y3&gt;1500)/N # mean(y3&gt;1500) theta3&lt;-mean(y3&gt;1500) #pr?ctica (real) solo se tiene acceso a una #muestra U&lt;-1:10 set.seed(888) s&lt;-sample(U,n) s y1 #estimaciones mean(y1[s]);theta1 mean(y2[s])*N ; theta2 mean(y3[s]&gt;1500) ; theta3 combn(U,3) combn(y1,3) combn(y1,3,mean) #theta1 t1s&lt;-apply(combn(y1,n),2,mean) mean(t1s) theta1 hist(t1s) plot(density(t1s)) shapiro.test(t1s) #theta2 t2s&lt;-apply(combn(y2,n),2,mean)*N mean(t2s);theta2 hist(t2s) plot(density(t2s)) shapiro.test(t2s) #theta3 t3s&lt;-apply(combn(y3,n)&gt;1500,2,mean) mean(t3s);theta3 hist(t3s) plot(density(t3s)) shapiro.test(t3s) ####Inferencia a partir de una muestra #libreria survey install.packages(&quot;survey&quot;) library(survey) bd&lt;-data.frame(y1,y2,y3=y3&gt;1500) set.seed(123) s&lt;-sample(1:15,7) bds&lt;-bd[s,] bds$w&lt;-15/7 bds$pk&lt;-7/15 bds dm1&lt;-svydesign(ids=~0,probs = ~pk,data=bds) svymean(~y1,design = dm1) svytotal(~y2,design = dm1) library(help=survey) 1.10 Gráficos de origen rm(list=ls()) ################ plot(0,0)#inicia una hoja en blanco plot(0,0,type = &quot;n&quot;) x&lt;-c(3,4,7,2) y&lt;-c(0,6,9,2) plot(x,y,type=&quot;p&quot;) plot(x,y,type=&quot;h&quot;) plot(x,y,type=&quot;l&quot;) plot(x[order(x)],y[order(x)],type=&quot;l&quot;) plot(x,y,type = &quot;b&quot;) plot(x,y,type = &quot;o&quot;) par(mfrow=c(2,2)) plot(x,y,xlim=c(0,10),ylim=c(0,10),main=&quot;c/PUNTOS&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10),main = &quot;S/puntos&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,main=&quot;sin ejes&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,ann = F,main = &quot;blanco&quot;) par(mfrow=c(1,1)) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,ann = F) points(x,y,cex=c(1,2,3,4)+1) points(x,y,cex=c(1,2,3,4)+1,pch=15) points(x,y,cex=c(1,2,3,4)+1,pch=15,col=&quot;blue&quot;) points(x,y,cex=c(1,2,3,4)+1,pch=0:3, col=c(&quot;red&quot;,&quot;darkblue&quot;,&quot;pink&quot;,&quot;darkgreen&quot;)) #points(x,y,cex=2) pdf(&quot;figura1.pdf&quot;,width = 15,height = 5) plot(x,y,type=&quot;n&quot;,xlim=c(0,20),ylim=c(0,20), axes = F,ann = F) points(seq(2,10,2),seq(2,10,2),type=&quot;p&quot;,lwd=4, col=&quot;blue&quot;,lty=4) text(seq(2,10,2),seq(2,10,2), labels = c(&quot;p1&quot;,&quot;p2&quot;,&quot;p3&quot;,&quot;p4&quot;,&quot;p5&quot;),pos=3, cex=1:5,col=&quot;brown&quot;) axis(1,seq(0,20,10),lwd=3) axis(2,seq(0,20,5),lwd=2) axis(4,seq(0,20,5),seq(0,400,100),lwd=2) axis(3,seq(0,20,10),c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),lwd=2) title(main=&quot;Plot en R&quot;,col.main=&quot;darkred&quot;, cex.main=3) title(xlab = &quot;Eje X&quot;,ylab=&quot;Eje Y&quot;,cex.lab=2, col.lab=&quot;gray&quot;) legend(&quot;topright&quot;,legend = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), lwd=c(1,2,3),col=c(&quot;darkred&quot;,&quot;black&quot;,&quot;gray&quot;)) dev.off() getwd() png(&quot;figura1.png&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,20),ylim=c(0,20), axes = F,ann = F) points(seq(2,10,2),seq(2,10,2),type=&quot;p&quot;,lwd=4, col=&quot;blue&quot;,lty=4) dev.off() pdf(&quot;sec.pdf&quot;) for(i in 1:10){ plot(i,0,xlim=c(0,10)) } dev.off() abline(h=c(2,3,4),lty=2) abline(v=c(2,3,4)) bd&lt;-as.data.frame(state.x77) bd$name&lt;-row.names(bd) head(bd) 1.11 ggplot ####################################### # Clase: gráficos en R, ggplot # Materia: Programación Estadística I # Fecha: 9 de Marzo ####################################### rm(list=ls()) ####################################### #install.packages(&quot;ggplot2&quot;) #install.packages(&quot;dplyr&quot;) #install.packages(&quot;maps&quot;) #install.packages(&quot;ggvis&quot;) library(ggplot2) library(dplyr) library(maps) library(ggvis) library(readxl) ####################################### urlfile&lt;-url(&#39;https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData&#39;) load(urlfile) names(computo)[18]&lt;-&quot;MAS&quot; ####################################### #The grammar of graphics is an answer to a question: what is a statistical graphic? #base graphics 1983 #grid 2000 #lattice 1993 #ggplot 2005 #ggvis 2014 ## Datos, estetica y geometria (layers) library(ggplot2) mpg ggplot(mpg, aes(x = displ, y = hwy)) + geom_area() ggplot(computo,aes(MAS,CC))+geom_point() ggplot(mpg, aes(displ, hwy)) + geom_line() ggplot(mpg, aes(displ)) + geom_histogram() #Color tamaño y forma ggplot(mpg, aes(displ, hwy,colour = class)) + geom_point() ggplot(mpg, aes(displ, hwy,shape = drv)) + geom_point() ggplot(mpg, aes(displ, hwy,size = cyl)) + geom_point() ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = &quot;blue&quot;)) ggplot(mpg, aes(displ, hwy)) + geom_point(colour = &quot;blue&quot;) ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class) ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class+drv) #geoms geom_smooth() # ajuste y en x geom_boxplot() geom_histogram() geom_freqpoly() geom_bar() geom_path() geom_line() #ajuste de un modelo ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth() ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ggplot(mpg, aes(drv, hwy)) + geom_point() ggplot(mpg, aes(drv, hwy)) + geom_jitter() ggplot(mpg, aes(drv, hwy)) + geom_boxplot() ggplot(mpg, aes(drv, hwy)) + geom_violin() ggplot(mpg, aes(hwy)) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mpg, aes(hwy)) + geom_freqpoly() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mpg, aes(hwy)) + geom_freqpoly(binwidth = 2.5) ggplot(mpg, aes(hwy)) + geom_freqpoly(binwidth = 1) ggplot(mpg, aes(hwy)) + geom_density() ggplot(mpg, aes(displ, colour = drv)) + geom_freqpoly(binwidth = 0.5) ggplot(mpg, aes(displ, fill = drv)) + geom_histogram(binwidth = 0.5) ggplot(mpg, aes(displ, fill = drv)) + geom_histogram(binwidth = 0.5) + facet_wrap(~drv, ncol = 1) ggplot(mpg, aes(manufacturer)) + geom_bar() aa&lt;-as.data.frame(table(mpg$class)) aa ggplot(aa,aes(Var1,Freq))+ geom_bar() ggplot(aa,aes(Var1,Freq)) + geom_bar(stat = &quot;identity&quot;) ggplot(drugs, aes(drug, effect)) + geom_point() ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(&quot;city driving (mpg)&quot;) + ylab(&quot;highway driving (mpg)&quot;) # Remove the axis labels with NULL ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(NULL) + ylab(NULL) ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) + xlim(&quot;f&quot;, &quot;r&quot;) + ylim(20, 30) #&gt; Warning: Removed 137 rows containing missing values (geom_point). # For continuous scales, use NA to set only one limit ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25, na.rm = TRUE) + ylim(NA, 30) #output p &lt;- ggplot(mpg, aes(displ, hwy, colour = factor(cyl))) + geom_point() # Save png to disk ggsave(&quot;plot.png&quot;, p, width = 5, height = 5) saveRDS(p, &quot;plot.rds&quot;) q &lt;- readRDS(&quot;plot.rds&quot;) ggplot(faithfuld, aes(eruptions, waiting)) + geom_contour(aes(z = density, colour = ..level..)) mi_counties &lt;- map_data(&quot;world&quot;) %&gt;% select(lon = long, lat, group, id = subregion) ggplot(mi_counties, aes(lon, lat)) + geom_point(size = .25, show.legend = FALSE) + coord_quickmap() ggplot(mi_counties, aes(lon, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;grey50&quot;) + coord_quickmap() #temas theme_bw() theme_grey() theme_linedraw() theme_light() theme_dark() theme_minimal() theme_classic() theme_void() 1.12 R Markdown “R Markdown” se introdujo por primera vez en el paquete knitr a principios de 2012. La idea era incrustar fragmentos de código (de R u otros) en los documentos de Markdown. De hecho, knitr soportó varios lenguajes de autoría desde el principio además de Markdown, incluidos LaTeX, HTML, AsciiDoc, reStructuredText y Textile. Markdown se ha convertido en el formato de documento más popular. La simplicidad de Markdown se destaca claramente entre estos formatos de documentos. 1.12.1 Instalación install.packages(&#39;rmarkdown&#39;) # Si se prefiere la versión en desarrollo if (!requireNamespace(&quot;devtools&quot;)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;rstudio/rmarkdown&#39;) Si el objetivo es usar Markdown para generar documentos PDF se necesita instalar Latex. Existen cheatsheets utiles para usar markdown, como: cheatsheets 1.12.2 YAML Header Al inicio del archivo y entre las lineas — --- title: Mi documento author: Juan Perez date: Marzo 22, 20220 output: html_document --- 1.12.3 Sintaxis básica Énfasis sobre el texto, *italic* **bold** _italic_ __bold__ Secciones, # Header 1 ## Header 2 ### Header 3 Items (viñetas) no ordenadas y ordenadas, * Item 1 * Item 2 + Item 2a + Item 2b 1. Item 1 2. Item 2 3. Item 3 + Item 3a + Item 3b Palabras clave con referencias web, [linked phrase](http://example.com) Imágenes simples o con titulo, ![](http://example.com/logo.png) ![optional caption text](figures/img.png) Blockquotes It’s always better to give than to receive. A friend once said: &gt; It&#39;s always better to give than to receive. Ecuaciones en linea y en párrafo, En linea \\(\\sum_i{x^2}\\) o en párrafo: \\[\\sum_i{x^2}\\] $equation$ $$ equation $$ 1.12.4 Tipos de documentos beamer_presentation github_document html_document ioslides_presentation latex_document md_document odt_document pdf_document powerpoint_presentation rtf_document slidy_presentation word_document 1.12.5 Chunks Los chunks son entornos que permiten incluir código en R dentro de las distintos tipos de documentos que genera Rmarkdown, los chunks inician con ```{r} y termina con ```, también es posible introducir chunks en linea con el texto, esto se logra introduciendo Texto ... `r &lt;code&gt;` ... texto La parte {r} del chunk sirve para introducir las distintas opciones que va a contener ese chunk, las opciones disponibles son: echo (default = TRUE), muestra el código del chunk en la salida del documento eval (default = TRUE), corre el código del chunk message (default = TRUE), muestra los mensajes que genera el chunk Existen funciones útiles para mejorar las salidas de tablas, tales como xtable y kable de la librería knitr. 1.13 Shiny Shiny es una librería de RStudio orientada a crear aplicaciones web interactivas con R. Una vez instalada existen dos formas de crear una aplicación en Shiny. Una alternativa cada vez mas popular es la de crear un documento shiny junto con Markdown. Mediante un solo archivo denominando app.R Mediante dos archivos separados, el server.R y ui.R Se recomienda que en cualquiera de las dos alternativas, estos archivos estén contenidos en alguna carpeta. Existen dos partes esenciales al momento de definir una app en Shiny, el UI que es una función que define la interfaz de la aplicación y el Server que define una función con instrucciones sobre cómo construir y reconstruir los objetos R que se mostraran en la UI. La composición básica según las formas de aplicarlas son: # app.R library(shiny) ui &lt;- fluidPage( numericInput(inputId = &quot;n&quot;, &quot;Sample size&quot;, value = 25), plotOutput(outputId = &quot;hist&quot;) ) server &lt;- function(input, output) { output$hist &lt;- renderPlot({ hist(rnorm(input$n)) }) } shinyApp(ui = ui, server = server) # ui.R fluidPage( numericInput(inputId = &quot;n&quot;, &quot;Sample size&quot;, value = 25), plotOutput(outputId = &quot;hist&quot;) ) # server.R function(input, output) { output$hist &lt;- renderPlot({ hist(rnorm(input$n)) }) 1.13.1 UI Al ser UI la interfaz esta permite la interacción directa con el usuario, a estas se las denominan los entradas \\(input\\), las opciones de input son: Botón de acción: actionButton(inputId, label, icon) Enlace: actionLink(inputId, label, icon,) Check box múltiple: checkboxGroupInput(inputId,label, choices, selected, inline) Check box simple: checkboxInput(inputId, label,value) Fecha: dateInput(inputId, label, value,min, max, format, startview,weekstart, language) Rango de fecha: dateRangeInput(inputId, label,start, end, min, max, format,startview, weekstart, language,separator) Cargar archivo: fileInput(inputId, label, multiple,accept) Entrada numérica: numericInput(inputId, label, value,min, max, step) Tipo contraseña: passwordInput(inputId, label,value) Selección tipo botones: radioButtons(inputId, label,choices, selected, inline) Seleccionable: selectInput(inputId, label, choices,selected, multiple, selectize,width, size) Slider: sliderInput(inputId, label, min,max, value, step, round, format,locale, ticks, animate, * width,sep,pre, post) Enviar submitButton(text, icon) Entrada de texto textInput(inputId, label, value) Los inputs principalmente tienen dos argumentos el inputId que se refiere al identificador del input, este se utiliza en el server, y el label que es la etiqueta que aparece en la interfaz visual, estos inputs se asignan a algún objeto (xx&lt;-input()). En el server se tiene acceso al input mediante input$xx. 1.13.2 Server En cuanto el server, este usa los distintos inputs para generar las salidas (outputs), las opciones de salidas disponibles son: DT::renderDataTable(expr, options,callback, escape, env, quoted) renderImage(expr, env, quoted,deleteFile) renderPlot(expr, width, height, res, …,env, quoted, func) renderPrint(expr, env, quoted, func,width) renderTable(expr,…, env, quoted, func) renderText(expr, env, quoted, func) renderUI(expr, env, quoted, func) Estos outputs “render” se asignan a un objeto y luego ellos deben ser incluidos dentro del UI con su correspondiente Output: dataTableOutput(outputId, icon, …) imageOutput(outputId, width, height,click, dblclick, hover, hoverDelay, inline,hoverDelayType, brush, clickId,hoverId) plotOutput(outputId, width, height, click,dblclick, hover, hoverDelay, inline,hoverDelayType, brush, clickId,hoverId) verbatimTextOutput(outputId) tableOutput(outputId) textOutput(outputId, container, inline) uiOutput(outputId, inline, container, …) y htmlOutput(outputId, inline, container, …) 1.14 Ejercicios Propuestos Crear una función que devuelva los \\(k\\) primeros números primos Crear una función que calcule la mediana para tablas de frecuencias con intervalos de clases Crear una función que calcule los Quantiles para tablas de frecuencias con intervalos de clases Realice una función para el calculo del tamaño de muestra para el muestreo aleatorio simple, considere la media, el margen de error relativo y coeficientes de confianza. Crear una función que realice la prueba de independencia Chi-cuadrado Empleando la ENDSA muestre por año y departamento el porcentaje de personas que fuman Utilizando la base de datos del COVID-19 genere un gráfico de evolución de contagios, muertes y recuperados. Use los gráficos de origen Utilizando la base de datos del COVID-19 genere un gráfico de evolución de contagios, muertes y recuperados. Use ggplot Utilizando la encuesta 2018, genere un reporte que presente: Total de población y viviendas por departamento y área Pobreza moderada por Departamento y área Indice de Gini por departamento Realice una función en Shiny empleando la base de datos electoral del \\(20o\\) que permita ver los resultados por recinto, seleccionando, su país, departamento, municipio, recinto. Usando la EH 2018, determine el porcentaje de personas que sufrieron un Atraco (Robo a personas) en la vía pública en los últimos 12 meses. "],
["scraping-web.html", "Chapter 2 Scraping Web 2.1 Pasos para la recopilación de información 2.2 Tecnologías de diseminación, extracción y almacenamiento Web 2.3 Librerías en R 2.4 Librería rvest 2.5 Ejemplo: COVID, Bolivia segura, Worldometers 2.6 Ejemplo: Ultracasas 2.7 Ejemplo: Ketal 2.8 Ejemplo: Ministerio de Educación 2.9 Ejemplo, informacion COVID-19 en Bolivia 2.10 APIs 2.11 Ejercicios Propuestos", " Chapter 2 Scraping Web La definición del scraping web que se toma en este documento proviene de (???) que expresa: El Web Scraping es la recolección automática de información de los sitios web. (obviamente no a través de un humano usando un navegador web). Un tema dentro del scraping web son las denominadas APIs (Application Programming Interface), estas son entradas a las páginas web diseñadas por los administradores de la página web, por lo mismo no siempre contienen toda la información que se desea. Aunque las API no son tan ubicuas como deberían, puede encontrar API para muchos tipos de información. Interesado en la música? Hay algunas API diferentes que pueden darle canciones, artistas, álbumes e incluso información sobre estilos musicales y artistas relacionados. ¿Necesitas datos deportivos? ESPN proporciona API para información de atletas, puntajes de juegos y más. Google tiene docenas de API en su sección de Desarrolladores para traducciones de idiomas, análisis, geolocalización y más. 2.1 Pasos para la recopilación de información Siguiendo a (???) que establece cinco pasos al momento de decidir recopilar información mediante el scraping web, estos pasos son: Asegúrese de saber exactamente qué tipo de información necesita. Esto puede ser específico (el producto interno bruto de todos los países de la OCDE durante los últimos 10 años'') o vago (opinión de la gente sobre el teléfono de la empresa X’‘, ``colaboración entre miembros del Senado de los Estados Unidos’’). Averigüe si hay fuentes de datos en la Web que puedan proporcionar información directa o indirecta sobre su problema. Si está buscando hechos concretos, esto probablemente sea fácil. Si está interesado en conceptos bastante vagos, esto es más difícil. La página de inicio de la embajada de un país podría ser una fuente valiosa para la acción de política exterior que a menudo se oculta detrás del telón de la diplomacia. Los tweets pueden contener tendencias de opinión sobre casi todo, las plataformas comerciales pueden informar sobre la satisfacción de los clientes con los productos, las tarifas de alquiler en los sitios web de propiedades pueden contener información sobre el atractivo actual de los barrios de la ciudad … Desarrolle una teoría del proceso de generación de datos cuando busque fuentes potenciales. ¿Cuándo se generaron los datos, cuándo se cargaron en la Web y quién lo hizo? ¿Existen áreas potenciales que no están cubiertas, son consistentes o precisas, y puede identificarlas y corregirlas? Equilibrar las ventajas y desventajas de las posibles fuentes de datos. Los aspectos relevantes pueden ser la disponibilidad (¡y la legalidad!), los costos de recolección, la compatibilidad de nuevas fuentes con la investigación existente, pero también factores muy subjetivos como la aceptación de la fuente de datos por parte de otros. También piense en posibles formas de validar la calidad de sus datos. ¿Existen otras fuentes independientes que brinden información similar para que sean posibles verificaciones cruzadas aleatorias? En caso de datos secundarios, ¿puede identificar la fuente original y verificar los errores de transferencia? ¡Toma una decisión! Elija la fuente de datos que le parezca más adecuada, documente los motivos de la decisión y comience con los preparativos para la recopilación. Si es factible, recopile datos de varias fuentes para validar las fuentes de datos. Muchos problemas y beneficios de varias estrategias de recopilación de datos salen a la luz solo después de la recopilación real. 2.2 Tecnologías de diseminación, extracción y almacenamiento Web Una vez elegida la fuente de datos (página web) y al menos con la intuición de lo que se quiere obtener, el siguiente paso es decidir el mecanismos para el scraping (raspado), esto esta relacionado al tipo de pagina web, ver si esta ofrece una entrada API y conocer sus limitaciones puede ser un punto de partida. Esto se denomina Technologies for disseminating, extracting, and storing web data Collecting, en el marco del uso de R la figura muestra las interacciones entre ellas. Tecnologías para difundir, extraer y almacenar datos web (considerando el entorno de R) 2.3 Librerías en R El siguiente cuadro presenta las librerías en R relacionadas al web Scraping, incluyendo los servicios API, que a la fecha del proyecto alcanza a 761 librerías, esto representa el \\(5.02\\)% de las librerías en en R. library(knitr) library(packagefinder) library(xtable) findPackage(c(&quot;API&quot;,&quot;scrape&quot;),limit.results=-1) 2.4 Librería rvest Rvest es parte del universo tidyverse y esta orientada al scrape de páginas web. La instalación es usual: #desde CRAN install.packages(&quot;rvest&quot;) #la versión en desarrollo desde github devtools::install_github(&quot;tidyverse/rvest&quot;) Existe la herramienta selectorgadget disponible en http://selectorgadget.com/, esta permite interactuar con las páginas web para seleccionar partes del documento usando un CSS selector. Las funciones mas importantes dentro de rvest son: read_html para cargar la estructura de la página html_nodes para extraer información de la página según el CSS selector html_text para extraer texto de un html_nodes html_table para extraer tablas y ponerlas en data frame 2.5 Ejemplo: COVID, Bolivia segura, Worldometers library(rvest) bs1&lt;-read_html(&quot;https://www.boliviasegura.gob.bo/index.php&quot;) bs2&lt;-read_html(&quot;https://www.boliviasegura.gob.bo/datos.php&quot;) aux&lt;-html_nodes(bs1,&quot;.casos&quot;) casos&lt;-html_text(aux) aux&lt;-html_nodes(bs1,&quot;.recuperados&quot;) recup&lt;-html_text(aux) aux&lt;-html_nodes(bs1,&quot;.decesos&quot;) muerte&lt;-html_text(aux) html_text(html_nodes(bs1,&quot;.card-text&quot;)) ## [1] &quot;BENEFICIARIOS&quot; ## [2] &quot;BENEFICIARIOS&quot; ## [3] &quot;BENEFICIARIOS&quot; ## [4] &quot;13 de Noviembre de 2020&quot; ## [5] &quot;El presente Decreto Supremo tiene por objeto reglamentar la Ley N° 1330, de 16 de septiembre de 2020, que establece el Pago del Bono Contra el Hambre.&quot; ## [6] &quot;29 de Octubre de 2020&quot; ## [7] &quot;Medidas de carácter excepcional y temporal para fomentar la cultura de donación voluntaria, altruista e informada de plasma hiperinmune de personas que hayan superado el Coronavirus (COVID-19).&quot; ## [8] &quot;28 de Octubre de 2020&quot; ## [9] &quot;Se amplía la vigencia de las medidas de la fase de posconfinamiento con vigilancia comunitaria activa de casos de Coronavirus (COVID-19), hasta el 30 de noviembre.&quot; tbs&lt;-html_table(bs2,fill = T) View(tbs) mun&lt;-tbs[[12]] table(mun$`Nivel Anterior`,mun$`Nivel Actual`) ## ## Alto Medio Moderado ## Alto 33 0 0 ## Medio 6 211 4 ## Moderado 0 14 71 t1&lt;-tbs[[2]] global&lt;-c(casos, recup,muerte) str(t1) ## &#39;data.frame&#39;: 9 obs. of 5 variables: ## $ Departamento: chr &quot;Beni&quot; &quot;Chuquisaca&quot; &quot;Cochabamba&quot; &quot;La Paz&quot; ... ## $ Hoy : int 1 10 10 62 6 1 43 37 17 ## $ Acumulado : chr &quot;7,331&quot; &quot;7,952&quot; &quot;14,127&quot; &quot;35,369&quot; ... ## $ Decesos : chr &quot;374&quot; &quot;515&quot; &quot;1,318&quot; &quot;1,157&quot; ... ## $ Recuperados : chr &quot;5,461&quot; &quot;6,990&quot; &quot;12,341&quot; &quot;31,351&quot; ... #herramientas para trabajar con texto t1$Acumulado ## [1] &quot;7,331&quot; &quot;7,952&quot; &quot;14,127&quot; &quot;35,369&quot; &quot;6,111&quot; &quot;2,394&quot; &quot;9,339&quot; ## [8] &quot;44,588&quot; &quot;16,545&quot; nchar(t1$Acumulado) ## [1] 5 5 6 6 5 5 5 6 6 substr(t1$Acumulado,1,3) ## [1] &quot;7,3&quot; &quot;7,9&quot; &quot;14,&quot; &quot;35,&quot; &quot;6,1&quot; &quot;2,3&quot; &quot;9,3&quot; &quot;44,&quot; &quot;16,&quot; grep(&quot;53&quot;,t1$Acumulado) ## integer(0) gsub(&quot;53&quot;,&quot;XX&quot;,t1$Acumulado) ## [1] &quot;7,331&quot; &quot;7,952&quot; &quot;14,127&quot; &quot;35,369&quot; &quot;6,111&quot; &quot;2,394&quot; &quot;9,339&quot; ## [8] &quot;44,588&quot; &quot;16,545&quot; t1$Acumulado&lt;-as.numeric(gsub(&quot;,&quot;,&quot;&quot;,t1$Acumulado)) t1$Decesos&lt;-as.numeric(gsub(&quot;,&quot;,&quot;&quot;,t1$Decesos)) t1$Recuperados&lt;-as.numeric(gsub(&quot;,&quot;,&quot;&quot;,t1$Recuperados)) library(ggplot2) ggplot(t1,aes(Acumulado,Decesos))+geom_point() ### worldometers wm&lt;-read_html(&quot;https://www.worldometers.info/coronavirus/&quot;) tabla&lt;-html_table(wm) t2&lt;-tabla[[1]] str(t2) ## &#39;data.frame&#39;: 236 obs. of 19 variables: ## $ # : int NA NA NA NA NA NA NA NA 1 2 ... ## $ Country,Other : chr &quot;North America&quot; &quot;Asia&quot; &quot;South America&quot; &quot;Europe&quot; ... ## $ TotalCases : chr &quot;17,426,351&quot; &quot;17,500,534&quot; &quot;11,495,951&quot; &quot;18,398,157&quot; ... ## $ NewCases : chr &quot;+51,386&quot; &quot;+114,412&quot; &quot;+1,881&quot; &quot;+122,903&quot; ... ## $ TotalDeaths : chr &quot;427,378&quot; &quot;301,599&quot; &quot;331,682&quot; &quot;423,062&quot; ... ## $ NewDeaths : chr &quot;+913&quot; &quot;+1,451&quot; &quot;+41&quot; &quot;+2,176&quot; ... ## $ TotalRecovered : chr &quot;10,604,232&quot; &quot;15,460,498&quot; &quot;10,219,793&quot; &quot;8,194,976&quot; ... ## $ NewRecovered : chr &quot;+20,925&quot; &quot;+98,326&quot; &quot;+3,452&quot; &quot;+85,001&quot; ... ## $ ActiveCases : chr &quot;6,394,741&quot; &quot;1,738,437&quot; &quot;944,476&quot; &quot;9,780,119&quot; ... ## $ Serious,Critical : chr &quot;31,024&quot; &quot;28,094&quot; &quot;16,805&quot; &quot;27,502&quot; ... ## $ Tot Cases/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Deaths/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ TotalTests : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Tests/1M pop : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Population : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ Continent : chr &quot;North America&quot; &quot;Asia&quot; &quot;South America&quot; &quot;Europe&quot; ... ## $ 1 Caseevery X ppl : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ 1 Deathevery X ppl: chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... ## $ 1 Testevery X ppl : chr &quot;&quot; &quot;&quot; &quot;&quot; &quot;&quot; ... aux&lt;-is.na(t2$`#`) t2&lt;-t2[!aux,] for(i in c(3:15,17:19)){ print(i) t2[,i]&lt;-gsub(&quot;+&quot;,&quot;&quot;,t2[,i]) t2[,i]&lt;-as.numeric(gsub(&quot;,&quot;,&quot;&quot;,t2[,i])) } ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## Warning: NAs introducidos por coerción ## [1] 8 ## Warning: NAs introducidos por coerción ## [1] 9 ## Warning: NAs introducidos por coerción ## [1] 10 ## [1] 11 ## [1] 12 ## [1] 13 ## [1] 14 ## [1] 15 ## [1] 17 ## [1] 18 ## [1] 19 t2$`Tests/1M pop` ## [1] 621009 106064 120539 543452 320769 658618 380021 ## [8] 491691 90016 131124 347334 23204 170674 75628 ## [15] 155417 232412 107801 93343 523824 21955 90423 ## [22] 288626 296030 296192 221850 17311 54256 25840 ## [29] 314267 109406 281450 326687 609406 463679 359694 ## [36] 341410 201221 60781 261991 217832 37723 1751807 ## [43] 223445 398042 146612 29611 198055 69129 363095 ## [50] 180480 31364 265685 79083 178335 404738 222898 ## [57] 245535 30821 110718 9694 207866 119663 238360 ## [64] 14374 27199 81597 40572 23611 141400 133883 ## [71] 1361447 17082 1221259 NA 65459 66280 267373 ## [78] 79083 490693 405193 40916 85642 3800 166598 ## [85] 793656 19207 4041 69395 86904 228519 2250723 ## [92] 432163 62292 397409 42107 369714 5554 13782 ## [99] 8399 358322 NA 23689 3411 13982 7537 ## [106] 5143 93588 385568 63712 NA 6008 457013 ## [113] NA 522607 184810 282378 40145 188773 12464 ## [120] 1007562 3030 22759 138387 103900 192906 NA ## [127] 54405 105874 112704 2181057 132460 582565 26694 ## [134] 54186 49103 3929 NA NA 94092 40199 ## [141] 175263 1164908 37649 91549 49917 5090 488024 ## [148] 6723 NA 13994 10662 5508 NA 18412 ## [155] 28192 152359 14608 NA 570 12355 261888 ## [162] 1950 560273 NA 7039 1142881 399083 13708 ## [169] 183196 2851990 33944 81958 237462 183557 4722 ## [176] NA 5167 3573 1319877 6070 NA 3497436 ## [183] NA 227514 269240 220986 14300 1799189 813392 ## [190] 171487 90446 52734 139813 177175 482002 48394 ## [197] 93823 78473 212118 6489 18575 55451 11059 ## [204] 62537 11234 NA 80827 229191 1264263 6484 ## [211] 498443 115562 NA NA 176018 NA 103022 ## [218] NA NA 111163 ggplot(t2, aes(`Tot Cases/1M pop`,`Tests/1M pop`,))+geom_point()+facet_wrap(~Continent) ## Warning: Removed 20 rows containing missing values (geom_point). 2.6 Ejemplo: Ultracasas Motivación: Explorar el mercado de casas en Bolivia ultra&lt;-read_html(&quot;https://www.ultracasas.com/buscar/casa-en-venta--en--la-paz---la-paz?page=1&quot;) #titulo aux1&lt;-html_text(html_nodes(ultra,&quot;.line-height-30px&quot;))[-1] #zona aux2&lt;-html_text(html_nodes(ultra,&quot;.text-ellipsis&quot;))[-1][seq(2,24,2)] #precio aux3&lt;-html_text(html_nodes(ultra,&quot;h4&quot;))[-c(1:2,15:17)] #metros, dormitorios, baños aux&lt;-html_text(html_nodes(ultra,&quot;.icon-default-color&quot;)) aux4&lt;-aux[seq(1,36,3)]#dormitorios aux5&lt;-aux[seq(2,36,3)]#baños/duchas aux6&lt;-aux[seq(3,36,3)]#metros2 #base inicial bd0&lt;-data.frame(titulo=aux1,zona=aux2,precio=aux3,bed=aux4,bath=aux5,m2=aux6) for(i in 2:26){ print(i) www&lt;-paste0(&quot;https://www.ultracasas.com/buscar/casa-en-venta--en--la-paz---la-paz?page=&quot;,i) ultra&lt;-read_html(www) #titulo aux1&lt;-html_text(html_nodes(ultra,&quot;.line-height-30px&quot;))[-1] #zona aux2&lt;-html_text(html_nodes(ultra,&quot;.text-ellipsis&quot;))[-1][seq(2,24,2)] #precio aux3&lt;-html_text(html_nodes(ultra,&quot;h4&quot;))[-c(1:2,15:17)] #metros, dormitorios, baños aux&lt;-html_text(html_nodes(ultra,&quot;.icon-default-color&quot;)) aux4&lt;-aux[seq(1,36,3)]#dormitorios aux5&lt;-aux[seq(2,36,3)]#baños/duchas aux6&lt;-aux[seq(3,36,3)]#metros2 #base inicial bd1&lt;-data.frame(titulo=aux1,zona=aux2,precio=aux3,bed=aux4,bath=aux5,m2=aux6) bd0&lt;-rbind(bd0,bd1) } ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 ## [1] 8 ## [1] 9 ## [1] 10 ## [1] 11 ## [1] 12 ## [1] 13 ## [1] 14 ## [1] 15 ## [1] 16 ## [1] 17 ## [1] 18 ## [1] 19 ## [1] 20 ## [1] 21 ## [1] 22 ## [1] 23 ## [1] 24 ## [1] 25 ## [1] 26 2.7 Ejemplo: Ketal Una de las potencialidades del scraping es poder rescatar información contenida en páginas web, según la estructura de esta, en esta sección se presenta un ejemplo del código y la recolección de información que puede realizarse. Para esto se eligió la página de supermercados Ketal que es abierta y publica los precios de sus distintos productos de forma categorizada por tipo de productos. El estudio de esta pagina tiene el principal objetivo de mostrar el proceso de extracción, el uso potencial de la información esta relacionada a tener la variación de los precios y para futuros estudios se podría completar el ejercicio incluyendo la parte de la demanda mediante la encuesta de presupuestos familiares y poder así pronosticar la inflación dentro de este sector y para la población potencial que acude a supermercados. El ejemplo presenta el proceso del scraping para 8 grupo de productos: Carne de aves Carne de cerdo Carne de res Pescado Leches Aceites Frutas y verduras Dulces Se emplea la librería rvest, que es un paquete diseña para para facilitar la descarga y la manipulación de HTML y XML. También se utiliza la extensión Selector Gadget https://selectorgadget.com, este permite la identificación de las estructuras de las páginas web. El objetivo del raspado es obtener el nombre del producto, su código, y el precio. Utilizando el selector gadget se identifica que en \\(.product-name\\) se guardan los nombres, en \\(.price.product-price\\) el precio del producto y en \\(data-id-product\\) el identificador del producto. En R el código es: #librerías para el raspado library(rvest) #Enlaces de los grupos de productos ave &lt;- read_html(&quot;https://www.ketal.com.bo/209-aves&quot;) cerdo&lt;-read_html(&quot;https://www.ketal.com.bo/208-cerdo&quot;) res&lt;-read_html(&quot;https://www.ketal.com.bo/210-res&quot;) fish&lt;-read_html(&quot;https://www.ketal.com.bo/17060162-pescados&quot;) leches&lt;-read_html(&quot;https://www.ketal.com.bo/53-leches-liquidas&quot;) aceites&lt;-read_html(&quot;https://www.ketal.com.bo/272-aceites-y-vinagres&quot;) frutaver&lt;-read_html( &quot;https://www.ketal.com.bo/50-frutas-y-verduras?id_category=50&amp;n=150&quot;) dulce&lt;-read_html( &quot;https://www.ketal.com.bo/15-chocolates-y-golosinas?id_category=15&amp;n=150&quot;) #definición de la base de datos que guarda la información bd&lt;-NULL #Algoritmo del raspado de los 8 productos k&lt;-1 for(i in list(ave,cerdo,res,fish,leches,aceites,frutaver,dulce)){ print(k) name&lt;-html_attr(html_nodes(i,&quot;.product-name&quot;),&quot;title&quot;)[-1] precio&lt;-html_text( html_nodes(i,&quot;.price.product-price&quot;),trim=T)[seq(1,length(name)*2,2)] code&lt;-html_attr(html_nodes(i,&quot;.add_to_compare&quot;),&quot;data-id-product&quot;) bd&lt;-rbind(bd,data.frame(name,precio,code)) k&lt;-k+1 } #se identifica el día day&lt;-substr(Sys.time(),1,10) #se guarda la informacion en base al dia del raspado bd$day&lt;-day save(bd,file = paste0(day,&quot;.RData&quot;)) 2.8 Ejemplo: Ministerio de Educación library(rvest) library(foreign) library(xlsx) ######################## bd&lt;-read.dta(&quot;mmUE.dta&quot;) rue&lt;-bd$cod_ue_det rue&lt;-unlist(strsplit(rue,&quot;;&quot;)) bd&lt;-data.frame(matrix(0,1,78)) k&lt;-1 for(i in 325:length(rue)){ print(i) urlmin&lt;-paste0(&quot;http://seie.minedu.gob.bo/reportes/mapas_unidades_educativas/ficha/ver/&quot;,rue[i]) ue &lt;- read_html(urlmin,encoding = &quot;UTF-8&quot;) jj&lt;-grepl(&quot;Error was&quot;,unlist(strsplit(html_text(html_nodes(ue,&quot;strong&quot;))[1:2],&quot;:&quot;))[c(2)]) if(jj==T){ next() } #nombre id (2) bd[k,1:2]&lt;-unlist(strsplit(html_text(html_nodes(ue,&quot;strong&quot;))[1:2],&quot;:&quot;))[c(2,4)] #Informacion administrativa (12) bd[k,3:14]&lt;-html_text(html_nodes(ue,&quot;dd&quot;)) #matricula #promovidos #reprobacion #abandono # (60) #matricula for(j in 1:4){ bd[k,(15*j):(14+15*j)]&lt;-as.integer(unlist(html_table(ue)[j])[-c(1:3)]) } #servicios (4) bd[k,75:78]&lt;-html_text(html_nodes(ue,&quot;strong&quot;))[18:21] k&lt;-k+1 } write.xlsx(bd,&quot;ue2017.xlsx&quot;,row.names = F) 2.9 Ejemplo, informacion COVID-19 en Bolivia library(rvest) #paso 0: Estadísticas del covid-19 en Bolivia #paso 1: cargar la www de interés bs&lt;-read_html(&quot;https://www.boliviasegura.gob.bo/&quot;) ww&lt;-read_html(&quot;https://www.worldometers.info/coronavirus/&quot;) #paso 2: Raspar la información de interés node0&lt;-html_nodes(bs,&quot;h1&quot;) node1&lt;-html_text(node0) node2&lt;-as.numeric(node1) barplot(node2,legend.text = c(&quot;Confirmados&quot;,&quot;Recuperados&quot;,&quot;Muertes&quot;)) aux&lt;-html_nodes(bs,&quot;.mapanuevos&quot;) #paso 3: trabajando con tablas tabla&lt;-html_table(bs,fill = T) tabla[[2]] #paso 4: limpieza #paso 5: Análisis 2.10 APIs Conocida también por la sigla API,“application programming interface”, es un conjunto de sub rutinas, funciones y procedimientos (o métodos, en la programación orientada a objetos) que ofrece cierta biblioteca para ser utilizado por otro software. Ofrece una entrada a los datos que distribuye el API. 2.10.1 API Banco Mundial La API ofrece acceso a las estadísticas que genera el Banco Mundial, existe un set extenso de estadísticas de la mayoría de los países. La API ofrece mas de 16000 indicadores de series de tiempo, muchos de los indicadores tienen una cobertura de 50 años. La API incluye el acceso a 45 bases de datos, incluyendo: World Development Indicators International Debt Statistcs Doing Business Human Capital Index Subnational Poverty Y otros, ver En R se puede acceder mediante la librería wbstats. #install.packages(&quot;wbstats&quot;) #paso 1, instalar el rtools: https://cran.r-project.org/bin/windows/Rtools/ #paso 2, instalar devtools install.packages(&quot;devtools&quot;) #paso 3, cargar el devtools library(devtools) #paso4, instalar wbstats install_github(&quot;nset-ornl/wbstats&quot;) library(wbstats) #acceso a todos los indicadores disponibles wbindex&lt;-wbindicators(&quot;es&quot;) #acceso al catalogo de datos wbcat&lt;-wbdatacatalog() #búsqueda de indicador index_edu&lt;-wbsearch(pattern = &quot;education&quot;) index_sal&lt;-wbsearch(pattern = &quot;health&quot;) # ver los países y sus códigos wbpais&lt;-wbcountries() #Comando para extraer los indicadores wb(country = &quot;BOL&quot;,indicator = &quot;NY.GDP.MKTP.CD&quot;, startdate = 2000, enddate = 2016) 2.10.2 API Google trends La libraría gtrendsR permite acceder a la API de google trends, esta permite acceder a las tendencias de búsqueda que se realizan mediante el motor de búsqueda de Google. install.packages(&quot;gtrendsR&quot;) library(gtrendsR) aux&lt;-countries res &lt;- gtrends(&quot;Coronavirus&quot;, geo = c(&quot;BO&quot;)) bd_cov&lt;-res$interest_over_time library(ggplot2) bd_cov$hits&lt;-as.numeric(gsub(&quot;&lt;&quot;,&quot;&quot;,bd_cov$hits)) ggplot(bd_cov,aes(date,hits))+geom_line() plot(res) eleccion2020&lt;-gtrends(c(&quot;Luis Arce&quot;,&quot;Carlos Mesa&quot;,&quot;Fernando Camacho&quot;), geo = c(&quot;BO&quot;),time=&quot;today 3-m&quot;) plot(eleccion2020) library(knitr) kable(head(res$interest_over_time,10)) 2.10.3 API Google Maps R tiene la librari googleway que permite tener acceso a la API de google maps, es necesario una llave de autentificación. Para la llave seguir los pasos en el enlace #install.packages(&quot;googleway&quot;) library(googleway) map_key &lt;- &#39;xxx&#39; google_map(key = map_key,location = c(-16.5030161,-68.1292566),zoom = 8) %&gt;% add_traffic() 2.11 Ejercicios Propuestos Extraer la fecha, y el precio de compra y venta del dolar de la página https://www.bcb.gob.bo Usando la página https://www.trabajopolis.bo/ seleccionar un departamento y armar una base de dato de ofertas laborales Armar una base de datos en base a la página https://www.infocasas.com.bo Explorar librarias API con acceso a Youtube y encontrar los 10 videos con más visualizaciones que incluyan a Bolivia en su titulo. Explorar la libraria gtrendsR y explorar en que meses en Bolivia es mas frecuente la búsqueda de Usar la informacion de worldometers y generar un gráfico de contagios por millon de los distintos paises. "],
["introducción-al-big-data.html", "Chapter 3 Introducción al Big Data 3.1 Definiendo al Big Data 3.2 Las 5 V en el Big Data 3.3 Ciclo de vida de un proyecto de análisis de datos 3.4 Inferencia y Big Data 3.5 Calidad de dato y Big Data 3.6 Captura y preservación 3.7 Análisis y modelado 3.8 Inferencia y ética 3.9 Ejercicios Propuestos", " Chapter 3 Introducción al Big Data 3.1 Definiendo al Big Data De forma simple el Big Data se define como: DATA&gt;RAM O de forma mas literal “cualquier cosa demasiado grande para caber en su computadora.” La Asociación Americana de Investigación de Opinión Pública menciona: “El término” Big Data “es una descripción imprecisa de un conjunto rico y complicado de características, prácticas, técnicas, cuestiones éticas y resultados, todos asociados con los datos”. 3.2 Las 5 V en el Big Data Velocidad Volumen Valor Variedad Veracidad 3.3 Ciclo de vida de un proyecto de análisis de datos Se pueden identificar 4 fases: Clarify: (Clarificar) Llegar a familiarizarse con los datos Develop: (Desarrollar) Crear un modelop de trabajo Productize: (Producir) Automatizar e integrar Publish: (Publicar) Socializar Estas fases pueden contener nodos adicionales según el proyecto: Subset: Extraer los datos a explorar, los datos de trabajo Clarify: (Clarificar) Llegar a familiarizarse con los datos Develop: (Desarrollar) Crear un modelop de trabajo *. Scale Up: Generalizar a la base de datos completa Productize: (Producir) Automatizar e integrar Publish: (Publicar) Socializar Otros ciclos de trabajo puedes ser: Identificar el problema Diseño del requerimiento de datos Procesar los datos Desarrollo del análisis sobre los datos Visualizar los datos 3.4 Inferencia y Big Data El objetivo de la inferencia es poder decir algo de la población objetivo a partir de la información disponibles. Se debe tener en cuenta los tipos de estudio provenientes; ya sean de encuestas probabilísticas, diseño experimentales o estudios de observación. Se debe estar seguro de la calidad de la base de datos proveniente, ya sean estos los errores de muestreo, procesos de calibración, ponderación, post estratificación en el caso de muestreo o el el propensity score y la estratificación principal para repara diseño experimentales rotos. Se puede distinguir tres metas en el proceso de inferencia: Descriptivo Causal Predictivo 3.4.1 Descriptivo La estadística descriptiva puede ser; (1) a un nivel simple de descripción de una base de datos sin la búsqueda de querer expandir los resultados (registros administrativos, censos, estudios de observación) o (2) para encuestas probabilisticas, realizar las estimaciones de la muestra con sus respectivos errores muestrales y a partir de estas estimaciones describir a la población Ejemplo: El INE estima a partir de la EH-2018 que la incidencia de pobreza moderadara en Bolivia para el 2018 alcanza el …% Ejemplos como este muestra que el propósito es puramente descriptivo en cuanto a la pobreza. 3.4.2 Causal Muchos investigadores buscan explorar hipótesis, aveces originadas en la teoria o en alguna relación observada de forma empírica, con la idea central de permitir la inferencia causal. La data para esto proviene de diseños experimentales o fuertes estudios no experimentales (cuasi-experimentales), el interés de estos estudios es principalmente encontrar el efecto de de una variable entre otra. \\[X \\rightarrow Y\\] Aspecto que es logrado fácilmente mediante los diseños experimentales. En este tipo de estudio el componente descriptivo no es tan importante como el metodo para identificar la causalidad. Es importante diferenciar en este punto la causalidad de la correlación. Ejemplo: (3ie) Este informe se basa en un estudio de Dupas, Duflo y Kremer que se realizó en colaboración con el gobierno de Ghana. El estudio examinó los impactos a mediano plazo de otorgar becas de cuatro años a estudiantes que no podían matricularse en escuelas secundarias superiores (SHS) debido a limitaciones financieras. Los investigadores encontraron que el programa de becas tuvo un impacto significativo en el logro educativo y las habilidades cognitivas, particularmente entre las niñas. El programa también tuvo un mayor impacto en las tasas de finalización de SHS de las niñas en términos porcentuales. Ejemplo: UDAPE el 2013 realizó el calculo del impacto de la renta dignidad en Bolivia empleando el método de regresión discontinua un método cuasi-experimental. Una de las debilidades principales de estos estudios es la falta o poca de validez externa, es decir, es dificil poder generalizar los resultados. Métodos cuasi-experimentales Diferencia en diferencia Propensity Score Matching (PSM) Probit. Variables intrumentales Modelos estructurales Regresión Discontinua. 3.4.3 Predictivo El pronóstico o predicción tiene un rol diferenciado según la ciencia de aplicación, teniendo un rol significativo dentro de las estadística oficiales, principalmente en lo social (proyecciones poblacionales) y económico (indicadores macroeconómicos), principalmente para hacedores de política, gobernates y empresarios. Similar a En la configuración de inferencia causal, es de suma importancia que conozcamos el proceso que generó los datos, y podemos descartar cualquier mecanismo de selección sistemática desconocido o no observado. Ejemplo: El Institute of Global Health, Faculty of Medicine, University of Geneva tiene una página web que realiza pronosticos por país para los casos de COVID-19. Enlace 3.5 Calidad de dato y Big Data La mayoría de los datos en el mundo real son ruidosos, inconsistentes y adolecen de valores perdidos, independientemente de su origen. Incluso si la recopilación de datos es barata, los costos de crear datos de alta calidad a partir de la fuente (limpieza, conservación, estandarización e integración) son considerables. La calidad de los datos se puede caracterizar de múltiples maneras: Precisión: ¿qué tan precisos son los valores de los atributos en los datos? Integridad: ¿están completos los datos? Consistencia: ¿Cuán consistentes son los valores en y entre las bases de datos? Puntualidad: ¿qué tan oportunos son los datos? Accesibilidad: ¿están disponibles todas las variables para el análisis? Los cientistas de datos tienen décadas de experiencia en la transformación de datos desordenados, ruidosos y no estructurados en un conjunto de datos bien definido, claramente estructurado y probado en calidad. El pre procesamiento es un proceso complejo y que lleva mucho tiempo porque es práctico: requiere juicio y no puede automatizarse de manera efectiva. Un flujo de trabajo típico comprende múltiples pasos desde la definición de datos hasta el análisis y termina con el filtrado. Es difícil exagerar el valor del pre-procesamiento para cualquier análisis de datos, pero esto es particularmente cierto en big data. Los datos deben analizarse, estandarizarse, deduplicarse y normalizarse. Análisis (parsing): Exploración de datos Estandarización (Standardization): Identificar variables que requieren transformación y ajustes. Duplicación: Consiste en eliminar registros redundantes Normalización (Normalization): Es el proceso de garantizar que los campos que se comparan entre archivos sean lo más similares posible en el sentido de que podrían haber sido generados por el mismo proceso. Como mínimo, se deben aplicar las mismas reglas de estandarización a ambos archivos. 3.6 Captura y preservación Se refiere al proceso de obtener la información de las distintas fuentes posibles y luego pasar a un proceso de preservacion. 3.6.1 Fuentes convencionales Estas estan basadas en la información que se distribuye de forma tradicional mediante bases de datos estructuradas, normalmente estas las distribuyen instituciones con amplios conocimientos en la gestion de bases de datos, para el caso de Bolivia se puede citar algunas: Instituto Nacional de Estadística SNIS UDAPE 3.6.2 Datos web y APIs Referirse al capítulo 2 de scraping web, estos son los mecanismos para extraer información en internet 3.6.3 Record Linkage Se refiere al proceso de concatenar o unir observaciones dispuestas en múltiples bases de datos. Puede ser usado para compensar la falta de información Se usa para crear estudios longitudinales Se pueden armar seudo-paneles Esto permite mejorar la cobertura (append), ampliar las temáticas de estudio (merge). Pre-procesamiento Matching: Une información a partir de una clave, existen muchos problemas con claves tipo texto. Aproximaciones a reglas para hacer math: Definir criterios para posibilitar el match basados en reglas, distancias cercanas, etc. Match basados en probabilidad: Fellegi–Sunter method 3.6.4 Bases de datos Una vez que los datos fueron recolectados y enlazados entre diferentes fuentes, es necesario guardar la información. Ahora se discute las alternativas para guardar la información. DBMS (databasemanagement systems) Sistema de gestión de base de datos: Decidir que herramienta usar segun la dimensión de los archivos. Bases de datos espaciales Múltiples formatos: https://juliael.carto.com/ 3.6.5 Programando con Big Data MapReduce: map, shuffle y reduce Apache hadoop MapReduce (Hadoop Distributed File System HDFS) Apache Spark 3.7 Análisis y modelado 3.7.1 Machine learning ¿Machine learning = Statistics? Verán que muchos métodos discutidos a lo largo de su formación como estadísticos aparecen dentro del maching learning y que son llamados con otros nombres. Al pensar en machine learning debemos asociarlo directamente con procesos computacionales, muchos otros conceptos giran al rededor de esta idea como la inteligencia artificial. Proceso de machine learning hoy: Permiten manejar autos de forma autónoma Puede recomendar libros, amistades, música, etc Identificar drogas, proteínas y ciertos génes Se usa para detectar ciertos tipos de cáncer y otras enfermedades médicas Ayudan a conocer que estudiantes necesitan un apoyo adicional Ayudan a persuadir por que candidato votar en las elecciones. 3.7.1.1 El proceso del machine learning Entender el problema y la meta Formular esto como un problema de machine learning Explorar y preparar los datos Feature engineeing Selección del método Evaluación Deployment 3.7.1.2 Formulación del problema En ML existen 2 grandes categorías Aprendizaje supervisado: Existe una \\(Y\\) que queremos predecir o clasificar a partir de los datos. El fin es el ajuste y la generalización * Clasificación (\\(Y\\) cualitativa) * Predicción * Regresión (\\(Y\\) cuantitativa) Aprendizaje no supervisado: No existe una variable objetivo, se quiere conocer, entender las asociaciones y patrones naturales en los datos. * Clustering * PCA, MCA 3.7.2 Análisis de texto: Entendiendo lo que la gente escribe Clasificación de documentos Análisis de sentimientos Etiquetado de discursos 3.7.3 Networks 3.8 Inferencia y ética 3.8.1 Información y visualización Los usuarios pueden escanear, reconocer, comprender y recordar representaciones visualmente estructuradas más rápidamente de lo que pueden procesar representaciones no estructuradas La ciencia de la visualización se basa en múltiples campos, como la psicología perceptiva, las estadísticas y el diseño gráfico para presentar información La efectividad de una visualización depende tanto de las necesidades de análisis como de los objetivos de diseño. El diseño, el desarrollo y la evaluación de una visualización se guían por la comprensión de los antecedentes y las metas del público objetivo. El desarrollo de una visualización efectiva es un proceso iterativo que generalmente incluye los siguientes pasos: Especificar las necesidades del usuario, tareas, requisitos de accesibilidad y criterios para el éxito. Preparar datos (limpiar, transformar). Diseñar representaciones visuales. Interacción de diseño. Planifique el intercambio de ideas, procedencia. Prototipo / evaluación, incluidas las pruebas de usabilidad. Implementar (supervisar el uso, proporcionar soporte al usuario, gestionar el proceso de revisión). 3.8.1.1 Dashboards 3.8.1.2 Elementos 3.8.1.3 Datos espaciales Datos temporales Datos jerarquicos Datos de redes Datos de texto Tarea: resumir los siguientes puntos del libro: Big Data and Social Science, Ian Foster. 3.8.2 Error e inferencia 3.8.3 Privacidad y confidencialidad 3.8.4 Workbooks 3.9 Ejercicios Propuestos Explorar los métodos cuasi-experimentales que existen Buscar informacion respecto a: los matriculados en educacion regular y universidad por año y departamento en Bolivia Empleando la fuente anterior, generar en R el có digo que cargue el archivo encontrado Buscar dos papers (1) donde se uso machine learning y (2) análisis de texto y comentar con al menos 5000 caractéres Buscar ejemplos (al menos uno) de bases de datos, páginas web u otros asociados a datos que no respeten los principios de privacidad y confidencialidad. "],
["big-data-en-r.html", "Chapter 4 Big Data en R 4.1 Limitaciones en R 4.2 A los límites de la memoria y más allá 4.3 Parallel R", " Chapter 4 Big Data en R R en un software que se adapta perfectamente con las fases del ciclo de vida de los proyectos de datos, ofreciendo herramientas para cada una de ellas; Entre las mas interesantes, R Markdown, Shiny, sweave, etc. Adicionalmente existen multiples librerias que permite trabajar con otros software, principalmente los asociados a bases de datos. En el capitulo 1 se enfatiza el uso de R para el manejo, procesamiento y análisis de bases de datos, sumado a esto el conocimiento de distintas estadísticas en base a los cursos de formación, en este capitulo se buscar unir ambos topicos y explicar como usar el poder del modelado matematico y de datos en R, considerando bases de datos grandes, esto sin necesidad de recurrir a otros equipos. Lo que se espera a partir de este capitulo es: Conocer las limitaciones de R para el Big Data y como ellas pueden resolverse El uso de las librerias ff, ffbase, ffbase2, y bigmemory, para el manejo de la memoria del equipo El uso de metodos estadisticos para objetos grandes en R, atraves de las librerias bigglm y ffbase Mejorar la velocidad del procesamiento de datos con librerias que permiten la computación paralela (parallel computing) Manipulación de datos más rápidos con el uso de la libreria data.table 4.1 Limitaciones en R A manera que uno va aprendiendo mas del R en la universidad o en el trabajo se valora cada vez mas la flexibilidad de R, su constante crecimiento, la ventaja de ser de codigo abierto, etc. Sin embargo, es importante tener en cuenta las limitaciones de R: Los datos se ajustan a la RAM R es generalmente muy lento comparado con otros lenguajes 4.1.1 Memoria R permite trabajar con bases de datos que no superen el 50-60% de la RAM de la computadora Existen soluciones en R sin tener que recurrir aun a plataformas como; Microsoft Azure, Amazon EC2, or Google Cloud Platform. Existe tambien la opción de trabajar con RStudio-Cloud, https://rstudio.cloud/plans/free 4.1.2 Velocidad de procesamiento R se considera un lenguaje interpretado y, como tal, su ejecución de código más lenta viene por definición. Procesos linea por linea El bajo rendimiento del código R puede deberse al hecho de que el lenguaje no está formalmente definido 4.2 A los límites de la memoria y más allá 4.2.1 Transformacion y agregación de datos con la librería ff y ffbase rm(list=ls()) install.packages(&quot;ff&quot;) install.packages(&quot;ffbase&quot;) #install.packages(&quot;ffbase2&quot;) / github library(ff) library(ffbase) setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata&quot;) bd1&lt;-read.csv(&quot;200613COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T) object.size(bd1)/1000000 head(bd1) #paso 1: tener un directorio para archivos temporales de ff system(&quot;mkdir ffdf&quot;) #paso 2: definir la carpeta temporal options(fftempdir=&quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\ffdf&quot;) #paso 3: Cargar la base de datos bd2&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, next.rows=100000,colClasses=NA,VERBOSE=F) bd3&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, first.rows=-1,colClasses=NA,VERBOSE=T) bd4&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, next.rows=10000,colClasses=NA,VERBOSE=T) object.size(bd1)/1000000 # CSV DE R object.size(bd2)/1000000 # CSV.FFDF 100000 object.size(bd3)/1000000 # CSV.FFDF TODO object.size(bd4)/1000000 # CSV.FFDF 1000 str(bd2) #USO DE LAS FFDF table(bd2$SEXO) barplot(table(bd4$SEXO)) install.packages(&quot;tictoc&quot;) library(tictoc) tic(&quot;R&quot;) table(bd1$EDAD,bd1$SEXO) toc() tic(&quot;ffdf&quot;) table(bd2$EDAD,bd2$SEXO) toc() tic(&quot;ffdf table&quot;) table.ff(bd2$EDAD,bd2$SEXO) toc() #R: 1 G -&gt; #FFDF: 200 M -&gt; class(bd1) class(bd2) #bd5&lt;-as.data.frame.ffdf(bd2) #apropos(&quot;as.data.frame&quot;) #library(Hmisc) #describe(bd1) #describe(bd5) #object.size(bd5) #object.size(bd1) #object.size(bd2) Funciones para objetos ff #sobre la bd2 library(doBy) library(dplyr) #res0&lt;-bd1 %&gt;% group_by(PAIS_NACIONALIDAD,SEXO) %&gt;% #summarise(mean(EDAD)) #bd2df&lt;-as.data.frame.ffdf(bd2) #object.size(bd2df) #bd2df %&gt;% group_by(PAIS_NACIONALIDAD,SEXO) %&gt;% summarise(mean(EDAD)) #summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=bd1) #summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=bd2df) tapply(bd1$EDAD, bd1$PAIS_NACIONALIDAD, mean) tapply(bd2$EDAD, bd2$PAIS_NACIONALIDAD, mean) res&lt;-ffdfdply(bd2,split=bd2$PAIS_NACIONALIDAD,FUN=function(x){ summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=x) }) object.size(res0) object.size(res) ss1&lt;-subset(bd1,EDAD&gt;60) ss2&lt;-subset(bd2,EDAD&gt;60) ss3&lt;-subset.ffdf(bd2,EDAD&gt;60) #memoria en R object.size(&quot;a&quot;) memory.size() memory.limit() (memory.size()/memory.limit())*100 memory.profile() #guardar objetos ffdf save.ffdf(res) rm(res) load.ffdf(&quot;ffdb&quot;) #convirtiendo a ff dd&lt;-as.ffdf(bd1) #forma lenta bd1$FECHA_ACTUALIZACION&lt;-as.factor(bd1$FECHA_ACTUALIZACION) str(bd1) #forma rapida i &lt;- sapply(bd1, is.character) bd1[,i] &lt;- lapply(bd1[,i], as.factor) str(bd1) dd&lt;-as.ffdf(bd1) mx0&lt;-read.csv(&quot;201115COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T) object.size(mx0)/10^6 mx1&lt;-read.csv.ffdf(file=&quot;201115COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, first.rows=-1,colClasses=NA,VERBOSE=T) object.size(mx0)/10^6 object.size(mx1)/10^6 table(mx0$SEXO) table(mx1$SEXO) 4.2.2 Modelos GLM con la las librerias ff y ffbase install.packages(&quot;biglm&quot;) library(ff) library(ffbase) library(biglm) model1&lt;-lm(EDAD~UCI,data = bd1) model2&lt;-lm(EDAD~UCI,data = bd2) model3&lt;-bigglm.ffdf(EDAD~SEXO,data = dd) summary(model1) summary(model2) summary(model3) 4.2.3 Expandiendo la memoria con la libreria bigmemory rm(list=ls()) library(bigmemory) #limitación de bigmemory, todos los variables de la base de datos de interés deben ser numéricas. load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\oct20.RData&quot;) bd1&lt;-computo[,13:25] str(bd1) object.size(bd1)/10^6 bd2&lt;-as.big.matrix(as.matrix(bd1)) object.size(bd1)/10^6 object.size(bd2)/10^6 object.size(bd1)/object.size(bd2) class(bd2) class(bd1) #xx&lt;-read.big.matrix(&quot;.csv&quot;,sep=&quot;,&quot;) #alternativamente read.big.matrix dim(bd2) dimnames(bd2) head(bd2) summary(bd1) summary(bd2) mean(bd2[,1]) summary(bd2[,2]) names(bd2) dimnames(bd2) lm(CC~MNR,data=bd2) library(bigtabulate) library(biganalytics) library(biglm) library(bigstatsr) library(bigalgebra) # salio de R el 2020 library(bigpca)# salio el 2018 de R reg0&lt;-bigglm.big.matrix(CC~MNR,data=bd2) summary(reg0) reg&lt;-bigglm.big.matrix(CC~FPV,data=bd2) summary(reg) class(bd2) 4.3 Parallel R Esta sección se enfoca en los métodos de computación paralelo, de tal forma que se pueda controlara la paralelizacion en una sola maquina. El objetivo es poder aprovechar los cores que tiene disponible los PC en la actualidad, R por defecto solo trabaja en una. Recursos online En R Un libro acerca del tema (aquí) Antes una función para calcular el tiempo de procesamiento. rm(list=ls()) teval&lt;-function(...){ gc() start&lt;-proc.time() result&lt;-eval(...) finish&lt;-proc.time() return(list(Duration=finish-start,Result=result)) } Veamos el rendimiento de lo visto antes. library(ff) library(ffbase) library(bigmemory) library(biganalytics) mr&lt;-matrix(rnorm(10000000*10),nrow = 10000000,ncol=10) mr&lt;-cbind(mr,binom=rbinom(10000000,5,0.7)) mbm&lt;-as.big.matrix(mr) mff&lt;-as.ffdf(as.data.frame(mr)) class(mr) class(mbm) class(mff) object.size(mr)/10^6 object.size(mbm)/10^6 object.size(mff)/10^6 teval(colMeans(mr)) teval(colmean(mbm)) #teval(colMeans(mff)) teval(apply(mr,2,mean)) teval(apply(mbm,2,mean)) teval(apply(mff,2,mean)) resul&lt;-list() teval( for(i in 1:ncol(mff)){ resul[[i]]&lt;-mean.ff(mff[[i]]) } ) teval(sapply(mff,mean)) teval(tapply(mr[,1], mr[,11], mean)) teval(tapply(mbm[,1], mbm[,11], mean)) teval(tapply(mff[[1]], mff[[11]], mean.ff)) aux&lt;-matrix(NA,6,10) teval( for(i in 1:10){ for(j in unique(mff[,11])){ aux[j+1,i]&lt;-mean(mff[mff[,11]==j,i]) } } ) 4.3.1 Libreria parallel Ahora sí, veamos la librería parallel y snow. Disponible desde la versión 2.14.0 Existen otras populares; multicore, snow (Simple Network of Workstations). La primera descontinuada y la segunda limitada. El manual https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf library(parallel) library(snow) #funciones propias mclapply()#no funciona completamente en windows parLapply() library(help=parallel) library(help=snow) #viendo los cores disponibles detectCores() #definiendo un cluster de trabajo #SOCK usa Rscript para lanzar más copias de R (en el mismo host u opcionalmente en otro lugar) #FORK makeForkCluster bifurca a los trabajadores en el host actual (que hereda el entorno de la sesión actual) (NO WINDOWS) cl &lt;- makeCluster(3, type = &quot;SOCK&quot;) stopCluster(cl) cl &lt;- makeCluster(2, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) cl &lt;- makeCluster(3, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) cl &lt;- makeCluster(4, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) #parApply(cl,mff[[1]],1,mean) #parApply(cl,mbm,2,mean) A&lt;-matrix(10^6,10) B&lt;-matrix(10^6,10) C&lt;-matrix(10^6,10) mclapply(X=list(A,B,C),FUN=mean,mc.preschedule = F,affinity.list = c(1,1,2)) mclapply(X=list(A,B,C),FUN=mean) "],
["r-y-spark.html", "Chapter 5 R y Spark 5.1 Introducción 5.2 Librería dplyr 5.3 Arrancando Spark 5.4 Análisis con Spark", " Chapter 5 R y Spark 5.1 Introducción En este capítulo emplearemos Spark junto con R para el procesamiento de datos. Spark es un framework que permite realizar procesos de división y paralelismo en un equipo o múltiples equipos. 5.1.1 Hadoop Google (2004) publicó un nuevo documento que describe cómo realizar operaciones en todo el Sistema de archivos de Google, un enfoque que se conoció como MapReduce. Como era de esperar, hay dos operaciones en MapReduce: map y reduce. La operación de map proporciona una forma arbitraria de transformar cada archivo en un nuevo archivo, mientras que la operación de reduce combina dos archivos. Ambas operaciones requieren un código de computadora personalizado, pero el marco MapReduce se encarga de ejecutarlas automáticamente en muchas computadoras a la vez. Estas dos operaciones son suficientes para procesar todos los datos disponibles en la web, al tiempo que proporcionan suficiente flexibilidad para extraer información significativa de la misma. 5.1.2 Spark En 2009, Apache Spark comenzó como un proyecto de investigación en AMPLab de UC Berkeley para mejorar MapReduce. Específicamente, Spark proporcionó un conjunto más rico de verbos más allá de MapReduce para facilitar la optimización del código que se ejecuta en múltiples máquinas. Spark también cargó datos en la memoria, lo que hace que las operaciones sean mucho más rápidas que el almacenamiento en disco de Hadoop Spark Componetes: Spark Core: Base donde se apoya el resto de los componentes Spark SQL: Procesamiento de dato estructurados y no estructurados Spark streaming: Procesamiento de datos en tiempo real. Spark MLLib: Machine learning Spark graph: Procesamiento de grafos Algunos conceptos importantes HDFS: Sistema de ficheros 5.1.3 Sparklyr Oficialmente, sparklyr es una interfaz R para Apache Spark. Está disponible en CRAN y funciona como cualquier otro paquete de CRAN, lo que significa que es independiente de las versiones de Spark, es fácil de instalar, sirve a la comunidad R, abarca otros paquetes y prácticas de la comunidad R, y así sucesivamente. Está alojado en GitHub y tiene licencia de Apache 2.0, que le permite clonar, modificar y contribuir de nuevo a este proyecto. 5.2 Librería dplyr #install.packages(&quot;dplyr&quot;) library(dplyr) load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) ## Obtener el promedio de edad por sexo de los jefes de hogar aux&lt;-eh19p[eh19p$s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot;,c(&quot;s02a_03&quot;,&quot;s02a_02&quot;)] aux&lt;-subset(eh19p,s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot;,c(&quot;s02a_03&quot;,&quot;s02a_02&quot;)) tapply(aux$s02a_03, aux$s02a_02, mean) #dplyr # %&gt;% operador pipe t1&lt;-eh19p %&gt;% filter(s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot;) %&gt;% group_by(s02a_02) %&gt;% summarise(mean(s02a_03)) t2&lt;-eh19p %&gt;% rename(sexo=s02a_02,edad=s02a_03,paren=s02a_05) %&gt;% filter(paren==&quot;1.JEFE O JEFA DEL HOGAR&quot;) %&gt;% group_by(sexo) %&gt;% summarise(edad=mean(edad)) ## Obtener el promedio de edad y años de educación por sexo de los jefes de hogar t3&lt;-eh19p %&gt;% rename(sexo=s02a_02,edad=s02a_03,paren=s02a_05) %&gt;% filter(paren==&quot;1.JEFE O JEFA DEL HOGAR&quot;) %&gt;% group_by(sexo) %&gt;% summarise(edad=mean(edad),aestudio=mean(aestudio,na.rm = T),med_aestudio=median(aestudio,na.rm = T),n=n()) eh19p %&gt;% rename(sexo=s02a_02,edad=s02a_03,paren=s02a_05) %&gt;% filter(paren==&quot;1.JEFE O JEFA DEL HOGAR&quot;) %&gt;% group_by(depto,area,sexo) %&gt;% summarise(edad=mean(edad),aestudio=mean(aestudio,na.rm = T),med_aestudio=median(aestudio,na.rm = T),n=n()) # ctr+mayus+m %&gt;% eh19p&lt;-eh19p %&gt;% mutate(jefe=(s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot;)) eh19p %&gt;% mutate(jefe=(s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot;)) %&gt;% group_by(jefe,s02a_02) %&gt;% summarise(mean(ylab,na.rm = T)) eh19p&lt;-eh19p %&gt;% rename(sexo=s02a_02) names(eh19p)[6]&lt;-&quot;edad&quot; # personas entre 15 y 30 años, por departamento y sexo, el promedio de horas a la semana de trabajo, años de educación y el mínimo y máximo de ingresos laborales y su rango eh19p %&gt;% filter(edad&gt;=15 &amp; edad&lt;=30) %&gt;% group_by(depto,sexo) %&gt;% summarise(horas=mean(tothrs,na.rm = T),edu=mean(aestudio,na.rm = T),min_ylab=min(ylab,na.rm = T),max_ylab=max(ylab,na.rm = T)) %&gt;% mutate(rango=max_ylab-min_ylab) eh19p %&gt;% slice_sample(n=20) %&gt;% select(edad) %&gt;% mutate(gedad=cut(edad,c(0,14,25,60,100),include.lowest =T)) eh19p %&gt;% mutate(gedad=cut(edad,c(0,5,14,25,60,100),include.lowest =T)) %&gt;% group_by(gedad) %&gt;% summarise(mean(aestudio,na.rm=T)) ## Por departamento, área y sexo de los jefes del hogar, mostrar el porcentaje de hogares con piso de tierra. aux&lt;-eh19p %&gt;% filter(jefe==T) %&gt;% select(folio,sexo,area,depto) aa&lt;-attributes(eh19v) aa$variable.labels unique(eh19v$s01a_09) aux2&lt;-eh19v %&gt;% mutate(tierra=s01a_09==&quot;1.TIERRA&quot;) %&gt;% select(folio,tierra) head(aux2) bd&lt;-merge(aux,aux2)## 1:1 (), n:1 , 1:n , n:n (X 30) (Y 20) XY 600 #x #Nombre materia nota ci #Juan m1 45 123 #Juan m2 57 123 #Juan m3 57 123 # #y #Nombre ci plan_estudio #Juan 123 2012 #Maria 124 2007 #merge(x,y,by=c(&quot;ci&quot;,&quot;Nombre&quot;),all=T) #xy #Nombre materia nota ci plan_estudio #Juan m1 45 123 2012 #Juan m2 57 123 2012 #Juan m3 57 123 2012 #Maria 124 2007 bd %&gt;% group_by(depto,area,sexo) %&gt;% summarise(Piso_Tierra=mean(tierra)*100) #se recomienda explorar el comando merge, y la variable folio es la llave para la unión de las bases de datos # Ejercicio. Obtener una tabla por departamento y área con la siguiente información # * % de hogares pobres extremos # * % de hogares con jefes del hogar mujeres # * Promedio de personas en el hogar load(url(&quot;https://github.com/AlvaroLimber/EST-384/raw/master/data/eh19.RData&quot;)) bd&lt;-eh19p %&gt;% filter(s02a_02==&quot;2.Mujer&quot;) %&gt;% mutate(pobreza=pext0==&quot;Pobre extremo&quot;,mujer=s02a_05==&quot;JEFE O JEFA DEL HOGAR&quot; ) %&gt;% group_by(depto,area) %&gt;% summarise(pobre=mean(pobreza)*100,mujer1=mean(mujer)) bd eh19p %&gt;% mutate(pobreza=(pext0==&quot;Pobre extremo&quot;)*1,jefa=(s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot; &amp; s02a_02==&quot;2.Mujer&quot;)*1, nper=1) %&gt;% group_by(depto,area,folio) %&gt;% summarise(nper=sum(nper),jefa=max(jefa),pobreza=max(pobreza)) %&gt;% group_by(depto,area) %&gt;% summarise(pobreza=mean(pobreza,na.rm=T)*100,jefa=mean(jefa)*100,nper=mean(nper)) t1&lt;-eh19p %&gt;% mutate(pobreza=(pext0==&quot;Pobre extremo&quot;)*1,jefa=(s02a_05==&quot;1.JEFE O JEFA DEL HOGAR&quot; &amp; s02a_02==&quot;2.Mujer&quot;)*1, nper=1) %&gt;% group_by(depto,area,folio) %&gt;% summarise(nper=sum(nper),jefa=max(jefa),pobreza=max(pobreza)) t2&lt;-t1 %&gt;% group_by(depto,area) %&gt;% summarise(pobreza=mean(pobreza,na.rm=T)*100,jefa=mean(jefa)*100,nper=mean(nper)) t2 xtable::xtable(t2) knitr::kable(t2) 5.3 Arrancando Spark 5.3.1 Prerequisitos R RStudio Java 8 system(&quot;java -version&quot;) 5.3.2 Instalación #instalar la librería sparlyr install.packages(&quot;sparklyr&quot;) #Habilitando la librería library(sparklyr) #versiones disponibles spark_available_versions() #versiones instaladas spark_installed_versions() #instalando spark desde R spark_install(&quot;3.0&quot;) #spark_install(version=&quot;2.3&quot;) 5.3.3 Soluciones a problemas de instalación install.packages(&quot;rJava&quot;) library(rJava) 5.3.4 Creando una sesión en Spark #inicia la sesión sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4.3&quot;) #sc &lt;- spark_connect(master = &quot;local&quot;, spark_home = &quot;C:\\\\Users\\\\ALVARO\\\\AppData\\\\Local\\\\spark&quot;,version=&quot;3.0.1&quot;) 5.3.5 Interface web spark_web(sc) http://localhost:4040/jobs/ http://127.0.0.1:4040/executors/ Jobs Stages Storage Environment Executors SQL 5.3.6 Configuración conf &lt;- spark_config() conf$`sparklyr.shell.driver-memory` &lt;- &quot;2G&quot; #conf$spark.memory.fraction &lt;- 0.8 #spark_home=&quot;C:\\\\Users\\\\ALVARO\\\\AppData\\\\Local\\\\spark\\\\spark-2.4.3-bin-hadoop2.7&quot; #conf$sparklyr.gateway.port &lt;- 9090 #conección #apagar conexión spark_disconnect(sc) spark_disconnect_all() 5.4 Análisis con Spark Pasos en el análisis de datos 5.4.1 Importando los datos Por lo general, importar significa que R leerá los archivos y los cargará en la memoria; cuando se usa Spark, los datos se importan a Spark, no a R. Colección de documentos #csv/importaciones sp_importaciones&lt;-spark_read_csv(sc, name=&quot;importaciones&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\importaciones\\\\importaciones_csv&quot;) ########### #esquema ########### top_rows &lt;- read.csv(&quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\importaciones\\\\importaciones_csv\\\\1993.csv&quot;,sep=&quot;,&quot;, nrows = 5) spec_with_r &lt;- sapply(top_rows, class) spec_with_r[6]&lt;-&quot;factor&quot; sp_importaciones2&lt;-spark_read_csv(sc, name=&quot;importaciones2&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\importaciones\\\\importaciones_csv&quot;, columns = spec_with_r) # memoria object.size(sp_importaciones) Comando copy_to m1&lt;-data.frame(matrix(rnorm(10^7),ncol=20)) object.size(m1)/(10^6) sp_m1&lt;-copy_to(sc,m1) rm(m1) load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\oct20.RData&quot;) object.size(computo)/10^6 sp_computo&lt;-copy_to(sc,computo) sp_eh19p&lt;-copy_to(sc,eh19p) rm(computo) Documento específico sp_covid&lt;-spark_read_csv(sc, name=&quot;covid&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\200614COVID19MEXICO.csv&quot;) sp_covid2&lt;-spark_read_csv(sc, name=&quot;covid2&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\201115COVID19MEXICO.csv&quot;) 5.4.2 Librería DPLYR (Gramática de manipulación de datos) https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html Eliminar un objeto en spark library(dplyr) db_drop_table(sc,&quot;importaciones2&quot;) db_drop_table(sc,&quot;m1&quot;) DPLYR en Spark sp_importaciones %&gt;% group_by(GESTION) %&gt;% count() sp_covid2 %&gt;% count() eh19v %&gt;% group_by(depto) %&gt;% count() # R sp_eh19v %&gt;% group_by(depto) %&gt;% count() # Spark library(dplyr) load(url(&quot;https://github.com/AlvaroLimber/EST-383/raw/master/data/oct20.RData&quot;)) names(computo)[1]&lt;-&quot;pais&quot; ############################## ### Columnas -&gt; variables ############################## #cambio de nombre de las variables computo&lt;-computo %&gt;% rename(mas=`MAS - IPSP`,cc=CC) computo&lt;-computo %&gt;% rename(idep=`Número departamento`,imun=5) names(computo) # Selección de variables bd1&lt;-computo %&gt;% select(pais,cc,mas) head(bd1) # Crear nuevas variables bd1&lt;-bd1 %&gt;% mutate(a=1,s_mas_cc=cc+mas,raiz_mas=sqrt(mas),lcc=log(cc)) bd1&lt;-bd1 %&gt;% mutate(mas100=mas&gt;100,mean_mas=mas&gt;mean(mas)) # ordenando las variables bd1&lt;-bd1 %&gt;% relocate(lcc:mas,before=a)#pendiente bd1[,c(&quot;cc&quot;,&quot;pais&quot;,&quot;a&quot;)] ############################## ### Filas-&gt; Observaciones ############################## #filtrado # elección presidencial solo Bolivia pv&lt;-computo %&gt;% filter(Elección==&quot;Presidente y Vicepresidente&quot;) pvbol&lt;-computo %&gt;% filter(Elección==&quot;Presidente y Vicepresidente&quot; &amp; pais==&quot;Bolivia&quot;) ############################## ### Resumen estadístico ############################## # summarise pvbol %&gt;% summarise(media_mas=mean(mas),media_cc=mean(cc),max_mas=max(mas),max_cc=max(cc)) pvbol %&gt;% summarise(p100mas=mean(mas&gt;100)*100,p100cc=mean(cc&gt;100)*100) # group_by pvbol %&gt;% group_by(Departamento) %&gt;% summarise(p100mas=mean(mas&gt;100)*100,p100cc=mean(cc&gt;100)*100) pvbol %&gt;% group_by(Departamento,Provincia) %&gt;% summarise(p100mas=mean(mas&gt;100)*100,p100cc=mean(cc&gt;100)*100) #count tally pvbol %&gt;% group_by(Departamento,Provincia) %&gt;% count() pvbol %&gt;% group_by(Departamento,Provincia) %&gt;% tally() # encadenamiento masivo pvbol %&gt;% filter(Departamento==&quot;Beni&quot;) %&gt;% select(mas,cc) %&gt;% plot() pvbol %&gt;% filter(Departamento==&quot;Beni&quot;) %&gt;% select(mas,cc) %&gt;% cor() 5.4.3 DPLYR en Spark # 0. Habilitar la librería library(sparklyr) library(dplyr) library(ggplot2) # 1. Conexión con Spark sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4.3&quot;) # 2. Definir la data (importación) sp_covid&lt;-spark_read_csv(sc, name=&quot;covid&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\200614COVID19MEXICO.csv&quot;)#específico sp_importaciones&lt;-spark_read_csv(sc, name=&quot;importaciones&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\importaciones\\\\importaciones_csv&quot;)#carpeta load(url(&quot;https://github.com/AlvaroLimber/EST-383/raw/master/data/oct20.RData&quot;)) pvbol&lt;-computo %&gt;% rename(eleccion=Elección) %&gt;% filter(eleccion==&quot;Presidente y Vicepresidente&quot;) sp_pvbol&lt;-copy_to(sc,pvbol) # 3. Exploración de los datos / (&quot;Modelado&quot;) names(pvbol) glimpse(sp_pvbol) glimpse(sp_importaciones) sdf_describe(sp_importaciones) sdf_describe(sp_pvbol,c(&quot;CC&quot;)) summary(pvbol$CC) sp_importaciones %&gt;% group_by(DESADU) %&gt;% count() a1&lt;-sp_importaciones %&gt;% group_by(DESADU) %&gt;% count() %&gt;% collect() # guardar la salida en R sp_a2&lt;-sp_importaciones %&gt;% group_by(DESADU) %&gt;% count() %&gt;% compute(&quot;a2&quot;) # guardar la salida en Spark sp_importaciones %&gt;% group_by(GESTION,DESADU) %&gt;% count() sp_importaciones %&gt;% group_by(DESADU, GESTION) %&gt;% count() sp_importaciones %&gt;% group_by(DESADU, GESTION) %&gt;% summarise() sp_importaciones %&gt;% count() # Generar un cuadro por gestión y producto, respecto el total de peso bruto y guardarlo en R. t1&lt;-sp_importaciones %&gt;% group_by(GESTION,NANDINA,DESNAN) %&gt;% summarise(peso_ton=sum(KILBRU,na.rm=T)/1000) %&gt;% arrange(GESTION,desc(peso_ton)) %&gt;% collect() library(DBI) aux&lt;-dbGetQuery(sc,&quot;Select * from importaciones limit 5&quot;) #lm(FOB~PAG,data=sp_importaciones) mod1&lt;-sp_importaciones %&gt;% select(FOB,PAG) %&gt;% ml_linear_regression(FOB~.) # 4. Análisis de los resultados # 5. Salida. Visualización ggplot(sp_importaciones,aes(DESDEPTO))+geom_bar()+facet_wrap(~GESTION) ggplot(sp_importaciones,aes(DESDEPTO))+geom_bar() #ggplot(sp_importaciones,aes(KILBRU))+geom_histogram() library(dbplot) sp_importaciones %&gt;% dbplot_bar(DESDEPTO) # Final. Cerrar la conexión a Spark spark_disconnect_all() ########################################################### # cargar el pvbol sp_pvbol&lt;-copy_to(sc,pvbol,name=&quot;elecciones&quot;) rm(pvbol) # interactuar con la base en spark sp_pvbol %&gt;% count() t0&lt;-sp_pvbol %&gt;% group_by(Departamento) %&gt;% count() #collect es para guardar la salida en la memoria de R t1&lt;-sp_pvbol %&gt;% group_by(Departamento) %&gt;% count() %&gt;% collect() # compute es para guardar la salida en la memoria de spark sp_t2&lt;-sp_pvbol %&gt;% group_by(Departamento) %&gt;% count() %&gt;% compute(&quot;t2&quot;) # consultas a spark sp_pvbol %&gt;% group_by(Departamento,Provincia) %&gt;% summarise(media_mas=mean(mas),media_cc=mean(cc),sd(mas)) # SQL show_query sp_pvbol %&gt;% group_by(Departamento) %&gt;% count() %&gt;% show_query() sp_pvbol %&gt;% group_by(Departamento) %&gt;% select(mas,cc) %&gt;% summarise_all(mean) %&gt;% show_query() # spark SQL library(DBI) e10&lt;-dbGetQuery(sc,&quot;Select * from elecciones limit 10&quot;) # figuras sp_pvbol %&gt;% select(cc) %&gt;% hist() library(ggplot2) ggplot(sp_pvbol,aes(cc))+geom_histogram() ggplot(sp_pvbol,aes(cc,mas))+geom_point() # recomendación ml load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\oct20.RData&quot;) head(computo) #filtrado aux&lt;-computo[computo$País==&quot;Bolivia&quot;,] #en R aux2&lt;-computo %&gt;% filter(País==&quot;Bolivia&quot;) #spark SQL library(DBI) top10 &lt;- dbGetQuery(sc, &quot;Select * from importaciones limit 10&quot;) top10 library(dplyr) sp_importaciones %&gt;% tally sp_covid2 %&gt;% tally oct20 %&gt;% tally covid_cache &lt;- sp_covid2 %&gt;% compute(&quot;covid_ch&quot;) covid_cache %&gt;% tally Para correr con las bases de la parte de importaciones #importaciones dimnames(sp_importaciones) sp_t1&lt;-sp_importaciones %&gt;% count(GESTION) %&gt;% compute(&quot;t1&quot;) aa&lt;-sp_importaciones %&gt;% count(GESTION) %&gt;% collect() aa&lt;-sp_importaciones %&gt;% count(GESTION,DESDEPTO) %&gt;% collect() sp_covid2 %&gt;% count(SEXO,NEUMONIA) sp_covid2 %&gt;% group_by(SEXO) %&gt;% summarise(mean(EDAD),na.rm=T) ww&lt;-sp_importaciones %&gt;% group_by(GESTION,DESDEPTO,NANDINA) %&gt;% summarise(total=sum(FOB,na.rm=T)) %&gt;% collect() sp_importaciones %&gt;% tbl(GESTION) oct20 %&gt;% select(MAS__IPSP,CC)%&gt;% summarise_all(mean) oct20 %&gt;% select(MAS__IPSP,CC)%&gt;% summarise_all(mean) %&gt;% show_query() oct20 %&gt;% summarise(qq=quantile(CC,probs=0.5)) oct20 %&gt;% select(MAS__IPSP,CC) %&gt;% ml_corr() #figuras oct20 %&gt;% select(CC) %&gt;% hist() library(dbplot) oct20 %&gt;% dbplot_histogram(CC,binwidth = 3) sp_importaciones %&gt;% dbplot_bar(GESTION) oct20 %&gt;% dbplot_raster(CC,MAS__IPSP) oct20 %&gt;% filter(PaAs==&quot;Bolivia&quot;)%&gt;% group_by(Departamento) %&gt;% summarise(voto=mean(CC)) %&gt;% ggplot(aes(Departamento,voto))+geom_bar(stat=&quot;identity&quot;) ggplot(oct20,aes(CC,MAS__IPSP))+geom_point() "]
]
