[
["index.html", "Big Data en R EST-383 Prefacio Audiencia Estructura del libro Software y acuerdos Bases de datos Agradecimiento", " Big Data en R EST-383 Alvaro Chirino Gutierrez 2020-06-29 Prefacio Este documento de Alvaro Chirino esta bajo la licencia de Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Audiencia El libro fue diseñado originalmente para los estudiantes de la materia de Programación Estadística I, una materia optativa del pregrado de la carrera de Estadística de la Universidad Mayor de San Andres. Este documento representa un primer acercamiento a los estudiantes de estadistica al software R y al mundo del Big Data. Estructura del libro El libro inluye 5 capitulos, estos son: Introducción a R Scraping Web en R Introducción al Big Data Big Data en R R y Spark Software y acuerdos sessionInfo() ## R version 4.0.2 (2020-06-22) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 18363) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=Spanish_Bolivia.1252 ## [2] LC_CTYPE=Spanish_Bolivia.1252 ## [3] LC_MONETARY=Spanish_Bolivia.1252 ## [4] LC_NUMERIC=C ## [5] LC_TIME=Spanish_Bolivia.1252 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets ## [6] methods base ## ## loaded via a namespace (and not attached): ## [1] compiler_4.0.2 magrittr_1.5 bookdown_0.18 ## [4] htmltools_0.4.0 tools_4.0.2 rstudioapi_0.11 ## [7] yaml_2.2.1 Rcpp_1.0.4.6 stringi_1.4.6 ## [10] rmarkdown_2.3 knitr_1.28 stringr_1.4.0 ## [13] xfun_0.13 digest_0.6.25 packrat_0.5.0 ## [16] rlang_0.4.6 evaluate_0.14 Bases de datos En este documento se emplearan 4 bases de datos del contecto Boliviano: Encuesta a Hogares 2018. Vivienda y Personas Encuesta de Demografia y Salud 1989 - 2008 Encuesta de Niños, niñas y adolescentes 2016 Computo oficial de las elecciones del 20 de Octubre de 2020 Bases de datos de contagios, muertes y recpurados del COVID-19 del Johns Hopkins Institute. Estas bases de datos se encuentran disponibles en formato \\(.RData\\) en el repositorio de Github del texto. Agradecimiento Eponine… "],
["acerca-del-autor.html", "Acerca del autor", " Acerca del autor "],
["introR.html", "Chapter 1 Introducción a R 1.1 Tipos de estructuras 1.2 Loops y condiciones 1.3 Funciones 1.4 Importacion de datos 1.5 Dataframe y exploración 1.6 Estadística descriptiva 1.7 Muestreo e inferencia 1.8 Gráficos de origen 1.9 ggplot 1.10 R Markdown 1.11 Shiny 1.12 Ejercicios Propuestos", " Chapter 1 Introducción a R R es un software de libre distribución 1.1 Tipos de estructuras Vectores Matrices Arrays Dataframes Listas rm(list=ls()) v&lt;-1:10 v2&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;2&quot;) as.character(v) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; &quot;5&quot; &quot;6&quot; &quot;7&quot; &quot;8&quot; &quot;9&quot; &quot;10&quot; as.numeric(v) ## [1] 1 2 3 4 5 6 7 8 9 10 x1&lt;-1 0/0 ## [1] NaN x2&lt;-NULL x2 ## NULL str(x1) ## num 1 typeof(x1) ## [1] &quot;double&quot; typeof(v2) ## [1] &quot;character&quot; # vectores con usos estadisticos rbinom(10,5,0.1) ## [1] 0 0 0 0 0 0 0 0 0 1 rnorm(10,5,10) ## [1] 4.862206 10.384849 -11.510792 16.969818 3.828309 ## [6] -2.981692 1.313199 10.115833 -1.953013 -4.876613 runif(10,50,500) ## [1] 332.88890 263.10526 137.15375 134.00201 490.36181 ## [6] 333.94038 85.53774 275.91771 405.52140 155.22953 rchisq(10,2) ## [1] 1.23526696 2.04038854 0.01250179 0.89720124 ## [5] 0.57161694 0.61395258 4.37264805 13.93431284 ## [9] 4.68452189 0.12599893 rexp(10,5) ## [1] 0.37267257 0.09152204 0.27715123 0.19265535 0.17951717 ## [6] 0.15593614 0.16868850 0.03631881 0.23381523 0.21421445 x&lt;-rchisq(1000,3) hist(x) (sum((x-mean(x))**4)/1000)/sd(x)**4 ## [1] 7.283317 median(rexp(100000,3)) ## [1] 0.2306003 quantile(x,c(2,56)/100) ## 2% 56% ## 0.2133979 2.5366242 quantile(x,1:100/100) ## 1% 2% 3% 4% 5% ## 0.1162187 0.2133979 0.3006864 0.3275293 0.3894250 ## 6% 7% 8% 9% 10% ## 0.4341136 0.4836237 0.5139160 0.5426615 0.5895566 ## 11% 12% 13% 14% 15% ## 0.6192063 0.6630744 0.6908705 0.7219548 0.7655145 ## 16% 17% 18% 19% 20% ## 0.8101292 0.8465363 0.8696904 0.9088002 0.9518806 ## 21% 22% 23% 24% 25% ## 0.9975714 1.0274603 1.0633165 1.1019253 1.1240964 ## 26% 27% 28% 29% 30% ## 1.1569501 1.2085644 1.2515338 1.2959707 1.3229408 ## 31% 32% 33% 34% 35% ## 1.3506174 1.4029185 1.4580180 1.5142839 1.5476276 ## 36% 37% 38% 39% 40% ## 1.5878327 1.6630227 1.6817367 1.7296962 1.7564547 ## 41% 42% 43% 44% 45% ## 1.7686320 1.8228404 1.8477938 1.9010097 1.9592456 ## 46% 47% 48% 49% 50% ## 1.9908298 2.0420424 2.0911873 2.1545156 2.2469812 ## 51% 52% 53% 54% 55% ## 2.3050174 2.3339509 2.3898131 2.4137987 2.4794124 ## 56% 57% 58% 59% 60% ## 2.5366242 2.5717563 2.6183147 2.7103507 2.7945538 ## 61% 62% 63% 64% 65% ## 2.8670761 2.9517173 3.0308926 3.1049917 3.1689466 ## 66% 67% 68% 69% 70% ## 3.2073000 3.2545555 3.3582068 3.4315574 3.5172837 ## 71% 72% 73% 74% 75% ## 3.5835556 3.6411947 3.7043509 3.7645543 3.8568067 ## 76% 77% 78% 79% 80% ## 3.9750376 4.0479472 4.1251037 4.1937705 4.2602822 ## 81% 82% 83% 84% 85% ## 4.3516771 4.4077546 4.5793508 4.7334553 4.9021268 ## 86% 87% 88% 89% 90% ## 5.0555582 5.1722877 5.2702716 5.4572143 5.7265385 ## 91% 92% 93% 94% 95% ## 5.9692056 6.1232828 6.4163388 6.7196904 6.8492111 ## 96% 97% 98% 99% 100% ## 7.2160794 8.1905264 8.7401611 9.8557399 17.4726612 a1&lt;-matrix(1:20,4,5) I&lt;-diag(rep(1,5)) b1&lt;-t(a1)%*%a1 b2&lt;-a1%*%t(a1) #solve(b1) #solve(b2) det(b1) ## [1] 2.791799e-37 det(b2) ## [1] 0 eigen(b1) ## eigen() decomposition ## $values ## [1] 2.864414e+03 5.585784e+00 -3.006427e-14 -5.964175e-14 ## [5] -1.744794e-13 ## ## $vectors ## [,1] [,2] [,3] [,4] ## [1,] -0.09654784 0.76855612 0.00000000 0.00000000 ## [2,] -0.24551564 0.48961420 0.08358281 0.54130760 ## [3,] -0.39448345 0.21067228 0.29202311 -0.78404241 ## [4,] -0.54345125 -0.06826963 -0.83479466 -0.05583796 ## [5,] -0.69241905 -0.34721155 0.45918874 0.29857278 ## [,5] ## [1,] 6.324555e-01 ## [2,] -6.324555e-01 ## [3,] -3.162278e-01 ## [4,] 1.609823e-15 ## [5,] 3.162278e-01 dim(b1) ## [1] 5 5 #indexaci?n v2&lt;-c(&quot;a&quot;,&quot;b&quot;,&quot;2&quot;) v2[1] ## [1] &quot;a&quot; v2[3] ## [1] &quot;2&quot; v3&lt;-1:20 v3&lt;-v3/10 v3 ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 ## [15] 1.5 1.6 1.7 1.8 1.9 2.0 v3[c(3,19)] ## [1] 0.3 1.9 v3[v3&gt;0.5] ## [1] 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 ## [15] 2.0 v3[v3==1] ## [1] 1 v3[v3!=0.9] ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 1.0 1.1 1.2 1.3 1.4 1.5 ## [15] 1.6 1.7 1.8 1.9 2.0 v3[v3&gt;0.5 &amp; v3&lt;1] ## [1] 0.6 0.7 0.8 0.9 v3[v3&gt;0.5 || v3&lt;1] ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 ## [15] 1.5 1.6 1.7 1.8 1.9 2.0 a1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 a1[1,] ## [1] 1 5 9 13 17 a1[,1] ## [1] 1 2 3 4 a1[3,1] ## [1] 3 a1[c(1,4),] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 4 8 12 16 20 a1[,c(2,4)] ## [,1] [,2] ## [1,] 5 13 ## [2,] 6 14 ## [3,] 7 15 ## [4,] 8 16 a1[c(1,4),c(2,4)] ## [,1] [,2] ## [1,] 5 13 ## [2,] 8 16 a1[,2]&gt;6 ## [1] FALSE FALSE TRUE TRUE a1[a1[,2]&gt;6,] ## [,1] [,2] [,3] [,4] [,5] ## [1,] 3 7 11 15 19 ## [2,] 4 8 12 16 20 mean(a1) ## [1] 10.5 a2&lt;-as.data.frame(a1) a2 ## V1 V2 V3 V4 V5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 str(a2) ## &#39;data.frame&#39;: 4 obs. of 5 variables: ## $ V1: int 1 2 3 4 ## $ V2: int 5 6 7 8 ## $ V3: int 9 10 11 12 ## $ V4: int 13 14 15 16 ## $ V5: int 17 18 19 20 mean(a2$V1) ## [1] 2.5 colMeans(a2) ## V1 V2 V3 V4 V5 ## 2.5 6.5 10.5 14.5 18.5 summary(a2) ## V1 V2 V3 ## Min. :1.00 Min. :5.00 Min. : 9.00 ## 1st Qu.:1.75 1st Qu.:5.75 1st Qu.: 9.75 ## Median :2.50 Median :6.50 Median :10.50 ## Mean :2.50 Mean :6.50 Mean :10.50 ## 3rd Qu.:3.25 3rd Qu.:7.25 3rd Qu.:11.25 ## Max. :4.00 Max. :8.00 Max. :12.00 ## V4 V5 ## Min. :13.00 Min. :17.00 ## 1st Qu.:13.75 1st Qu.:17.75 ## Median :14.50 Median :18.50 ## Mean :14.50 Mean :18.50 ## 3rd Qu.:15.25 3rd Qu.:19.25 ## Max. :16.00 Max. :20.00 cov(a2) ## V1 V2 V3 V4 V5 ## V1 1.666667 1.666667 1.666667 1.666667 1.666667 ## V2 1.666667 1.666667 1.666667 1.666667 1.666667 ## V3 1.666667 1.666667 1.666667 1.666667 1.666667 ## V4 1.666667 1.666667 1.666667 1.666667 1.666667 ## V5 1.666667 1.666667 1.666667 1.666667 1.666667 cor(a2) ## V1 V2 V3 V4 V5 ## V1 1 1 1 1 1 ## V2 1 1 1 1 1 ## V3 1 1 1 1 1 ## V4 1 1 1 1 1 ## V5 1 1 1 1 1 a2 ## V1 V2 V3 V4 V5 ## 1 1 5 9 13 17 ## 2 2 6 10 14 18 ## 3 3 7 11 15 19 ## 4 4 8 12 16 20 1.2 Loops y condiciones rm(list = ls()) x&lt;-5 ## if if(x&gt;6){ hist(rnorm(100,3,2)) } ## if else if(x&gt;6){ hist(rnorm(100,3,2)) } else { boxplot(rnorm(100,3,2)) } ## if encadenado x&lt;-60 if(x&gt;6){ hist(rnorm(100,3,2)) mean(x) } else if(x&lt;6){ boxplot(rnorm(100,3,2)) } else if(typeof(x)==&quot;double&quot;){ print(&quot;hola&quot;) } else { print(&quot;hola hola&quot;) } ##for for(i in 1:100){ print(i) } for(i in c(2,7,9,15,19)){ print(i) } for(i in c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;)){ print(i) } for(i in 1:5){ for(j in 6:10){ print(i*j) } } for(i in 1:5){ for(j in 6:10){ aux&lt;-i*j print(aux) if(aux==20){ break() print(&quot;hola&quot;) boxplot(rnorm(100,i,j)) } } } #while z&lt;-1 k&lt;-1 while(z&gt;0.0001){ print(k) z&lt;-1/k k&lt;-k+1 } 1.3 Funciones fx&lt;-function(x){ y&lt;-8*x**2 return(y) } fx(5) fx(1:10) curve(fx,xlim = c(-20,20),ylim=c(0,1000)) plot(fx) ##comando de estadisticas de tendencia central tendencia&lt;-function(x){ n&lt;-length(x) cat(&quot;media:&quot;,fill = T) print(sum(x)/n) cat(&quot;mediana:&quot;,fill = T) x&lt;-sort(x) if(n%%2==0){ me&lt;-(x[n/2]+x[n/2+1])/2 } else { me&lt;-x[ceiling(n/2)] } print(me) cat(&quot;moda:&quot;,fill = T) tt&lt;-table(x) mm&lt;-max(tt) print(names(tt)[(table(x)==mm)]) cat(&quot;media cuadrática:&quot;,fill = T) mc&lt;-sqrt(sum(x**2)/n) print(mc) cat(&quot;media armónica&quot;,fill = T) ma&lt;-n/sum(1/x) print(ma) cat(&quot;media geométrica:&quot;,fill = T) mg&lt;-prod(x)**(1/n) print(mg) } xx&lt;-scan() a&lt;-tendencia(xx) tendencia2&lt;-function(x){ n&lt;-length(x) media&lt;-sum(x)/n x&lt;-sort(x) if(n%%2==0){ me&lt;-(x[n/2]+x[n/2+1])/2 } else { me&lt;-x[ceiling(n/2)] } tt&lt;-table(x) mm&lt;-max(tt) mo&lt;-names(tt)[(table(x)==mm)] mc&lt;-sqrt(sum(x**2)/n) ma&lt;-n/sum(1/x) mg&lt;-prod(x)**(1/n) aux&lt;-list(media,me,mo,mc,ma,mg) return(aux) } a&lt;-tendencia2(xx) tendenciaF&lt;-function(x,f){ n&lt;-sum(f) media&lt;-sum(x*f)/n } 1.4 Importacion de datos #importando bases de datos getwd() setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) dir() ##SPSS ## activar o instalar librerias library(foreign) eh18d&lt;-read.spss(&quot;EH2018_Discriminacion.sav&quot;, to.data.frame = T,use.value.labels=F) names(eh18d) table(eh18d$s10a_01a) attributes(eh18d)$variable.labels[5] head(eh18d) ##CSV setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\CSV&quot;) dir() eh18d2&lt;-read.csv2(&quot;EH2018_Discriminacion.csv&quot;) aux&lt;-read.csv(&quot;EH2018_Discriminacion.csv&quot;,sep=&quot;;&quot;) table(eh18d2$s10a_01a) daux&lt;-c(&quot;Sexo&quot;,&quot;O. Sexual&quot;,&quot;Edad&quot;,&quot;piel&quot;,&quot;Pertenencia&quot;, &quot;idioma&quot;,&quot;vestimente&quot;,&quot;procedencia&quot;,&quot;discapacidad&quot;, &quot;religion&quot;,&quot;clase&quot;,&quot;otro&quot;) for(i in 5:16){ barplot(prop.table(table(eh18d2[,i]))*100,main=daux[i-4]) } 1.5 Dataframe y exploración rm(list=ls()) setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) library(foreign) dir() eh18&lt;-read.spss(&quot;EH2018_Persona.sav&quot;, to.data.frame = T,use.value.labels=F) ################################################# names(eh18)#nombres de las variable str(eh18) class(eh18) ##renombrando variables names(eh18)[c(5,6)]&lt;-c(&quot;sexo&quot;,&quot;edad&quot;) names(eh18) ##crear variable (mujer 1=mujer, 0 eoc) eh18$mujer&lt;-eh18$sexo==2 sum(eh18$mujer) mean(eh18$mujer) #exportación (&quot;write&quot;) write.dta(eh18,&quot;eh18p.dta&quot;) getwd() #excel library(readxl) install.packages(&quot;writexl&quot;) library(writexl) library(help=writexl) write_xlsx(eh18,&quot;eh18p.xlsx&quot;) apropos(&quot;read&quot;) apropos(&quot;write&quot;) install.packages(&quot;haven&quot;) library(haven) eh18$s03a_02e&lt;-substr(as.character(eh18$s03a_02e),1,120) eh18$s03a_02e&lt;-gsub(&quot; &quot;,&quot; &quot;,as.character(eh18$s03a_02e)) gsub(&quot;a&quot;,&quot;7&quot;,&quot;hola&quot;) gsub(&quot;`&quot;,&quot;&#39;&quot;,&quot;`s04a_04e`, `s04a_07ge`, `s04b_15e`, `s04b_16e`, `s04b_17e`, `s04b_18e`, `s04e_30c`, `s04f_35e`, `s05a_04`, `s05c_13_e`, `s05d_21e`, `s05d_22_e`, `s06a_06e`, `s06a_09e`, `s06a_10e`, `s06b_11a`, `s06b_11b`, `s06b_12a`, `s06b_12b`, `s06b_13`, `s06b_20e`, `s06e_34e`, `s06f_40a`, `s06f_40b`, `s06h_54e`, `s06h_55e`, `s06h_56e`, `s07a_01e1e`, `s07a_01e2e`, `s07a_02ce`, `s07b_05de`, `s07b_05ee`, `s07c_08e`, `s07c_09e`&quot;) nchar(eh18$s03a_02e) vv&lt;-c(&#39;s04a_04e&#39;, &#39;s04a_07ge&#39;, &#39;s04b_15e&#39;, &#39;s04b_16e&#39;, &#39;s04b_17e&#39;, &#39;s04b_18e&#39;, &#39;s04e_30c&#39;, &#39;s04f_35e&#39;, &#39;s05a_04&#39;, &#39;s05c_13_e&#39;, &#39;s05d_21e&#39;, &#39;s05d_22_e&#39;, &#39;s06a_06e&#39;, &#39;s06a_09e&#39;, &#39;s06a_10e&#39;, &#39;s06b_11a&#39;, &#39;s06b_11b&#39;, &#39;s06b_12a&#39;, &#39;s06b_12b&#39;, &#39;s06b_13&#39;, &#39;s06b_20e&#39;, &#39;s06e_34e&#39;, &#39;s06f_40a&#39;, &#39;s06f_40b&#39;, &#39;s06h_54e&#39;, &#39;s06h_55e&#39;, &#39;s06h_56e&#39;, &#39;s07a_01e1e&#39;, &#39;s07a_01e2e&#39;, &#39;s07a_02ce&#39;, &#39;s07b_05de&#39;, &#39;s07b_05ee&#39;, &#39;s07c_08e&#39;, &#39;s07c_09e&#39;) for(i in vv){ eh18[,i]&lt;-gsub(&quot; &quot;,&quot; &quot;,as.character(eh18[,i])) eh18[,i]&lt;-substr(eh18[,i],1,120) } write_sav(eh18,&quot;eh18p.sav&quot;) save(eh18,vv,file=&quot;eh18p.RData&quot;) rm(list=ls()) getwd() load(&quot;eh18p.RData&quot;) set.seed(123456)####semilla a&lt;-matrix(rnorm(20),5,4) apply(a, 1, sum) apply(a, 2, mean) apply(a, 1, sd) mc&lt;-function(x){ n&lt;-length(x) aux&lt;-sqrt(sum(x^2)/n) return(aux) } apply(a, 1, mc) apply(a, 2, mc) tapply(eh18$mujer,eh18$depto,mean) tapply(eh18$mujer,list(eh18$depto,eh18$area),mean) tapply(eh18$edad,list(eh18$depto,eh18$area),mean) tapply(eh18$edad&gt;60,list(eh18$depto,eh18$area),mean) tapply(eh18$edad&lt;5,list(eh18$depto,eh18$area),mean) tapply(eh18$p0,list(eh18$depto,eh18$area),mean,na.rm=T) aux&lt;-tapply(eh18$p0,list(eh18$depto,eh18$area,eh18$niv_ed_g),mean,na.rm=T) as.data.frame(aux) lapply() mapply() sapply() 1.6 Estadística descriptiva #Estadística descriptiva rm(list=ls()) setwd(&quot;C:\\\\Users\\\\PC18\\\\Documents\\\\R\\\\est383\\\\data\\\\eh18\\\\SPSS&quot;) #encuesta a hogares 2018 load(&quot;eh18p.RData&quot;) data()#muestra las bases de datos disponibles en R ChickWeight View(ChickWeight) help(&quot;ChickWeight&quot;) ##Porcentajes y tablas de fracuencias t1&lt;-table(eh18$sexo)#tabla de frcuencias t2&lt;-table(eh18$depto) t3&lt;-table(eh18$edad) barplot(t1,col=c(&quot;blue&quot;,&quot;darkgreen&quot;))#diagramas de barra barplot(t3,horiz = T) barplot(t3,col=&quot;red&quot;) ldep&lt;-c(&quot;CH&quot;,&quot;LP&quot;,&quot;CB&quot;,&quot;OR&quot;,&quot;PT&quot;,&quot;TJ&quot;,&quot;SC&quot;,&quot;BN&quot;,&quot;PD&quot;) barplot(t2,col=&quot;black&quot;,names.arg = ldep) #llevando a % t2p&lt;-prop.table(t2)*100 barplot(t2p,col=&quot;black&quot;,names.arg = ldep,ylab=&quot;%&quot;) pie(t2p) t3&lt;-as.data.frame(t3) t3$F&lt;-cumsum(t3$Freq) t3$r&lt;-prop.table(t3$Freq) t3$R&lt;-cumsum(t3$r) head(t3) table(ChickWeight$Time) table(ChickWeight$Diet) dim(table(ChickWeight$Chick)) #tabla de contingencia aux&lt;-table(ChickWeight$Chick,ChickWeight$Diet) aux[aux!=0]&lt;-1 apply(aux,2,sum) table(ChickWeight$Time) table(ChickWeight$Diet[ChickWeight$Time==0]) ##medidas de tendencia central mean(eh18$edad) median(eh18$edad) mean(eh18$ylab,na.rm = T) median(eh18$ylab,na.rm = T) hist(eh18$ylab) plot(density(eh18$ylab,na.rm = T)) locator(1) summary(eh18$ylab) #medidas de dispersion var(eh18$ylab,na.rm = T) sd(eh18$ylab,na.rm = T) max(eh18$ylab,na.rm = T)-min(eh18$ylab,na.rm = T) sd(eh18$ylab,na.rm = T)/mean(eh18$ylab,na.rm = T) #medidas de forma quantile(eh18$ylab,c(0.1,0.44,0.99),na.rm=T) hist(eh18$edad) #Coeficiente de asimetría #EDAD x&lt;-eh18$edad N&lt;-length(x) as&lt;-(sum((x-mean(x))^3)/N)/sd(x)^3 as #iNGRESO LABORAL x&lt;-eh18$ylab x&lt;-x[is.na(x)==F] x&lt;-x[complete.cases(x)] N&lt;-length(x) as&lt;-(sum((x-mean(x))^3)/N)/sd(x)^3 as #kurtosis... plot(density(rnorm(1000,sd=40)),ylim=c(0,0.08)) points(density(rnorm(1000,sd=10)),type=&quot;l&quot;,col=&quot;red&quot;) points(density(rnorm(1000,sd=5)),type = &quot;l&quot;,col=&quot;blue&quot;) lm(ylab~edad,data=eh18,weights = f) lm(ylab~-1+edad,data=eh18) summary(lm(ylab~edad,data=eh18)) 1.7 Muestreo e inferencia rm(list=ls()) N&lt;-15;n&lt;-6 choose(N,n) set.seed(123) y1&lt;-rnorm(N,10,5) set.seed(234) y2&lt;-rexp(N,3) set.seed(345) y3&lt;-runif(N,500,3000) #par?metros theta1&lt;-mean(y1) theta2&lt;-sum(y2) # sum(y3&gt;1500)/N # mean(y3&gt;1500) theta3&lt;-mean(y3&gt;1500) #pr?ctica (real) solo se tiene acceso a una #muestra U&lt;-1:10 set.seed(888) s&lt;-sample(U,n) s y1 #estimaciones mean(y1[s]);theta1 mean(y2[s])*N ; theta2 mean(y3[s]&gt;1500) ; theta3 combn(U,3) combn(y1,3) combn(y1,3,mean) #theta1 t1s&lt;-apply(combn(y1,n),2,mean) mean(t1s) theta1 hist(t1s) plot(density(t1s)) shapiro.test(t1s) #theta2 t2s&lt;-apply(combn(y2,n),2,mean)*N mean(t2s);theta2 hist(t2s) plot(density(t2s)) shapiro.test(t2s) #theta3 t3s&lt;-apply(combn(y3,n)&gt;1500,2,mean) mean(t3s);theta3 hist(t3s) plot(density(t3s)) shapiro.test(t3s) ####Inferencia a partir de una muestra #libreria survey install.packages(&quot;survey&quot;) library(survey) bd&lt;-data.frame(y1,y2,y3=y3&gt;1500) set.seed(123) s&lt;-sample(1:15,7) bds&lt;-bd[s,] bds$w&lt;-15/7 bds$pk&lt;-7/15 bds dm1&lt;-svydesign(ids=~0,probs = ~pk,data=bds) svymean(~y1,design = dm1) svytotal(~y2,design = dm1) library(help=survey) 1.8 Gráficos de origen rm(list=ls()) ################ plot(0,0)#inicia una hoja en blanco plot(0,0,type = &quot;n&quot;) x&lt;-c(3,4,7,2) y&lt;-c(0,6,9,2) plot(x,y,type=&quot;p&quot;) plot(x,y,type=&quot;h&quot;) plot(x,y,type=&quot;l&quot;) plot(x[order(x)],y[order(x)],type=&quot;l&quot;) plot(x,y,type = &quot;b&quot;) plot(x,y,type = &quot;o&quot;) par(mfrow=c(2,2)) plot(x,y,xlim=c(0,10),ylim=c(0,10),main=&quot;c/PUNTOS&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10),main = &quot;S/puntos&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,main=&quot;sin ejes&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,ann = F,main = &quot;blanco&quot;) par(mfrow=c(1,1)) plot(x,y,type=&quot;n&quot;,xlim=c(0,10),ylim=c(0,10), axes = F,ann = F) points(x,y,cex=c(1,2,3,4)+1) points(x,y,cex=c(1,2,3,4)+1,pch=15) points(x,y,cex=c(1,2,3,4)+1,pch=15,col=&quot;blue&quot;) points(x,y,cex=c(1,2,3,4)+1,pch=0:3, col=c(&quot;red&quot;,&quot;darkblue&quot;,&quot;pink&quot;,&quot;darkgreen&quot;)) #points(x,y,cex=2) pdf(&quot;figura1.pdf&quot;,width = 15,height = 5) plot(x,y,type=&quot;n&quot;,xlim=c(0,20),ylim=c(0,20), axes = F,ann = F) points(seq(2,10,2),seq(2,10,2),type=&quot;p&quot;,lwd=4, col=&quot;blue&quot;,lty=4) text(seq(2,10,2),seq(2,10,2), labels = c(&quot;p1&quot;,&quot;p2&quot;,&quot;p3&quot;,&quot;p4&quot;,&quot;p5&quot;),pos=3, cex=1:5,col=&quot;brown&quot;) axis(1,seq(0,20,10),lwd=3) axis(2,seq(0,20,5),lwd=2) axis(4,seq(0,20,5),seq(0,400,100),lwd=2) axis(3,seq(0,20,10),c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;),lwd=2) title(main=&quot;Plot en R&quot;,col.main=&quot;darkred&quot;, cex.main=3) title(xlab = &quot;Eje X&quot;,ylab=&quot;Eje Y&quot;,cex.lab=2, col.lab=&quot;gray&quot;) legend(&quot;topright&quot;,legend = c(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;), lwd=c(1,2,3),col=c(&quot;darkred&quot;,&quot;black&quot;,&quot;gray&quot;)) dev.off() getwd() png(&quot;figura1.png&quot;) plot(x,y,type=&quot;n&quot;,xlim=c(0,20),ylim=c(0,20), axes = F,ann = F) points(seq(2,10,2),seq(2,10,2),type=&quot;p&quot;,lwd=4, col=&quot;blue&quot;,lty=4) dev.off() pdf(&quot;sec.pdf&quot;) for(i in 1:10){ plot(i,0,xlim=c(0,10)) } dev.off() abline(h=c(2,3,4),lty=2) abline(v=c(2,3,4)) bd&lt;-as.data.frame(state.x77) bd$name&lt;-row.names(bd) head(bd) 1.9 ggplot ####################################### # Clase: gráficos en R, ggplot # Materia: Programación Estadística I # Fecha: 9 de Marzo ####################################### rm(list=ls()) ####################################### #install.packages(&quot;ggplot2&quot;) #install.packages(&quot;dplyr&quot;) #install.packages(&quot;maps&quot;) #install.packages(&quot;ggvis&quot;) library(ggplot2) library(dplyr) library(maps) library(ggvis) library(readxl) ####################################### urlfile&lt;-url(&#39;https://raw.githubusercontent.com/AlvaroLimber/EST-383/master/data/oct20.RData&#39;) load(urlfile) names(computo)[18]&lt;-&quot;MAS&quot; ####################################### #The grammar of graphics is an answer to a question: what is a statistical graphic? #base graphics 1983 #grid 2000 #lattice 1993 #ggplot 2005 #ggvis 2014 ## Datos, estetica y geometria (layers) library(ggplot2) mpg ggplot(mpg, aes(x = displ, y = hwy)) + geom_area() ggplot(computo,aes(MAS,CC))+geom_point() ggplot(mpg, aes(displ, hwy)) + geom_line() ggplot(mpg, aes(displ)) + geom_histogram() #Color tamaño y forma ggplot(mpg, aes(displ, hwy,colour = class)) + geom_point() ggplot(mpg, aes(displ, hwy,shape = drv)) + geom_point() ggplot(mpg, aes(displ, hwy,size = cyl)) + geom_point() ggplot(mpg, aes(displ, hwy)) + geom_point(aes(colour = &quot;blue&quot;)) ggplot(mpg, aes(displ, hwy)) + geom_point(colour = &quot;blue&quot;) ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class) ggplot(mpg, aes(displ, hwy)) + geom_point() + facet_wrap(~class+drv) #geoms geom_smooth() # ajuste y en x geom_boxplot() geom_histogram() geom_freqpoly() geom_bar() geom_path() geom_line() #ajuste de un modelo ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth() ggplot(mpg, aes(displ, hwy)) + geom_point() + geom_smooth(method = &quot;lm&quot;) ggplot(mpg, aes(drv, hwy)) + geom_point() ggplot(mpg, aes(drv, hwy)) + geom_jitter() ggplot(mpg, aes(drv, hwy)) + geom_boxplot() ggplot(mpg, aes(drv, hwy)) + geom_violin() ggplot(mpg, aes(hwy)) + geom_histogram() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mpg, aes(hwy)) + geom_freqpoly() #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ggplot(mpg, aes(hwy)) + geom_freqpoly(binwidth = 2.5) ggplot(mpg, aes(hwy)) + geom_freqpoly(binwidth = 1) ggplot(mpg, aes(hwy)) + geom_density() ggplot(mpg, aes(displ, colour = drv)) + geom_freqpoly(binwidth = 0.5) ggplot(mpg, aes(displ, fill = drv)) + geom_histogram(binwidth = 0.5) ggplot(mpg, aes(displ, fill = drv)) + geom_histogram(binwidth = 0.5) + facet_wrap(~drv, ncol = 1) ggplot(mpg, aes(manufacturer)) + geom_bar() aa&lt;-as.data.frame(table(mpg$class)) aa ggplot(aa,aes(Var1,Freq))+ geom_bar() ggplot(aa,aes(Var1,Freq)) + geom_bar(stat = &quot;identity&quot;) ggplot(drugs, aes(drug, effect)) + geom_point() ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(&quot;city driving (mpg)&quot;) + ylab(&quot;highway driving (mpg)&quot;) # Remove the axis labels with NULL ggplot(mpg, aes(cty, hwy)) + geom_point(alpha = 1 / 3) + xlab(NULL) + ylab(NULL) ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25) + xlim(&quot;f&quot;, &quot;r&quot;) + ylim(20, 30) #&gt; Warning: Removed 137 rows containing missing values (geom_point). # For continuous scales, use NA to set only one limit ggplot(mpg, aes(drv, hwy)) + geom_jitter(width = 0.25, na.rm = TRUE) + ylim(NA, 30) #output p &lt;- ggplot(mpg, aes(displ, hwy, colour = factor(cyl))) + geom_point() # Save png to disk ggsave(&quot;plot.png&quot;, p, width = 5, height = 5) saveRDS(p, &quot;plot.rds&quot;) q &lt;- readRDS(&quot;plot.rds&quot;) ggplot(faithfuld, aes(eruptions, waiting)) + geom_contour(aes(z = density, colour = ..level..)) mi_counties &lt;- map_data(&quot;world&quot;) %&gt;% select(lon = long, lat, group, id = subregion) ggplot(mi_counties, aes(lon, lat)) + geom_point(size = .25, show.legend = FALSE) + coord_quickmap() ggplot(mi_counties, aes(lon, lat, group = group)) + geom_polygon(fill = &quot;white&quot;, colour = &quot;grey50&quot;) + coord_quickmap() #temas theme_bw() theme_grey() theme_linedraw() theme_light() theme_dark() theme_minimal() theme_classic() theme_void() 1.10 R Markdown “R Markdown” se introdujo por primera vez en el paquete knitr a principios de 2012. La idea era incrustar fragmentos de código (de R u otros) en los documentos de Markdown. De hecho, knitr soportó varios lenguajes de autoría desde el principio además de Markdown, incluidos LaTeX, HTML, AsciiDoc, reStructuredText y Textile. Markdown se ha convertido en el formato de documento más popular. La simplicidad de Markdown se destaca claramente entre estos formatos de documentos. 1.10.1 Instalación install.packages(&#39;rmarkdown&#39;) # Si se prefiere la versión en desarrollo if (!requireNamespace(&quot;devtools&quot;)) install.packages(&#39;devtools&#39;) devtools::install_github(&#39;rstudio/rmarkdown&#39;) Si el objetivo es usar Markdown para generar documentos PDF se necesita instalar Latex. Existen cheatsheets utiles para usar markdown, como: cheatsheets 1.10.2 YAML Header Al inicio del archivo y entre las lineas — --- title: Mi documento author: Juan Perez date: Marzo 22, 20220 output: html_document --- 1.10.3 Sintaxis básica Énfasis sobre el texto, *italic* **bold** _italic_ __bold__ Secciones, # Header 1 ## Header 2 ### Header 3 Items (viñetas) no ordenadas y ordenadas, * Item 1 * Item 2 + Item 2a + Item 2b 1. Item 1 2. Item 2 3. Item 3 + Item 3a + Item 3b Palabras clave con referencias web, [linked phrase](http://example.com) Imágenes simples o con titulo, ![](http://example.com/logo.png) ![optional caption text](figures/img.png) Blockquotes It’s always better to give than to receive. A friend once said: &gt; It&#39;s always better to give than to receive. Ecuaciones en linea y en párrafo, En linea \\(\\sum_i{x^2}\\) o en párrafo: \\[\\sum_i{x^2}\\] $equation$ $$ equation $$ 1.10.4 Tipos de documentos beamer_presentation github_document html_document ioslides_presentation latex_document md_document odt_document pdf_document powerpoint_presentation rtf_document slidy_presentation word_document 1.10.5 Chunks Los chunks son entornos que permiten incluir código en R dentro de las distintos tipos de documentos que genera Rmarkdown, los chunks inician con ```{r} y termina con ```, también es posible introducir chunks en linea con el texto, esto se logra introduciendo Texto ... `r &lt;code&gt;` ... texto La parte {r} del chunk sirve para introducir las distintas opciones que va a contener ese chunk, las opciones disponibles son: echo (default = TRUE), muestra el código del chunk en la salida del documento eval (default = TRUE), corre el código del chunk message (default = TRUE), muestra los mensajes que genera el chunk Existen funciones útiles para mejorar las salidas de tablas, tales como xtable y kable de la librería knitr. 1.11 Shiny Shiny es una librería de RStudio orientada a crear aplicaciones web interactivas con R. Una vez instalada existen dos formas de crear una aplicación en Shiny. Una alternativa cada vez mas popular es la de crear un documento shiny junto con Markdown. Mediante un solo archivo denominando app.R Mediante dos archivos separados, el server.R y ui.R Se recomienda que en cualquiera de las dos alternativas, estos archivos estén contenidos en alguna carpeta. Existen dos partes esenciales al momento de definir una app en Shiny, el UI que es una función que define la interfaz de la aplicación y el Server que define una función con instrucciones sobre cómo construir y reconstruir los objetos R que se mostraran en la UI. La composición básica según las formas de aplicarlas son: # app.R library(shiny) ui &lt;- fluidPage( numericInput(inputId = &quot;n&quot;, &quot;Sample size&quot;, value = 25), plotOutput(outputId = &quot;hist&quot;) ) server &lt;- function(input, output) { output$hist &lt;- renderPlot({ hist(rnorm(input$n)) }) } shinyApp(ui = ui, server = server) # ui.R fluidPage( numericInput(inputId = &quot;n&quot;, &quot;Sample size&quot;, value = 25), plotOutput(outputId = &quot;hist&quot;) ) # server.R function(input, output) { output$hist &lt;- renderPlot({ hist(rnorm(input$n)) }) 1.11.1 UI Al ser UI la interfaz esta permite la interacción directa con el usuario, a estas se las denominan los entradas \\(input\\), las opciones de input son: Botón de acción: actionButton(inputId, label, icon) Enlace: actionLink(inputId, label, icon,) Check box múltiple: checkboxGroupInput(inputId,label, choices, selected, inline) Check box simple: checkboxInput(inputId, label,value) Fecha: dateInput(inputId, label, value,min, max, format, startview,weekstart, language) Rango de fecha: dateRangeInput(inputId, label,start, end, min, max, format,startview, weekstart, language,separator) Cargar archivo: fileInput(inputId, label, multiple,accept) Entrada numérica: numericInput(inputId, label, value,min, max, step) Tipo contraseña: passwordInput(inputId, label,value) Selección tipo botones: radioButtons(inputId, label,choices, selected, inline) Seleccionable: selectInput(inputId, label, choices,selected, multiple, selectize,width, size) Slider: sliderInput(inputId, label, min,max, value, step, round, format,locale, ticks, animate, * width,sep,pre, post) Enviar submitButton(text, icon) Entrada de texto textInput(inputId, label, value) Los inputs principalmente tienen dos argumentos el inputId que se refiere al identificador del input, este se utiliza en el server, y el label que es la etiqueta que aparece en la interfaz visual, estos inputs se asignan a algún objeto (xx&lt;-input()). En el server se tiene acceso al input mediante input$xx. 1.11.2 Server En cuanto el server, este usa los distintos inputs para generar las salidas (outputs), las opciones de salidas disponibles son: DT::renderDataTable(expr, options,callback, escape, env, quoted) renderImage(expr, env, quoted,deleteFile) renderPlot(expr, width, height, res, …,env, quoted, func) renderPrint(expr, env, quoted, func,width) renderTable(expr,…, env, quoted, func) renderText(expr, env, quoted, func) renderUI(expr, env, quoted, func) Estos outputs “render” se asignan a un objeto y luego ellos deben ser incluidos dentro del UI con su correspondiente Output: dataTableOutput(outputId, icon, …) imageOutput(outputId, width, height,click, dblclick, hover, hoverDelay, inline,hoverDelayType, brush, clickId,hoverId) plotOutput(outputId, width, height, click,dblclick, hover, hoverDelay, inline,hoverDelayType, brush, clickId,hoverId) verbatimTextOutput(outputId) tableOutput(outputId) textOutput(outputId, container, inline) uiOutput(outputId, inline, container, …) y htmlOutput(outputId, inline, container, …) 1.12 Ejercicios Propuestos Crear una función que devuelva los \\(k\\) primeros números primos Crear una función que calcule la mediana para tablas de frecuencias con intervalos de clases Crear una función que calcule los Quantiles para tablas de frecuencias con intervalos de clases Realice una función para el calculo del tamaño de muestra para el muestreo aleatorio simple, considere la media, el margen de error relativo y coeficientes de confianza. Crear una función que realice la prueba de independencia Chi-cuadrado Empleando la ENDSA muestre por año y departamento el porcentaje de personas que fuman Utilizando la base de datos del COVID-19 genere un gráfico de evolución de contagios, muertes y recuperados. Use los gráficos de origen Utilizando la base de datos del COVID-19 genere un gráfico de evolución de contagios, muertes y recuperados. Use ggplot Utilizando la encuesta 2018, genere un reporte que presente: Total de población y viviendas por departamento y área Pobreza moderada por Departamento y área Indice de Gini por departamento Realice una función en Shiny empleando la base de datos electoral del \\(20o\\) que permita ver los resultados por recinto, seleccionando, su país, departamento, municipio, recinto. Usando la EH 2018, determine el porcentaje de personas que sufrieron un Atraco (Robo a personas) en la vía pública en los últimos 12 meses. "],
["scraping-web.html", "Chapter 2 Scraping Web 2.1 Pasos para la recopilación de información 2.2 Tecnologías de diseminación, extracción y almacenamiento Web 2.3 Librerías en R 2.4 Librería rvest 2.5 Ejemplo: Ketal 2.6 Ejemplo: Ministerio de Educación 2.7 Ejemplo, informacion COVID-19 en Bolivia 2.8 APIs 2.9 Ejercicios Propuestos", " Chapter 2 Scraping Web La definición del scraping web que se toma en este documento proviene de (???) que expresa: El Web Scraping es la recolección automática de información de los sitios web. (obviamente no a través de un humano usando un navegador web). Un tema dentro del scraping web son las denominadas APIs (Application Programming Interface), estas son entradas a las paginas web diseñadas por los administradores de la pagina web, por lo mismo no siempre contienen toda la información que se desea. Aunque las API no son tan ubicuas como deberían, puede encontrar API para muchos tipos de información. Interesado en la música? Hay algunas API diferentes que pueden darle canciones, artistas, álbumes e incluso información sobre estilos musicales y artistas relacionados. ¿Necesitas datos deportivos? ESPN proporciona API para información de atletas, puntajes de juegos y más. Google tiene docenas de API en su sección de Desarrolladores para traducciones de idiomas, análisis, geolocalización y más. 2.1 Pasos para la recopilación de información Siguiendo a (???) que establece cinco pasos al momento de decidir recopilar información mediante el scraping web, estos pasos son: Asegúrese de saber exactamente qué tipo de información necesita. Esto puede ser específico (el producto interno bruto de todos los países de la OCDE durante los últimos 10 años'') o vago (opinión de la gente sobre el teléfono de la empresa X’‘, ``colaboración entre miembros del Senado de los Estados Unidos’’). Averigüe si hay fuentes de datos en la Web que puedan proporcionar información directa o indirecta sobre su problema. Si está buscando hechos concretos, esto probablemente sea fácil. Si está interesado en conceptos bastante vagos, esto es más difícil. La página de inicio de la embajada de un país podría ser una fuente valiosa para la acción de política exterior que a menudo se oculta detrás del telón de la diplomacia. Los tweets pueden contener tendencias de opinión sobre casi todo, las plataformas comerciales pueden informar sobre la satisfacción de los clientes con los productos, las tarifas de alquiler en los sitios web de propiedades pueden contener información sobre el atractivo actual de los barrios de la ciudad … Desarrolle una teoría del proceso de generación de datos cuando busque fuentes potenciales. ¿Cuándo se generaron los datos, cuándo se cargaron en la Web y quién lo hizo? ¿Existen áreas potenciales que no están cubiertas, son consistentes o precisas, y puede identificarlas y corregirlas? Equilibrar las ventajas y desventajas de las posibles fuentes de datos. Los aspectos relevantes pueden ser la disponibilidad (¡y la legalidad!), los costos de recolección, la compatibilidad de nuevas fuentes con la investigación existente, pero también factores muy subjetivos como la aceptación de la fuente de datos por parte de otros. También piense en posibles formas de validar la calidad de sus datos. ¿Existen otras fuentes independientes que brinden información similar para que sean posibles verificaciones cruzadas aleatorias? En caso de datos secundarios, ¿puede identificar la fuente original y verificar los errores de transferencia? ¡Toma una decisión! Elija la fuente de datos que le parezca más adecuada, documente los motivos de la decisión y comience con los preparativos para la recopilación. Si es factible, recopile datos de varias fuentes para validar las fuentes de datos. Muchos problemas y beneficios de varias estrategias de recopilación de datos salen a la luz solo después de la recopilación real. 2.2 Tecnologías de diseminación, extracción y almacenamiento Web Una vez elegida la fuente de datos (página web) y al menos con la intuición de lo que se quiere obtener, el siguiente paso es decidir el mecanismos para el scraping (raspado), esto esta relacionado al tipo de pagina web, ver si esta ofrece una entrada API y conocer sus limitaciones puede ser un punto de partida. Esto se denomina Technologies for disseminating, extracting, and storing web data Collecting, en el marco del uso de R la figura muestra las interacciones entre ellas. Tecnologías para difundir, extraer y almacenar datos web (considerando el entorno de R) 2.3 Librerías en R El siguiente cuadro presenta las librerías en R relacionadas al web Scraping, incluyendo los servicios API, que a la fecha del proyecto alcanza a 761 librerías, esto representa el \\(5.02\\)% de las librerías en en R. library(knitr) library(packagefinder) library(xtable) findPackage(c(&quot;API&quot;,&quot;scrape&quot;),limit.results=-1) 2.4 Librería rvest Rvest es parte del universo tidyverse y esta orientada al scrape de páginas web. La instalación es usual: #desde CRAN install.packages(&quot;rvest&quot;) #la versión en desarrollo desde github devtools::install_github(&quot;tidyverse/rvest&quot;) Existe la herramienta selectorgadget disponible en http://selectorgadget.com/, esta permite interactuar con las paginas web para seleccionar partes del documento usando un CSS selector. Las funciones mas importantes dentro de rvest son: read_html para cargar la estructura de la página html_nodes para extraer informacion de la pagina segun el CSS selector html_text para extraer texto de un html_nodes html_table para extraer tablas y ponerlas en data frame 2.5 Ejemplo: Ketal Una de las potencialidades del scraping es poder rescatar información contenida en páginas web, según la estructura de esta, en esta sección se presenta un ejemplo del código y la recolección de información que puede realizarse. Para esto se eligió la página de supermercados Ketal que es abierta y publica los precios de sus distintos productos de forma categorizada por tipo de productos. El estudio de esta pagina tiene el principal objetivo de mostrar el proceso de extracción, el uso potencial de la información esta relacionada a tener la variación de los precios y para futuros estudios se podría completar el ejercicio incluyendo la parte de la demanda mediante la encuesta de presupuestos familiares y poder así pronosticar la inflación dentro de este sector y para la población potencial que acude a supermercados. El ejemplo presenta el proceso del scraping para 8 grupo de productos: Carne de aves Carne de cerdo Carne de res Pescado Leches Aceites Frutas y verduras Dulces Se emplea la libreria rvest, que es un paquete diseña para para facilitar la descarga y la manipulación de HTML y XML. También se utiliza la extensión Selector Gadget https://selectorgadget.com, este permite la identificación de las estructuras de las páginas web. El objetivo del raspado es obtener el nombre del producto, su código, y el precio. Utilizando el selector gadget se identifica que en \\(.product-name\\) se guardan los nombres, en \\(.price.product-price\\) el precio del producto y en \\(data-id-product\\) el identificador del producto. En R el código es: #librerías para el raspado library(rvest) #Enlaces de los grupos de productos ave &lt;- read_html(&quot;https://www.ketal.com.bo/209-aves&quot;) cerdo&lt;-read_html(&quot;https://www.ketal.com.bo/208-cerdo&quot;) res&lt;-read_html(&quot;https://www.ketal.com.bo/210-res&quot;) fish&lt;-read_html(&quot;https://www.ketal.com.bo/17060162-pescados&quot;) leches&lt;-read_html(&quot;https://www.ketal.com.bo/53-leches-liquidas&quot;) aceites&lt;-read_html(&quot;https://www.ketal.com.bo/272-aceites-y-vinagres&quot;) frutaver&lt;-read_html( &quot;https://www.ketal.com.bo/50-frutas-y-verduras?id_category=50&amp;n=150&quot;) dulce&lt;-read_html( &quot;https://www.ketal.com.bo/15-chocolates-y-golosinas?id_category=15&amp;n=150&quot;) #definición de la base de datos que guarda la información bd&lt;-NULL #Algoritmo del raspado de los 8 productos k&lt;-1 for(i in list(ave,cerdo,res,fish,leches,aceites,frutaver,dulce)){ print(k) name&lt;-html_attr(html_nodes(i,&quot;.product-name&quot;),&quot;title&quot;)[-1] precio&lt;-html_text( html_nodes(i,&quot;.price.product-price&quot;),trim=T)[seq(1,length(name)*2,2)] code&lt;-html_attr(html_nodes(i,&quot;.add_to_compare&quot;),&quot;data-id-product&quot;) bd&lt;-rbind(bd,data.frame(name,precio,code)) k&lt;-k+1 } #se identifica el día day&lt;-substr(Sys.time(),1,10) #se guarda la informacion en base al dia del raspado bd$day&lt;-day save(bd,file = paste0(day,&quot;.RData&quot;)) 2.6 Ejemplo: Ministerio de Educación library(rvest) library(foreign) library(xlsx) ######################## bd&lt;-read.dta(&quot;mmUE.dta&quot;) rue&lt;-bd$cod_ue_det rue&lt;-unlist(strsplit(rue,&quot;;&quot;)) bd&lt;-data.frame(matrix(0,1,78)) k&lt;-1 for(i in 325:length(rue)){ print(i) urlmin&lt;-paste0(&quot;http://seie.minedu.gob.bo/reportes/mapas_unidades_educativas/ficha/ver/&quot;,rue[i]) ue &lt;- read_html(urlmin,encoding = &quot;UTF-8&quot;) jj&lt;-grepl(&quot;Error was&quot;,unlist(strsplit(html_text(html_nodes(ue,&quot;strong&quot;))[1:2],&quot;:&quot;))[c(2)]) if(jj==T){ next() } #nombre id (2) bd[k,1:2]&lt;-unlist(strsplit(html_text(html_nodes(ue,&quot;strong&quot;))[1:2],&quot;:&quot;))[c(2,4)] #Informacion administrativa (12) bd[k,3:14]&lt;-html_text(html_nodes(ue,&quot;dd&quot;)) #matricula #promovidos #reprobacion #abandono # (60) #matricula for(j in 1:4){ bd[k,(15*j):(14+15*j)]&lt;-as.integer(unlist(html_table(ue)[j])[-c(1:3)]) } #servicios (4) bd[k,75:78]&lt;-html_text(html_nodes(ue,&quot;strong&quot;))[18:21] k&lt;-k+1 } write.xlsx(bd,&quot;ue2017.xlsx&quot;,row.names = F) 2.7 Ejemplo, informacion COVID-19 en Bolivia library(rvest) ## Loading required package: xml2 #paso 0: Estadísticas del covid-19 en Bolivia #paso 1: cargar la www de interés bs&lt;-read_html(&quot;https://www.boliviasegura.gob.bo/&quot;) ww&lt;-read_html(&quot;https://www.worldometers.info/coronavirus/&quot;) #paso 2: Raspar la información de interés node0&lt;-html_nodes(bs,&quot;h1&quot;) node1&lt;-html_text(node0) node2&lt;-as.numeric(node1) barplot(node2,legend.text = c(&quot;Confirmados&quot;,&quot;Recuperados&quot;,&quot;Muertes&quot;)) aux&lt;-html_nodes(bs,&quot;.mapanuevos&quot;) #paso 3: trabajando con tablas tabla&lt;-html_table(bs,fill = T) tabla[[2]] ## Departamento Hoy Acumulado Decesos Recuperados ## 1 Beni 66 4,041 218 98 ## 2 Chuquisaca 59 637 31 23 ## 3 Cochabamba 52 3,468 165 138 ## 4 La Paz 108 2,882 76 382 ## 5 Oruro 84 878 55 113 ## 6 Pando 0 567 25 14 ## 7 Potosí 38 641 19 50 ## 8 Santa Cruz 418 17,873 412 7669 ## 9 Tarija 23 537 13 30 #paso 4: limpieza #paso 5: Análisis 2.8 APIs Conocida también por la sigla API,“application programming interface”, es un conjunto de subrutinas, funciones y procedimientos (o métodos, en la programación orientada a objetos) que ofrece cierta biblioteca para ser utilizado por otro software. Ofrece una entrada a los datos que distribuye el API. 2.8.1 API Banco Mundial La API ofrece acceso a las estadísticas que genera el Banco Mundial, existe un set extenso de estadisticas de la mayoria de los paises. La API ofrece mas de 16000 indicadores de series de tiempo, muchos de los indicadores tienen una cobertura de 50 años. La API incluye el acceso a 45 bases de datos, incluyendo: World Development Indicators International Debt Statistcs Doing Business Human Capital Index Subnational Poverty Y otros, ver En R s epuede acceder mediante la libreria wbstats. install.packages(&quot;wbstats&quot;) library(wbstats) #acceso a todos los indicadores disponibles wbindicators(&quot;es&quot;) #acceso al catalogo de datos wbdatacatalog() #búsqueda de indicador wbsearch(pattern = &quot;education&quot;) # ver los países y sus códigos wbcountries() #Comando para extraer los indicadores wb(country = &quot;BOL&quot;,indicator = &quot;NY.GDP.MKTP.CD&quot;, startdate = 2000, enddate = 2016) 2.8.2 API Google trends La librari gtrendsR permite acceder a la API de google trends, esta permite acceder a las tendencias de busqueda que se realizan mediante el motor de busqueda de Google. #install.packages(&quot;gtrendsR&quot;) library(gtrendsR) library(knitr) res &lt;- gtrends(&quot;Coronavirus&quot;, geo = c(&quot;BO&quot;)) plot(res) kable(head(res$interest_over_time,10)) 2.8.3 API Google Maps R tiene la librari googleway que permite tener acceso a la API de google maps, es necesario una llave de autentificación. Para la llave seguir los pasos en el enlace #install.packages(&quot;googleway&quot;) library(googleway) map_key &lt;- &#39;xxx&#39; google_map(key = map_key,location = c(-16.5030161,-68.1292566),zoom = 8) %&gt;% add_traffic() 2.9 Ejercicios Propuestos Extraer la fecha, y el precio de compra y venta del dolar de la página https://www.bcb.gob.bo Usando la página https://www.trabajopolis.bo/ seleccionar un departamento y armar una base de dato de ofertas laborales Armar una base de datos en base a la página https://www.infocasas.com.bo Explorar librarias API con acceso a Youtube y encontrar los 10 videos con más visualizaciones que incluyan a Bolivia en su titulo. Explorar la libraria gtrendsR y explorar en que meses en Bolivia es mas frecuente la búsqueda de Usar la informacion de worldometers y generar un gráfico de contagios por millon de los distintos paises. "],
["introducción-al-big-data.html", "Chapter 3 Introducción al Big Data 3.1 Definiendo al Big Data 3.2 Las 5 V en el Big Data 3.3 Ciclo de vida de un proyecto de análisis de datos 3.4 Inferencia y Big Data 3.5 Calidad de dato y Big Data 3.6 Captura y preservación 3.7 Análisis y modelado 3.8 Inferencia y ética 3.9 Ejercicios Propuestos", " Chapter 3 Introducción al Big Data 3.1 Definiendo al Big Data De forma simple el Big Data se define como: DATA&gt;RAM O de forma mas literal “cualquier cosa demasiado grande para caber en su computadora.” La Asociación Americana de Investigación de Opinión Pública menciona: “El término” Big Data “es una descripción imprecisa de un conjunto rico y complicado de características, prácticas, técnicas, cuestiones éticas y resultados, todos asociados con los datos”. 3.2 Las 5 V en el Big Data Velocidad Volumen Valor Variedad Veracidad 3.3 Ciclo de vida de un proyecto de análisis de datos Se pueden identificar 4 fases: Clarify: (Clarificar) Llegar a familiarizarse con los datos Develop: (Desarrollar) Crear un modelop de trabajo Productize: (Producir) Automatizar e integrar Publish: (Publicar) Socializar Estas fases pueden contener nodos adicionales según el proyecto: Subset: Extraer los datos a explorar, los datos de trabajo Clarify: (Clarificar) Llegar a familiarizarse con los datos Develop: (Desarrollar) Crear un modelop de trabajo *. Scale Up: Generalizar a la base de datos completa Productize: (Producir) Automatizar e integrar Publish: (Publicar) Socializar Otros ciclos de trabajo puedes ser: Identificar el problema Diseño del requerimiento de datos Procesar los datos Desarrollo del análisis sobre los datos Visualizar los datos 3.4 Inferencia y Big Data El objetivo de la inferencia es poder decir algo de la población objetivo a partir de la información disponibles. Se debe tener en cuenta los tipos de estudio provenientes; ya sean de encuestas probabilísticas, diseño experimentales o estudios de observación. Se debe estar seguro de la calidad de la base de datos proveniente, ya sean estos los errores de muestreo, procesos de calibración, ponderación, post estratificación en el caso de muestreo o el el propensity score y la estratificación principal para repara diseño experimentales rotos. Se puede distinguir tres metas en el proceso de inferencia: Descriptivo Causal Predictivo 3.4.1 Descriptivo La estadística descriptiva puede ser; (1) a un nivel simple de descripción de una base de datos sin la búsqueda de querer expandir los resultados (registros administrativos, censos, estudios de observación) o (2) para encuestas probabilisticas, realizar las estimaciones de la muestra con sus respectivos errores muestrales y a partir de estas estimaciones describir a la población Ejemplo: El INE estima a partir de la EH-2018 que la incidencia de pobreza moderadara en Bolivia para el 2018 alcanza el …% Ejemplos como este muestra que el propósito es puramente descriptivo en cuanto a la pobreza. 3.4.2 Causal Muchos investigadores buscan explorar hipótesis, aveces originadas en la teoria o en alguna relación observada de forma empírica, con la idea central de permitir la inferencia causal. La data para esto proviene de diseños experimentales o fuertes estudios no experimentales (cuasi-experimentales), el interés de estos estudios es principalmente encontrar el efecto de de una variable entre otra. \\[X \\rightarrow Y\\] Aspecto que es logrado fácilmente mediante los diseños experimentales. En este tipo de estudio el componente descriptivo no es tan importante como el metodo para identificar la causalidad. Es importante diferenciar en este punto la causalidad de la correlación. Ejemplo: (3ie) Este informe se basa en un estudio de Dupas, Duflo y Kremer que se realizó en colaboración con el gobierno de Ghana. El estudio examinó los impactos a mediano plazo de otorgar becas de cuatro años a estudiantes que no podían matricularse en escuelas secundarias superiores (SHS) debido a limitaciones financieras. Los investigadores encontraron que el programa de becas tuvo un impacto significativo en el logro educativo y las habilidades cognitivas, particularmente entre las niñas. El programa también tuvo un mayor impacto en las tasas de finalización de SHS de las niñas en términos porcentuales. Ejemplo: UDAPE el 2013 realizó el calculo del impacto de la renta dignidad en Bolivia empleando el método de regresión discontinua un método cuasi-experimental. Una de las debilidades principales de estos estudios es la falta o poca de validez externa, es decir, es dificil poder generalizar los resultados. Métodos cuasi-experimentales Diferencia en diferencia Propensity Score Matching (PSM) Probit. Variables intrumentales Modelos estructurales Regresión Discontinua. 3.4.3 Predictivo El pronóstico o predicción tiene un rol diferenciado según la ciencia de aplicación, teniendo un rol significativo dentro de las estadística oficiales, principalmente en lo social (proyecciones poblacionales) y económico (indicadores macroeconómicos), principalmente para hacedores de política, gobernates y empresarios. Similar a En la configuración de inferencia causal, es de suma importancia que conozcamos el proceso que generó los datos, y podemos descartar cualquier mecanismo de selección sistemática desconocido o no observado. Ejemplo: El Institute of Global Health, Faculty of Medicine, University of Geneva tiene una página web que realiza pronosticos por país para los casos de COVID-19. Enlace 3.5 Calidad de dato y Big Data La mayoría de los datos en el mundo real son ruidosos, inconsistentes y adolecen de valores perdidos, independientemente de su origen. Incluso si la recopilación de datos es barata, los costos de crear datos de alta calidad a partir de la fuente (limpieza, conservación, estandarización e integración) son considerables. La calidad de los datos se puede caracterizar de múltiples maneras: Precisión: ¿qué tan precisos son los valores de los atributos en los datos? Integridad: ¿están completos los datos? Consistencia: ¿Cuán consistentes son los valores en y entre las bases de datos? Puntualidad: ¿qué tan oportunos son los datos? Accesibilidad: ¿están disponibles todas las variables para el análisis? Los cientistas de datos tienen décadas de experiencia en la transformación de datos desordenados, ruidosos y no estructurados en un conjunto de datos bien definido, claramente estructurado y probado en calidad. El pre procesamiento es un proceso complejo y que lleva mucho tiempo porque es práctico: requiere juicio y no puede automatizarse de manera efectiva. Un flujo de trabajo típico comprende múltiples pasos desde la definición de datos hasta el análisis y termina con el filtrado. Es difícil exagerar el valor del pre-procesamiento para cualquier análisis de datos, pero esto es particularmente cierto en big data. Los datos deben analizarse, estandarizarse, deduplicarse y normalizarse. Análisis (parsing): Exploración de datos Estandarización (Standardization): Identificar variables que requieren transformación y ajustes. Duplicación: Consiste en eliminar registros redundantes Normalización (Normalization): Es el proceso de garantizar que los campos que se comparan entre archivos sean lo más similares posible en el sentido de que podrían haber sido generados por el mismo proceso. Como mínimo, se deben aplicar las mismas reglas de estandarización a ambos archivos. 3.6 Captura y preservación Se refiere al proceso de obtener la información de las distintas fuentes posibles y luego pasar a un proceso de preservacion. 3.6.1 Fuentes convencionales Estas estan basadas en la información que se distribuye de forma tradicional mediante bases de datos estructuradas, normalmente estas las distribuyen instituciones con amplios conocimientos en la gestion de bases de datos, para el caso de Bolivia se puede citar algunas: Instituto Nacional de Estadística SNIS UDAPE 3.6.2 Datos web y APIs Referirse al capítulo 2 de scraping web, estos son los mecanismos para extraer información en internet 3.6.3 Record Linkage Se refiere al proceso de concatenar o unir observaciones dispuestas en múltiples bases de datos. Puede ser usado para compensar la falta de información Se usa para crear estudios longitudinales Se pueden armar seudo-paneles Esto permite mejorar la cobertura (append), ampliar las temáticas de estudio (merge). Pre-procesamiento Matching: Une información a partir de una clave, existen muchos problemas con claves tipo texto. Aproximaciones a reglas para hacer math: Definir criterios para posibilitar el match basados en reglas, distancias cercanas, etc. Match basados en probabilidad: Fellegi–Sunter method 3.6.4 Bases de datos Una vez que los datos fueron recolectados y enlazados entre diferentes fuentes, es necesario guardar la información. Ahora se siscute las alternativas para guardar la información. DBMS (databasemanagement systems) Sistema de gestión de base de datos: Decidir que herramienta usar segun la dimensión de los archivos. Bases de datos espaciales Múltiples formatos: https://juliael.carto.com/ 3.6.5 Programando con Big Data MapReduce: map, shuffle y reduce Apache hadoop MapReduce (Hadoop Distributed File System HDFS) Apache Spark 3.7 Análisis y modelado 3.7.1 Machine learning ¿Machine learning = Statistics? Veran que muchos métodos discutidos a lo largo de su formación como estadisticos aparecen dentro del matching learning y que son llamados con otros nombres. Al pensar en machine learning debemos asociarlo directamente con procesos computacionales, muchos otros conceptos giran al rededor de esta idea como la inteligencia artificial. Proceso de machine learning hoy: Permiten manejar autos de forma autonoma Puede recomendar libros, amistades, música, etc Identificar drogas, proteinas y ciertos genes Se usa para detectar ciertos tipos de cancer y otras enfermedades médicas Ayudan a conocer que estudiantes necesitan un apoyo adicional Ayudan a persuadir por que candidato votar en las elecciones. 3.7.1.1 El proceso del machine learning Entender el problema y la meta Formular esto como un problema de machine learning Explorar y preparar los datos Feature engineeing Selección del método Evaluación Deployment 3.7.1.2 Formulación del problema En ML existen 2 grandes categorías Aprendizaje supervisado: Existe una \\(Y\\) que queremos predecir o clasificar a partir de los datos. El fin es el ajuste y la generalización * Clasificación (\\(Y\\) cualitativa) * Predicción * Regresión (\\(Y\\) cuantitativa) Aprendizaje no supervisado: No existe una variable objetivo, se quiere conocer, entender las asociaciones y patrones naturales en los datos. * Clustering * PCA, MCA 3.7.2 Análisis de texto: Entendiendo lo que la gente escribe Clasificación de documentos Análisis de sentimientos Etiquetado de discursos 3.7.3 Networks 3.8 Inferencia y ética 3.8.1 Información y visualización Los usuarios pueden escanear, reconocer, comprender y recordar representaciones visualmente estructuradas más rápidamente de lo que pueden procesar representaciones no estructuradas La ciencia de la visualización se basa en múltiples campos, como la psicología perceptiva, las estadísticas y el diseño gráfico para presentar información La efectividad de una visualización depende tanto de las necesidades de análisis como de los objetivos de diseño. El diseño, el desarrollo y la evaluación de una visualización se guían por la comprensión de los antecedentes y las metas del público objetivo. El desarrollo de una visualización efectiva es un proceso iterativo que generalmente incluye los siguientes pasos: Especificar las necesidades del usuario, tareas, requisitos de accesibilidad y criterios para el éxito. Preparar datos (limpiar, transformar). Diseñar representaciones visuales. Interacción de diseño. Planifique el intercambio de ideas, procedencia. Prototipo / evaluación, incluidas las pruebas de usabilidad. Implementar (supervisar el uso, proporcionar soporte al usuario, gestionar el proceso de revisión). 3.8.1.1 Dashboards 3.8.1.2 Elementos 3.8.1.3 Datos espaciales Datos temporales Datos jerarquicos Datos de redes Datos de texto Tarea: resumir los siguientes puntos del libro: Big Data and Social Science, Ian Foster. 3.8.2 Error e inferencia 3.8.3 Privacidad y confidencialidad 3.8.4 Workbooks 3.9 Ejercicios Propuestos Explorar los métodos cuasi-experimentales que existen Buscar informacion respecto a: los matriculados en educacion regular y universidad por año y departamento en Bolivia Empleando la fuente anterior, generar en R el có digo que cargue el archivo encontrado Buscar dos papers (1) donde se uso machine learning y (2) análisis de texto y comentar con al menos 5000 caractéres Buscar ejemplos (al menos uno) de bases de datos, páginas web u otros asociados a datos que no respeten los principios de privacidad y confidencialidad. "],
["big-data-en-r.html", "Chapter 4 Big Data en R 4.1 Limitaciones en R 4.2 A los límites de la memoria y más allá 4.3 Parallel R", " Chapter 4 Big Data en R R en un software que se adapta perfectamente con las fases del ciclo de vida de los proyectos de datos, ofreciendo herramientas para cada una de ellas; Entre las mas interesantes, R Markdown, Shiny, sweave, etc. Adicionalmente existen multiples librerias que permite trabajar con otros software, principalmente los asociados a bases de datos. En el capitulo 1 se enfatiza el uso de R para el manejo, procesamiento y análisis de bases de datos, sumado a esto el conocimiento de distintas estadísticas en base a los cursos de formación, en este capitulo se buscar unir ambos topicos y explicar como usar el poder del modelado matematico y de datos en R, considerando bases de datos grandes, esto sin necesidad de recurrir a otros equipos. Lo que se espera a partir de este capitulo es: Conocer las limitaciones de R para el Big Data y como ellas pueden resolverse El uso de las librerias ff, ffbase, ffbase2, y bigmemory, para el manejo de la memoria del equipo El uso de metodos estadisticos para objetos grandes en R, atraves de las librerias bigglm y ffbase Mejorar la velocidad del procesamiento de datos con librerias que permiten la computación paralela (parallel computing) Manipulación de datos más rápidos con el uso de la libreria data.table 4.1 Limitaciones en R A manera que uno va aprendiendo mas del R en la universidad o en el trabajo se valora cada vez mas la flexibilidad de R, su constante crecimiento, la ventaja de ser de codigo abierto, etc. Sin embargo, es importante tener en cuenta las limitaciones de R: Los datos se ajustan a la RAM R es generalmente muy lento comparado con otros lenguajes 4.1.1 Memoria R permite trabajar con bases de datos que no superen el 50-60% de la RAM de la computadora Existen soluciones en R sin tener que recurrir aun a plataformas como; Microsoft Azure, Amazon EC2, or Google Cloud Platform. Existe tambien la opción de trabajar con RStudio-Cloud, https://rstudio.cloud/plans/free 4.1.2 Velocidad de procesamiento R se considera un lenguaje interpretado y, como tal, su ejecución de código más lenta viene por definición. Procesos linea por linea El bajo rendimiento del código R puede deberse al hecho de que el lenguaje no está formalmente definido 4.2 A los límites de la memoria y más allá 4.2.1 Transformacion y agregación de datos con la librería ff y ffbase install.packages(&quot;ff&quot;) install.packages(&quot;ffbase&quot;) #install.packages(&quot;ffbase2&quot;) / github library(ff) library(ffbase) setwd(&quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata&quot;) bd1&lt;-read.csv(&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T) object.size(bd1) head(bd1) #paso 1: tener un directorio para archivos temporales de ff system(&quot;mkdir ffdf&quot;) #paso 2: definir la carpeta temporal options(fftempdir=&quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\ffdf&quot;) #paso 3: Cargar la base de datos bd2&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, next.rows=100000,colClasses=NA,VERBOSE=F) bd3&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, first.rows=-1,colClasses=NA,VERBOSE=T) bd4&lt;-read.csv.ffdf(file=&quot;200614COVID19MEXICO.csv&quot;,sep=&quot;,&quot;,header=T, next.rows=1000,colClasses=NA,VERBOSE=T) object.size(bd1)/1000000 # CSV DE R object.size(bd2)/1000000 # CSV.FFDF 100000 object.size(bd3)/1000000 # CSV.FFDF TODO object.size(bd4)/1000000 # CSV.FFDF 1000 #USO DE LAS FFDF table(bd2$SEXO) barplot(table(bd2$SEXO)) library(tictoc) tic(&quot;R&quot;) table(bd1$SEXO) toc() tic(&quot;ffdf&quot;) table(bd2$SEXO) toc() tic(&quot;ffdf table&quot;) table.ff(bd2$SEXO) toc() #R: 1 G -&gt; #FFDF: 200 M -&gt; class(bd1) class(bd2) bd5&lt;-as.data.frame.ffdf(bd2) library(Hmisc) describe(bd1) describe(bd5) object.size(bd5) object.size(bd1) object.size(bd2) Funciones para objetos ff #sobre la bd2 library(doBy) library(dplyr) res0&lt;-bd1 %&gt;% group_by(PAIS_NACIONALIDAD,SEXO) %&gt;% summarise(mean(EDAD)) bd2df&lt;-as.data.frame.ffdf(bd2) object.size(bd2df) bd2df %&gt;% group_by(PAIS_NACIONALIDAD,SEXO) %&gt;% summarise(mean(EDAD)) summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=bd1) summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=bd2df) tapply(bd1$EDAD, bd1$PAIS_NACIONALIDAD, mean) tapply(bd2$EDAD, bd2$PAIS_NACIONALIDAD, mean) res&lt;-ffdfdply(bd2,split=bd2$PAIS_NACIONALIDAD,FUN=function(x){ summaryBy(EDAD~PAIS_NACIONALIDAD+SEXO,FUN=mean,data=x) }) object.size(res0) object.size(res) ss1&lt;-subset(bd1,EDAD&gt;60) ss2&lt;-subset(bd2,EDAD&gt;60) ss3&lt;-subset.ffdf(bd2,EDAD&gt;60) aaa&lt;-bd1 bbb&lt;-bd1 rm(aaa,bbb) #memoria en R object.size(&quot;a&quot;) memory.size() memory.limit() (memory.size()/memory.limit())*100 memory.profile() #guardar objetos ffdf save.ffdf(res) rm(res) load.ffdf(&quot;ffdb&quot;) #convirtiendo a ff dd&lt;-as.ffdf(bd1) #forma lenta bd1$FECHA_ACTUALIZACION&lt;-as.factor(bd1$FECHA_ACTUALIZACION) str(bd1) #forma rapida i &lt;- sapply(bd1, is.character) bd1[,i] &lt;- lapply(bd1[,i], as.factor) str(bd1) dd&lt;-as.ffdf(bd1) 4.2.2 Modelos GLM con la las librerias ff y ffbase #install.packages(&quot;biglm&quot;) library(biglm) model1&lt;-lm(EDAD~UCI,data = bd1) model2&lt;-lm(EDAD~UCI,data = bd2) model3&lt;-bigglm.ffdf(EDAD~UCI,data = bd2) summary(model1) summary(model2) summary(model3) 4.2.3 Expandiendo la memoria con la libreria bigmemory rm(list=ls()) library(bigmemory) #limitación de bigmemory, todos los variables de la base de datos de interés deben ser numéricas. load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\oct20.RData&quot;) bd1&lt;-computo[,13:25] str(bd1) object.size(bd1)/10^6 bd2&lt;-as.big.matrix(as.matrix(bd1)) object.size(bd1) object.size(bd2) object.size(bd1)/object.size(bd2) #xx&lt;-read.big.matrix(&quot;.csv&quot;,sep=&quot;,&quot;) #alternativamente read.big.matrix dim(bd2) dimnames(bd2) head(bd2) summary(bd1) summary(bd2) mean(bd2[,1]) summary(bd2[,2]) names(bd2) dimnames(bd2) lm(CC~MNR,data=bd2) library(bigtabulate) library(biganalytics) library(biglm) library(bigstatsr) library(bigalgebra) # salio de R el 2020 library(bigpca)# salio el 2018 de R reg0&lt;-bigglm.big.matrix(CC~MNR,data=bd2) summary(reg0) reg&lt;-bigglm.big.matrix(CC~FPV,data=bd2) summary(reg) class(bd2) 4.3 Parallel R Esta seccion se enfoca en los métodos de computación paralelo, de tal forma que se pueda controlara la paralelizacion en una sola maquina. El objetivo es poder aprovechar los cores que tiene disponible los PC en la actualidad, R por defecto solo trabaja en una. Recursos online En R Un libro acerca del tema (aquí) Antes una función para calcular el tiempo de procesamiento. rm(list=ls()) teval&lt;-function(...){ gc() start&lt;-proc.time() result&lt;-eval(...) finish&lt;-proc.time() return(list(Duration=finish-start,Result=result)) } Veamos el rendimiento de lo visto antes. library(ff) library(ffbase) library(bigmemory) library(biganalytics) mr&lt;-matrix(rnorm(10000000*10),nrow = 10000000,ncol=10) mr&lt;-cbind(mr,binom=rbinom(10000000,5,0.7)) mbm&lt;-as.big.matrix(mr) mff&lt;-as.ffdf(as.data.frame(mr)) class(mr) class(mbm) class(mff) object.size(mr)/10^6 object.size(mbm)/10^6 object.size(mff)/10^6 teval(colMeans(mr)) teval(colmean(mbm)) #teval(colMeans(mff)) teval(apply(mr,2,mean)) teval(apply(mbm,2,mean)) teval(apply(mff,2,mean)) resul&lt;-list() teval( for(i in 1:ncol(mff)){ resul[[i]]&lt;-mean.ff(mff[[i]]) } ) teval(sapply(mff,mean)) teval(tapply(mr[,1], mr[,11], mean)) teval(tapply(mbm[,1], mbm[,11], mean)) teval(tapply(mff[[1]], mff[[11]], mean.ff)) aux&lt;-matrix(NA,6,10) teval( for(i in 1:10){ for(j in unique(mff[,11])){ aux[j+1,i]&lt;-mean(mff[mff[,11]==j,i]) } } ) 4.3.1 Libreria parallel Ahora sí, veamos la librería parallel y snow. Disponible desde la versión 2.14.0 Existen otras populares; multicore, snow (Simple Network of Workstations). La primera descontinuada y la segunda limitada. El manual https://stat.ethz.ch/R-manual/R-devel/library/parallel/doc/parallel.pdf library(parallel) library(snow) #funciones propias mclapply()#no funciona completamente en windows parLapply() library(help=parallel) library(help=snow) #viendo los cores disponibles detectCores() #definiendo un cluster de trabajo #SOCK usa Rscript para lanzar más copias de R (en el mismo host u opcionalmente en otro lugar) #FORK makeForkCluster bifurca a los trabajadores en el host actual (que hereda el entorno de la sesión actual) (NO WINDOWS) cl &lt;- makeCluster(3, type = &quot;SOCK&quot;) stopCluster(cl) cl &lt;- makeCluster(2, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) cl &lt;- makeCluster(3, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) cl &lt;- makeCluster(4, type = &quot;SOCK&quot;) teval(parApply(cl,mr,2,mean)) stopCluster(cl) #parApply(cl,mff[[1]],1,mean) #parApply(cl,mbm,2,mean) A&lt;-matrix(10^6,10) B&lt;-matrix(10^6,10) C&lt;-matrix(10^6,10) mclapply(X=list(A,B,C),FUN=mean,mc.preschedule = F,affinity.list = c(1,1,2)) mclapply(X=list(A,B,C),FUN=mean) "],
["r-y-spark.html", "Chapter 5 R y Spark 5.1 Introducción 5.2 Arrancando Spark 5.3 Análisis con Spark 5.4 Modelado", " Chapter 5 R y Spark 5.1 Introducción En este capítulo emplearemos Spark junto con R para el procesamiento de datos. Spark es un framework que permite realizar procesos de división y paralelismo en un equipo o múltiples equipos. 5.1.1 Hadoop Google (2004) publicó un nuevo documento que describe cómo realizar operaciones en todo el Sistema de archivos de Google, un enfoque que se conoció como MapReduce. Como era de esperar, hay dos operaciones en MapReduce: map y reduce. La operación de map proporciona una forma arbitraria de transformar cada archivo en un nuevo archivo, mientras que la operación de reduce combina dos archivos. Ambas operaciones requieren un código de computadora personalizado, pero el marco MapReduce se encarga de ejecutarlas automáticamente en muchas computadoras a la vez. Estas dos operaciones son suficientes para procesar todos los datos disponibles en la web, al tiempo que proporcionan suficiente flexibilidad para extraer información significativa de la misma. 5.1.2 Spark En 2009, Apache Spark comenzó como un proyecto de investigación en AMPLab de UC Berkeley para mejorar MapReduce. Específicamente, Spark proporcionó un conjunto más rico de verbos más allá de MapReduce para facilitar la optimización del código que se ejecuta en múltiples máquinas. Spark también cargó datos en la memoria, lo que hace que las operaciones sean mucho más rápidas que el almacenamiento en disco de Hadoop Spark Componetes: Spark Core: Base donde se apoya el resto de los componentes Spark SQL: Procesamiento de dato estructurados y no estructurados Spark streaming: Procesamiento de datos en tiempo real. Spark MLLib: Machine learning Spark graph: Procesamiento de grafos Algunos conceptos importantes HDFS: Sistema de ficheros 5.1.3 Sparklyr Oficialmente, sparklyr es una interfaz R para Apache Spark. Está disponible en CRAN y funciona como cualquier otro paquete de CRAN, lo que significa que es independiente de las versiones de Spark, es fácil de instalar, sirve a la comunidad R, abarca otros paquetes y prácticas de la comunidad R, y así sucesivamente. Está alojado en GitHub y tiene licencia de Apache 2.0, que le permite clonar, modificar y contribuir de nuevo a este proyecto. 5.2 Arrancando Spark 5.2.1 Prerequisitos R RStudio Java 8 system(&quot;java -version&quot;) 5.2.2 Instalación #Habilitando la libreria library(sparklyr) #versiones disponibles spark_available_versions() #versiones instaladas spark_installed_versions() #instalando spark desde R spark_install() #si se quiere desinstalar alguna versión en particular 5.2.3 Creando una sesión en Spark #configuración conf &lt;- spark_config() conf$`sparklyr.shell.driver-memory` &lt;- &quot;2G&quot; conf$spark.memory.fraction &lt;- 0.8 #conección sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.4.3&quot;,config = conf) #apagar conexión spark_disconnect(sc) spark_disconnect_all() 5.2.4 Interface web spark_web(sc) http://localhost:4040/jobs/ http://127.0.0.1:4040/executors/ Jobs Stages Storage Environment Executors SQL 5.3 Análisis con Spark Pasos en el análisis de datos 5.3.1 Importando los datos Por lo general, importar significa que R leerá los archivos y los cargará en la memoria; cuando se usa Spark, los datos se importan a Spark, no a R. Pasos en el análisis de datos Pasos en el análisis de datos Colección de documentos #Schema top_rows &lt;- read.csv(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\covid_mx\\\\200627COVID19MEXICO.csv&quot;,sep=&quot;,&quot;, nrows = 5) spec_with_r &lt;- sapply(top_rows, class) spec_with_r #csv/covid mexico sp_covid&lt;-spark_read_csv(sc, name=&quot;covid&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\covid_mx\\\\&quot;) # esquema sp_covid&lt;-spark_read_csv(sc, name=&quot;covid&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\covid_mx\\\\&quot;, columns = spec_with_r) # memoria object.size(sp_covid) #spark SQL library(DBI) top10 &lt;- dbGetQuery(sc, &quot;Select * from covid limit 10&quot;) top10 Comando copy_to load(&quot;C:\\\\Users\\\\ALVARO\\\\Documents\\\\GitHub\\\\EST-383\\\\data\\\\oct20.RData&quot;) oct20&lt;-copy_to(sc,computo) Documento específico sp_covid2&lt;-spark_read_csv(sc, name=&quot;covid2&quot;, path = &quot;C:\\\\Users\\\\ALVARO\\\\Desktop\\\\db_bolivia\\\\bigdata\\\\200614COVID19MEXICO.csv&quot;, columns = spec_with_r) 5.3.2 Librería DPLYR (Gramática de manipulación de datos) https://cran.r-project.org/web/packages/dplyr/vignettes/dplyr.html sp_covid %&gt;% tally sp_covid2 %&gt;% tally oct20 %&gt;% tally covid_cache &lt;- sp_covid %&gt;% compute(&quot;covid_ch&quot;) covid_cache %&gt;% tally dimnames(sp_covid) sp_covid %&gt;% count(SEXO) sp_covid2 %&gt;% count(SEXO,NEUMONIA) sp_covid2 %&gt;% group_by(SEXO) %&gt;% summarise(mean(EDAD),na.rm=T) 5.4 Modelado "]
]
