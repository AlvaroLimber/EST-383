20/11/18 16:47:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 16:47:59 INFO SparkContext: Running Spark version 2.4.3
20/11/18 16:47:59 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 16:47:59 INFO SparkContext: Submitted application: sparklyr
20/11/18 16:48:00 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 16:48:00 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 16:48:00 INFO SecurityManager: Changing view acls groups to: 
20/11/18 16:48:00 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 16:48:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 16:48:00 INFO Utils: Successfully started service 'sparkDriver' on port 61815.
20/11/18 16:48:00 INFO SparkEnv: Registering MapOutputTracker
20/11/18 16:48:00 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 16:48:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 16:48:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 16:48:01 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-98603ac3-8dfd-4c0d-a628-e55e41d8cad3
20/11/18 16:48:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/11/18 16:48:01 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 16:48:01 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 16:48:01 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 16:48:02 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 16:48:02 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:61815/jars/sparklyr-2.4-2.11.jar with timestamp 1605732482214
20/11/18 16:48:02 INFO Executor: Starting executor ID driver on host localhost
20/11/18 16:48:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61836.
20/11/18 16:48:03 INFO NettyBlockTransferService: Server created on 127.0.0.1:61836
20/11/18 16:48:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 16:48:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 61836, None)
20/11/18 16:48:03 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:61836 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 61836, None)
20/11/18 16:48:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 61836, None)
20/11/18 16:48:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 61836, None)
20/11/18 16:48:04 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
20/11/18 16:48:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
20/11/18 16:48:04 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
20/11/18 16:48:07 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/11/18 16:48:19 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/11/18 16:48:21 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 16:48:21 INFO ObjectStore: ObjectStore, initialize called
20/11/18 16:48:22 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 16:48:22 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 16:48:26 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 16:48:28 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 16:48:28 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 16:48:29 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 16:48:29 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 16:48:29 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 16:48:29 INFO ObjectStore: Initialized ObjectStore
20/11/18 16:48:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/11/18 16:48:30 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 16:48:30 INFO HiveMetaStore: Added admin role in metastore
20/11/18 16:48:30 INFO HiveMetaStore: Added public role in metastore
20/11/18 16:48:30 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 16:48:32 INFO HiveMetaStore: 0: get_all_databases
20/11/18 16:48:32 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_databases	
20/11/18 16:48:32 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/11/18 16:48:32 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/11/18 16:48:32 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 16:48:34 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/Temp/67bf7f76-207b-4f86-bf30-edf45dd56fad_resources
20/11/18 16:48:35 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/67bf7f76-207b-4f86-bf30-edf45dd56fad
20/11/18 16:48:35 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/67bf7f76-207b-4f86-bf30-edf45dd56fad
20/11/18 16:48:35 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/67bf7f76-207b-4f86-bf30-edf45dd56fad/_tmp_space.db
20/11/18 16:48:35 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
20/11/18 16:48:35 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:48:35 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:48:35 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 16:48:35 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 16:48:35 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 16:48:35 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:48:35 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:48:35 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:48:35 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:48:35 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 16:48:35 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 16:48:37 INFO CodeGenerator: Code generated in 487.8139 ms
20/11/18 16:48:38 INFO CodeGenerator: Code generated in 40.0399 ms
20/11/18 16:48:38 INFO CodeGenerator: Code generated in 17.0835 ms
20/11/18 16:48:39 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 16:48:39 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:114)
20/11/18 16:48:39 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 16:48:39 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 16:48:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 16:48:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/18 16:48:39 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114), which has no missing parents
20/11/18 16:48:40 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
20/11/18 16:48:40 INFO ContextCleaner: Cleaned accumulator 1
20/11/18 16:48:40 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
20/11/18 16:48:40 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:61836 (size: 4.2 KB, free: 912.3 MB)
20/11/18 16:48:40 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/11/18 16:48:40 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 16:48:40 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/11/18 16:48:41 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
20/11/18 16:48:41 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/18 16:48:41 INFO Executor: Fetching spark://127.0.0.1:61815/jars/sparklyr-2.4-2.11.jar with timestamp 1605732482214
20/11/18 16:48:41 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:61815 after 67 ms (0 ms spent in bootstraps)
20/11/18 16:48:41 INFO Utils: Fetching spark://127.0.0.1:61815/jars/sparklyr-2.4-2.11.jar to C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e\fetchFileTemp1593492367298593126.tmp
20/11/18 16:48:41 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-f7f57ceb-404e-4584-a98e-04de82013434/userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e/sparklyr-2.4-2.11.jar to class loader
20/11/18 16:48:42 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver
20/11/18 16:48:42 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1447 ms on localhost (executor driver) (1/1)
20/11/18 16:48:42 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/18 16:48:42 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:114) finished in 2,633 s
20/11/18 16:48:42 INFO DAGScheduler: looking for newly runnable stages
20/11/18 16:48:42 INFO DAGScheduler: running: Set()
20/11/18 16:48:42 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/18 16:48:42 INFO DAGScheduler: failed: Set()
20/11/18 16:48:42 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114), which has no missing parents
20/11/18 16:48:42 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
20/11/18 16:48:42 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
20/11/18 16:48:42 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:61836 (size: 3.8 KB, free: 912.3 MB)
20/11/18 16:48:42 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/11/18 16:48:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 16:48:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 16:48:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 16:48:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/18 16:48:42 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 16:48:42 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 23 ms
20/11/18 16:48:42 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1775 bytes result sent to driver
20/11/18 16:48:42 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 208 ms on localhost (executor driver) (1/1)
20/11/18 16:48:42 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 16:48:42 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0,255 s
20/11/18 16:48:42 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 3,372757 s
20/11/18 16:48:43 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:48:43 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:48:43 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:48:43 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:48:43 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 16:48:43 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 16:48:43 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 16:48:43 INFO DAGScheduler: Registering RDD 10 (count at utils.scala:114)
20/11/18 16:48:43 INFO DAGScheduler: Got job 1 (count at utils.scala:114) with 1 output partitions
20/11/18 16:48:43 INFO DAGScheduler: Final stage: ResultStage 3 (count at utils.scala:114)
20/11/18 16:48:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)
20/11/18 16:48:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 2)
20/11/18 16:48:43 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:114), which has no missing parents
20/11/18 16:48:43 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
20/11/18 16:48:43 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
20/11/18 16:48:43 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:61836 (size: 4.2 KB, free: 912.3 MB)
20/11/18 16:48:43 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
20/11/18 16:48:43 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 16:48:43 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/11/18 16:48:43 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
20/11/18 16:48:43 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
20/11/18 16:48:43 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1456 bytes result sent to driver
20/11/18 16:48:43 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 29 ms on localhost (executor driver) (1/1)
20/11/18 16:48:43 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/11/18 16:48:43 INFO DAGScheduler: ShuffleMapStage 2 (count at utils.scala:114) finished in 0,038 s
20/11/18 16:48:43 INFO DAGScheduler: looking for newly runnable stages
20/11/18 16:48:43 INFO DAGScheduler: running: Set()
20/11/18 16:48:43 INFO DAGScheduler: waiting: Set(ResultStage 3)
20/11/18 16:48:43 INFO DAGScheduler: failed: Set()
20/11/18 16:48:43 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:114), which has no missing parents
20/11/18 16:48:43 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
20/11/18 16:48:43 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
20/11/18 16:48:43 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:61836 (size: 3.8 KB, free: 912.3 MB)
20/11/18 16:48:43 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
20/11/18 16:48:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[13] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 16:48:43 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/11/18 16:48:43 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 16:48:43 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/11/18 16:48:43 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 16:48:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/11/18 16:48:43 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1775 bytes result sent to driver
20/11/18 16:48:43 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 23 ms on localhost (executor driver) (1/1)
20/11/18 16:48:43 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/11/18 16:48:43 INFO DAGScheduler: ResultStage 3 (count at utils.scala:114) finished in 0,035 s
20/11/18 16:48:43 INFO DAGScheduler: Job 1 finished: count at utils.scala:114, took 0,083280 s
20/11/18 16:49:32 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 16:49:32 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 16:49:32 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 16:49:32 INFO MemoryStore: MemoryStore cleared
20/11/18 16:49:32 INFO BlockManager: BlockManager stopped
20/11/18 16:49:32 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 16:49:32 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 16:49:32 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 16:49:32 INFO SparkContext: Successfully stopped SparkContext
20/11/18 16:49:32 INFO ShutdownHookManager: Shutdown hook called
20/11/18 16:49:32 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434
20/11/18 16:49:32 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 16:49:32 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e
20/11/18 16:49:32 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-f7f57ceb-404e-4584-a98e-04de82013434\userFiles-1da1f1f3-2e12-4c22-ad17-27e6dc5af89e\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 16:49:32 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-d430109a-408a-4d83-a4f2-cfb45fd6347d
20/11/18 16:49:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 16:49:53 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 16:49:53 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 16:49:53 INFO SecurityManager: Changing view acls groups to: 
20/11/18 16:49:53 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 16:49:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 16:49:56 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 16:49:56 INFO SparkContext: Running Spark version 3.0.0
20/11/18 16:49:56 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 16:49:56 INFO ResourceUtils: ==============================================================
20/11/18 16:49:56 INFO ResourceUtils: Resources for spark.driver:

20/11/18 16:49:56 INFO ResourceUtils: ==============================================================
20/11/18 16:49:56 INFO SparkContext: Submitted application: sparklyr
20/11/18 16:49:56 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 16:49:56 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 16:49:56 INFO SecurityManager: Changing view acls groups to: 
20/11/18 16:49:56 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 16:49:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 16:49:56 INFO Utils: Successfully started service 'sparkDriver' on port 61945.
20/11/18 16:49:57 INFO SparkEnv: Registering MapOutputTracker
20/11/18 16:49:57 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 16:49:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 16:49:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 16:49:57 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 16:49:57 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-2c37fb4b-7d90-4255-be25-4d571666a928
20/11/18 16:49:57 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 16:49:57 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 16:49:57 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 16:49:57 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 16:49:58 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 16:49:58 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:61945/jars/sparklyr-3.0-2.12.jar with timestamp 1605732598110
20/11/18 16:49:58 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 16:49:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 61968.
20/11/18 16:49:58 INFO NettyBlockTransferService: Server created on 127.0.0.1:61968
20/11/18 16:49:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 16:49:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 61968, None)
20/11/18 16:49:58 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:61968 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 61968, None)
20/11/18 16:49:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 61968, None)
20/11/18 16:49:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 61968, None)
20/11/18 16:49:59 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 16:49:59 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 16:49:59 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 16:49:59 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 16:50:05 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 16:50:05 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 16:50:06 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO
20/11/18 16:50:06 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/78aff927-108c-4626-a4db-19c71d54c22c
20/11/18 16:50:07 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/78aff927-108c-4626-a4db-19c71d54c22c
20/11/18 16:50:07 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/78aff927-108c-4626-a4db-19c71d54c22c/_tmp_space.db
20/11/18 16:50:07 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 16:50:08 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 16:50:08 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 16:50:08 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 16:50:08 INFO ObjectStore: ObjectStore, initialize called
20/11/18 16:50:08 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 16:50:08 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 16:50:11 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 16:50:11 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 16:50:21 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 16:50:21 INFO ObjectStore: Initialized ObjectStore
20/11/18 16:50:21 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 16:50:21 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 16:50:21 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 16:50:22 INFO HiveMetaStore: Added admin role in metastore
20/11/18 16:50:22 INFO HiveMetaStore: Added public role in metastore
20/11/18 16:50:22 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_all_functions
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 16:50:22 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_database: default
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 16:50:22 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 16:50:22 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 16:50:24 INFO CodeGenerator: Code generated in 514.7554 ms
20/11/18 16:50:24 INFO CodeGenerator: Code generated in 21.3334 ms
20/11/18 16:50:25 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 16:50:25 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 16:50:25 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 16:50:25 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 16:50:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 16:50:25 INFO DAGScheduler: Missing parents: List()
20/11/18 16:50:25 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 16:50:25 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 16:50:25 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 16:50:25 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:61968 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 16:50:25 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 16:50:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 16:50:25 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 16:50:25 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 16:50:25 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 16:50:25 INFO Executor: Fetching spark://127.0.0.1:61945/jars/sparklyr-3.0-2.12.jar with timestamp 1605732598110
20/11/18 16:50:25 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:61945 after 56 ms (0 ms spent in bootstraps)
20/11/18 16:50:25 INFO Utils: Fetching spark://127.0.0.1:61945/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af\fetchFileTemp8197431174931944497.tmp
20/11/18 16:50:26 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b/userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af/sparklyr-3.0-2.12.jar to class loader
20/11/18 16:50:27 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 16:50:27 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
20/11/18 16:50:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2727 bytes result sent to driver
20/11/18 16:50:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1727 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 16:50:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 16:50:27 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,125 s
20/11/18 16:50:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 16:50:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 16:50:27 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,233584 s
20/11/18 16:58:40 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 16:58:40 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 16:58:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 16:58:40 INFO MemoryStore: MemoryStore cleared
20/11/18 16:58:40 INFO BlockManager: BlockManager stopped
20/11/18 16:58:40 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 16:58:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 16:58:40 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 16:58:40 INFO SparkContext: Successfully stopped SparkContext
20/11/18 16:58:40 INFO ShutdownHookManager: Shutdown hook called
20/11/18 16:58:40 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-d92ac39a-e163-4189-8645-714ab58e6ebb
20/11/18 16:58:40 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af
20/11/18 16:58:40 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 16:58:40 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b
20/11/18 16:58:40 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-3c5977ab-7f54-4a52-a283-a386c72f6b2b\userFiles-34d04cf8-518d-463e-91f0-ce9fcb31b1af\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:04:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:04:06 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:04:06 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:04:06 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:04:06 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:04:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:04:09 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:04:09 INFO SparkContext: Running Spark version 3.0.0
20/11/18 17:04:09 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:04:10 INFO ResourceUtils: ==============================================================
20/11/18 17:04:10 INFO ResourceUtils: Resources for spark.driver:

20/11/18 17:04:10 INFO ResourceUtils: ==============================================================
20/11/18 17:04:10 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:04:10 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:04:10 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:04:10 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:04:10 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:04:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:04:10 INFO Utils: Successfully started service 'sparkDriver' on port 62376.
20/11/18 17:04:10 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:04:10 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:04:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:04:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:04:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 17:04:10 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-ecacbc8c-589c-45a0-a3ca-ac452956c946
20/11/18 17:04:10 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 17:04:10 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:04:10 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:04:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:04:11 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:04:11 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:62376/jars/sparklyr-3.0-2.12.jar with timestamp 1605733451386
20/11/18 17:04:11 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 17:04:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62399.
20/11/18 17:04:11 INFO NettyBlockTransferService: Server created on 127.0.0.1:62399
20/11/18 17:04:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:04:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 62399, None)
20/11/18 17:04:11 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:62399 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 62399, None)
20/11/18 17:04:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 62399, None)
20/11/18 17:04:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 62399, None)
20/11/18 17:04:12 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:04:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 17:04:12 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 17:04:12 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 17:04:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 17:04:18 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:04:20 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/7295c573-c9a0-4add-8578-6e809e56a2cc
20/11/18 17:04:20 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/7295c573-c9a0-4add-8578-6e809e56a2cc
20/11/18 17:04:20 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/7295c573-c9a0-4add-8578-6e809e56a2cc/_tmp_space.db
20/11/18 17:04:20 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 17:04:24 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 17:04:24 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 17:04:24 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:04:25 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:04:25 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:04:25 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:04:26 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 17:04:27 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:04:34 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:04:34 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:04:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 17:04:35 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 17:04:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:04:36 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:04:36 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:04:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_all_functions
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:04:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:04:36 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:04:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:04:38 INFO CodeGenerator: Code generated in 531.1161 ms
20/11/18 17:04:38 INFO CodeGenerator: Code generated in 21.5024 ms
20/11/18 17:04:39 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:04:39 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 17:04:39 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:04:39 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:04:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:04:39 INFO DAGScheduler: Missing parents: List()
20/11/18 17:04:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 17:04:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 17:04:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 17:04:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:62399 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:04:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 17:04:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:04:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:04:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 17:04:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 17:04:39 INFO Executor: Fetching spark://127.0.0.1:62376/jars/sparklyr-3.0-2.12.jar with timestamp 1605733451386
20/11/18 17:04:40 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:62376 after 68 ms (0 ms spent in bootstraps)
20/11/18 17:04:40 INFO Utils: Fetching spark://127.0.0.1:62376/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f\fetchFileTemp9077491122268850126.tmp
20/11/18 17:04:40 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-796f1bc6-049e-433c-98c3-f6ee155794af/userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f/sparklyr-3.0-2.12.jar to class loader
20/11/18 17:04:41 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 17:04:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 20 ms
20/11/18 17:04:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/11/18 17:04:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1830 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 17:04:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:04:41 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,203 s
20/11/18 17:04:41 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 17:04:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 17:04:41 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,301972 s
20/11/18 17:04:41 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:04:41 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:04:41 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:04:42 INFO MemoryStore: MemoryStore cleared
20/11/18 17:04:42 INFO BlockManager: BlockManager stopped
20/11/18 17:04:42 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:04:42 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:04:42 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:04:42 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:04:42 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:04:42 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-45d2e937-2119-4afc-b4d2-4641002de4d9
20/11/18 17:04:42 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af
20/11/18 17:04:42 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:04:42 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f
20/11/18 17:04:42 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-796f1bc6-049e-433c-98c3-f6ee155794af\userFiles-ebd8d19c-04bb-43dc-940f-4917c1ed251f\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:04:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:04:59 INFO SparkContext: Running Spark version 2.3.3
20/11/18 17:04:59 WARN SparkConf: In Spark 1.0 and later spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone and LOCAL_DIRS in YARN).
20/11/18 17:04:59 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:05:00 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:05:00 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:05:00 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:05:00 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:05:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:05:00 INFO Utils: Successfully started service 'sparkDriver' on port 62478.
20/11/18 17:05:00 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:05:00 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:05:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:05:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:05:00 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\blockmgr-591fd75e-04cd-48d6-a462-17c44e29a927
20/11/18 17:05:01 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/11/18 17:05:01 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:05:01 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:05:02 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:05:02 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:05:03 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-2.3-2.11.jar at spark://127.0.0.1:62478/jars/sparklyr-2.3-2.11.jar with timestamp 1605733503141
20/11/18 17:05:03 INFO Executor: Starting executor ID driver on host localhost
20/11/18 17:05:03 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62499.
20/11/18 17:05:03 INFO NettyBlockTransferService: Server created on 127.0.0.1:62499
20/11/18 17:05:03 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:05:03 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 62499, None)
20/11/18 17:05:03 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:62499 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 62499, None)
20/11/18 17:05:03 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 62499, None)
20/11/18 17:05:03 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 62499, None)
20/11/18 17:05:04 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:05:04 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive').
20/11/18 17:05:04 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive'.
20/11/18 17:05:08 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/11/18 17:05:14 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/11/18 17:05:16 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:05:16 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:05:16 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:05:16 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:05:19 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:05:21 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:05:21 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:05:22 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:05:22 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:05:23 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:05:23 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:05:24 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/11/18 17:05:26 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:05:28 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:05:28 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:05:28 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:05:29 INFO HiveMetaStore: 0: get_all_databases
20/11/18 17:05:29 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_databases	
20/11/18 17:05:29 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/11/18 17:05:29 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/11/18 17:05:29 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:05:29 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive/ALVARO
20/11/18 17:05:29 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/Temp/0db30ea3-3ee6-424f-94a2-9da60ec733b8_resources
20/11/18 17:05:29 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive/ALVARO/0db30ea3-3ee6-424f-94a2-9da60ec733b8
20/11/18 17:05:30 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive/0db30ea3-3ee6-424f-94a2-9da60ec733b8
20/11/18 17:05:30 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive/ALVARO/0db30ea3-3ee6-424f-94a2-9da60ec733b8/_tmp_space.db
20/11/18 17:05:30 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/hive
20/11/18 17:05:30 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:05:30 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:05:30 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:05:30 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:05:30 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:05:32 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:05:32 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:05:32 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:05:32 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:05:32 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:05:32 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:05:34 INFO CodeGenerator: Code generated in 472.456 ms
20/11/18 17:05:35 INFO CodeGenerator: Code generated in 41.7337 ms
20/11/18 17:05:35 INFO CodeGenerator: Code generated in 21.9673 ms
20/11/18 17:05:35 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:05:35 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:114)
20/11/18 17:05:35 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:05:35 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:05:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:05:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/18 17:05:36 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114), which has no missing parents
20/11/18 17:05:36 INFO ContextCleaner: Cleaned accumulator 1
20/11/18 17:05:36 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 8.3 KB, free 912.3 MB)
20/11/18 17:05:36 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.3 KB, free 912.3 MB)
20/11/18 17:05:36 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:62499 (size: 4.3 KB, free: 912.3 MB)
20/11/18 17:05:36 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1039
20/11/18 17:05:36 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:05:36 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/11/18 17:05:36 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7869 bytes)
20/11/18 17:05:36 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/18 17:05:36 INFO Executor: Fetching spark://127.0.0.1:62478/jars/sparklyr-2.3-2.11.jar with timestamp 1605733503141
20/11/18 17:05:37 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:62478 after 38 ms (0 ms spent in bootstraps)
20/11/18 17:05:37 INFO Utils: Fetching spark://127.0.0.1:62478/jars/sparklyr-2.3-2.11.jar to C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2\fetchFileTemp2118203313379218646.tmp
20/11/18 17:05:37 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.3.3-bin-hadoop2.7/tmp/local/spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be/userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2/sparklyr-2.3-2.11.jar to class loader
20/11/18 17:05:37 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver
20/11/18 17:05:37 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 989 ms on localhost (executor driver) (1/1)
20/11/18 17:05:37 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/18 17:05:37 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:114) finished in 1,896 s
20/11/18 17:05:37 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:05:37 INFO DAGScheduler: running: Set()
20/11/18 17:05:37 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/18 17:05:37 INFO DAGScheduler: failed: Set()
20/11/18 17:05:38 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114), which has no missing parents
20/11/18 17:05:38 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.5 KB, free 912.3 MB)
20/11/18 17:05:38 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.9 KB, free 912.3 MB)
20/11/18 17:05:38 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:62499 (size: 3.9 KB, free: 912.3 MB)
20/11/18 17:05:38 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1039
20/11/18 17:05:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:05:38 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:05:38 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7754 bytes)
20/11/18 17:05:38 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/18 17:05:38 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks out of 1 blocks
20/11/18 17:05:38 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 69 ms
20/11/18 17:05:38 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1775 bytes result sent to driver
20/11/18 17:05:38 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 216 ms on localhost (executor driver) (1/1)
20/11/18 17:05:38 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:05:38 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0,242 s
20/11/18 17:05:38 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,405145 s
20/11/18 17:05:48 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:05:48 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:05:48 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:05:48 INFO MemoryStore: MemoryStore cleared
20/11/18 17:05:48 INFO BlockManager: BlockManager stopped
20/11/18 17:05:48 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:05:48 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:05:48 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1074)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1947)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1361)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1946)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:573)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:05:48 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:05:48 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:05:48 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be
20/11/18 17:05:48 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1074)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:05:48 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2
20/11/18 17:05:48 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.3.3-bin-hadoop2.7\tmp\local\spark-d9e9ee99-9daa-470b-9af0-8e4e5862e7be\userFiles-5962b5c3-dc62-4826-89ea-b771b02bd4e2
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1074)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1992)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:05:48 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-78ed5de7-00c1-415b-87b9-c53882ba34c5
20/11/18 17:07:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:07:06 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:07:06 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:07:06 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:07:06 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:07:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:07:09 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:07:09 INFO SparkContext: Running Spark version 3.0.0
20/11/18 17:07:09 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:07:10 INFO ResourceUtils: ==============================================================
20/11/18 17:07:10 INFO ResourceUtils: Resources for spark.driver:

20/11/18 17:07:10 INFO ResourceUtils: ==============================================================
20/11/18 17:07:10 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:07:10 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:07:10 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:07:10 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:07:10 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:07:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:07:10 INFO Utils: Successfully started service 'sparkDriver' on port 62614.
20/11/18 17:07:10 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:07:10 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:07:10 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:07:10 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:07:10 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 17:07:10 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-52d84c40-6ea0-46a6-b742-585470968636
20/11/18 17:07:10 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 17:07:10 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:07:10 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:07:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:07:11 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:07:11 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:62614/jars/sparklyr-3.0-2.12.jar with timestamp 1605733631428
20/11/18 17:07:11 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 17:07:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 62637.
20/11/18 17:07:11 INFO NettyBlockTransferService: Server created on 127.0.0.1:62637
20/11/18 17:07:11 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:07:11 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 62637, None)
20/11/18 17:07:11 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:62637 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 62637, None)
20/11/18 17:07:11 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 62637, None)
20/11/18 17:07:11 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 62637, None)
20/11/18 17:07:12 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:07:12 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 17:07:12 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 17:07:12 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 17:07:18 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 17:07:18 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:07:20 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/319431e9-b4b4-488d-b033-3d2277cb1b7d
20/11/18 17:07:20 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/319431e9-b4b4-488d-b033-3d2277cb1b7d
20/11/18 17:07:20 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/319431e9-b4b4-488d-b033-3d2277cb1b7d/_tmp_space.db
20/11/18 17:07:20 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 17:07:21 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 17:07:21 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 17:07:21 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:07:21 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:07:21 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:07:21 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:07:22 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 17:07:24 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:07:35 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:07:35 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:07:35 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 17:07:35 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 17:07:35 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:07:36 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:07:36 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:07:36 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_all_functions
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:07:36 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:07:36 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:07:36 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:07:38 INFO CodeGenerator: Code generated in 494.2313 ms
20/11/18 17:07:38 INFO CodeGenerator: Code generated in 19.0035 ms
20/11/18 17:07:39 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:07:39 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 17:07:39 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:07:39 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:07:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:07:39 INFO DAGScheduler: Missing parents: List()
20/11/18 17:07:39 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 17:07:39 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 17:07:39 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 17:07:39 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:62637 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:07:39 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 17:07:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:07:39 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:07:39 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 17:07:39 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 17:07:39 INFO Executor: Fetching spark://127.0.0.1:62614/jars/sparklyr-3.0-2.12.jar with timestamp 1605733631428
20/11/18 17:07:40 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:62614 after 113 ms (0 ms spent in bootstraps)
20/11/18 17:07:40 INFO Utils: Fetching spark://127.0.0.1:62614/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4\fetchFileTemp5557045899120359077.tmp
20/11/18 17:07:41 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6/userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4/sparklyr-3.0-2.12.jar to class loader
20/11/18 17:07:41 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 17:07:41 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 18 ms
20/11/18 17:07:41 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/11/18 17:07:41 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 2308 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 17:07:41 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:07:42 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,667 s
20/11/18 17:07:42 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 17:07:42 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 17:07:42 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,766796 s
20/11/18 17:12:18 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:12:18 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:12:18 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:62637 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:12:18 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:12:18 INFO MemoryStore: MemoryStore cleared
20/11/18 17:12:18 INFO BlockManager: BlockManager stopped
20/11/18 17:12:18 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:12:18 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:12:18 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:12:18 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:12:18 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:12:18 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6
20/11/18 17:12:18 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:12:18 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4
20/11/18 17:12:18 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-078779a4-a41d-4b4d-956b-e8df3acc6fb6\userFiles-86279f6b-6d02-4a4b-aacf-7b0866bfd6d4\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:12:18 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-e39ff8ba-bfc7-47e8-a28a-8f67218ba2f7
20/11/18 17:23:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:23:39 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:23:39 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:23:39 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:23:39 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:23:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:23:42 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:23:43 INFO SparkContext: Running Spark version 3.0.0
20/11/18 17:23:43 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:23:43 INFO ResourceUtils: ==============================================================
20/11/18 17:23:43 INFO ResourceUtils: Resources for spark.driver:

20/11/18 17:23:43 INFO ResourceUtils: ==============================================================
20/11/18 17:23:43 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:23:43 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:23:43 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:23:43 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:23:43 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:23:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:23:43 INFO Utils: Successfully started service 'sparkDriver' on port 63054.
20/11/18 17:23:43 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:23:43 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:23:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:23:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:23:43 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 17:23:43 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-606f77cd-4e81-4a0c-bb8d-b0dfb1cf28a2
20/11/18 17:23:44 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 17:23:44 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:23:44 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:23:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:23:44 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:23:44 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:63054/jars/sparklyr-3.0-2.12.jar with timestamp 1605734624875
20/11/18 17:23:45 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 17:23:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63078.
20/11/18 17:23:45 INFO NettyBlockTransferService: Server created on 127.0.0.1:63078
20/11/18 17:23:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:23:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63078, None)
20/11/18 17:23:45 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63078 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 63078, None)
20/11/18 17:23:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63078, None)
20/11/18 17:23:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 63078, None)
20/11/18 17:23:46 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:23:46 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 17:23:46 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 17:23:46 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 17:23:56 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 17:23:57 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 17:23:57 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:24:04 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/bcab2409-5dee-487f-a020-c4ab8833e095
20/11/18 17:24:04 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/bcab2409-5dee-487f-a020-c4ab8833e095
20/11/18 17:24:04 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/bcab2409-5dee-487f-a020-c4ab8833e095/_tmp_space.db
20/11/18 17:24:04 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 17:24:06 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 17:24:06 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 17:24:06 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:24:06 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:24:06 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:24:06 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:24:09 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:24:12 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:24:12 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:24:12 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 17:24:12 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 17:24:13 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:24:13 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:24:13 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:24:13 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_all_functions
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:24:13 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:24:13 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:24:13 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:24:19 INFO CodeGenerator: Code generated in 486.3705 ms
20/11/18 17:24:19 INFO CodeGenerator: Code generated in 21.5185 ms
20/11/18 17:24:19 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:24:19 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 17:24:19 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:24:19 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:24:19 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:24:19 INFO DAGScheduler: Missing parents: List()
20/11/18 17:24:19 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 17:24:19 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 17:24:19 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 17:24:19 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:63078 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:24:19 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 17:24:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:24:19 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:24:20 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 17:24:20 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 17:24:20 INFO Executor: Fetching spark://127.0.0.1:63054/jars/sparklyr-3.0-2.12.jar with timestamp 1605734624875
20/11/18 17:24:20 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:63054 after 135 ms (0 ms spent in bootstraps)
20/11/18 17:24:20 INFO Utils: Fetching spark://127.0.0.1:63054/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a\fetchFileTemp6074995955804498225.tmp
20/11/18 17:24:21 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-525834f9-db18-4c1f-8907-3472ff1d2df7/userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a/sparklyr-3.0-2.12.jar to class loader
20/11/18 17:24:22 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 17:24:22 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 15 ms
20/11/18 17:24:22 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/11/18 17:24:22 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 2497 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 17:24:22 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:24:22 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,872 s
20/11/18 17:24:22 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 17:24:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 17:24:22 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,972798 s
20/11/18 17:30:28 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:63078 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:33:01 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:33:01 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:33:01 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:33:01 INFO MemoryStore: MemoryStore cleared
20/11/18 17:33:01 INFO BlockManager: BlockManager stopped
20/11/18 17:33:01 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:33:01 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:33:01 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:33:01 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:33:01 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:33:01 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7
20/11/18 17:33:01 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:33:01 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-28e8b2f6-ce82-4c3f-81d5-5e073733b53a
20/11/18 17:33:01 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a
20/11/18 17:33:01 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-525834f9-db18-4c1f-8907-3472ff1d2df7\userFiles-5fd569ca-386b-4b7f-9799-f865ba3b402a\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:33:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:33:18 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:33:18 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:33:18 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:33:18 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:33:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:33:21 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:33:22 INFO SparkContext: Running Spark version 3.0.0
20/11/18 17:33:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:33:22 INFO ResourceUtils: ==============================================================
20/11/18 17:33:22 INFO ResourceUtils: Resources for spark.driver:

20/11/18 17:33:22 INFO ResourceUtils: ==============================================================
20/11/18 17:33:22 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:33:22 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:33:22 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:33:22 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:33:22 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:33:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:33:22 INFO Utils: Successfully started service 'sparkDriver' on port 63312.
20/11/18 17:33:22 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:33:22 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:33:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:33:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:33:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 17:33:22 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-3d58d75a-e13b-40d6-a02a-75025affb8a3
20/11/18 17:33:22 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 17:33:22 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:33:22 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:33:23 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:33:23 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:33:23 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:63312/jars/sparklyr-3.0-2.12.jar with timestamp 1605735203862
20/11/18 17:33:24 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 17:33:24 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63336.
20/11/18 17:33:24 INFO NettyBlockTransferService: Server created on 127.0.0.1:63336
20/11/18 17:33:24 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:33:24 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63336, None)
20/11/18 17:33:24 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63336 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 63336, None)
20/11/18 17:33:24 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63336, None)
20/11/18 17:33:24 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 63336, None)
20/11/18 17:33:25 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:33:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 17:33:25 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 17:33:25 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 17:33:31 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 17:33:35 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:33:40 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 17:33:41 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/c92e5f9d-3ef3-4618-b6ac-73bfc7e77ae2
20/11/18 17:33:41 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/c92e5f9d-3ef3-4618-b6ac-73bfc7e77ae2
20/11/18 17:33:42 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/c92e5f9d-3ef3-4618-b6ac-73bfc7e77ae2/_tmp_space.db
20/11/18 17:33:42 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 17:33:43 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 17:33:43 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 17:33:43 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:33:43 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:33:44 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:33:44 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:33:46 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:33:50 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:33:50 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:33:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 17:33:50 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 17:33:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:33:50 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:33:50 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:33:50 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:33:50 INFO HiveMetaStore: 0: get_all_functions
20/11/18 17:33:50 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 17:33:50 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:33:50 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:33:51 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:33:51 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:33:51 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:33:51 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:33:51 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:33:51 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:33:51 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:33:51 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:33:51 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:34:01 INFO CodeGenerator: Code generated in 3550.764 ms
20/11/18 17:34:01 INFO CodeGenerator: Code generated in 24.1271 ms
20/11/18 17:34:01 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:34:01 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 17:34:01 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:34:01 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:34:01 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:34:01 INFO DAGScheduler: Missing parents: List()
20/11/18 17:34:01 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 17:34:02 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 17:34:02 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 17:34:02 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:63336 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:34:02 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 17:34:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:34:02 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:34:02 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 17:34:02 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 17:34:02 INFO Executor: Fetching spark://127.0.0.1:63312/jars/sparklyr-3.0-2.12.jar with timestamp 1605735203862
20/11/18 17:34:03 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:63312 after 154 ms (0 ms spent in bootstraps)
20/11/18 17:34:03 INFO Utils: Fetching spark://127.0.0.1:63312/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7\fetchFileTemp7094286298631661508.tmp
20/11/18 17:34:03 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-5f475629-1f32-471d-a198-6509176ac2d1/userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7/sparklyr-3.0-2.12.jar to class loader
20/11/18 17:34:04 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 17:34:04 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 26 ms
20/11/18 17:34:04 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/11/18 17:34:04 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 2346 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 17:34:04 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:34:04 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,887 s
20/11/18 17:34:04 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 17:34:04 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 17:34:04 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 3,025956 s
20/11/18 17:36:55 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:36:55 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:36:55 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:36:55 INFO MemoryStore: MemoryStore cleared
20/11/18 17:36:55 INFO BlockManager: BlockManager stopped
20/11/18 17:36:55 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:36:55 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:36:55 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:36:55 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:36:55 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:36:55 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7
20/11/18 17:36:55 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:36:55 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1
20/11/18 17:36:55 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-5f475629-1f32-471d-a198-6509176ac2d1\userFiles-9b1ee36d-583a-439f-9ecd-0eccfefc0dc7\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:36:55 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-0eceea24-5fdd-4035-8a35-1bdd7e6b8cd2
20/11/18 17:37:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:37:17 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:37:17 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:37:17 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:37:17 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:37:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:37:20 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:37:20 INFO SparkContext: Running Spark version 3.0.0
20/11/18 17:37:20 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:37:20 INFO ResourceUtils: ==============================================================
20/11/18 17:37:20 INFO ResourceUtils: Resources for spark.driver:

20/11/18 17:37:20 INFO ResourceUtils: ==============================================================
20/11/18 17:37:20 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:37:20 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:37:20 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:37:20 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:37:20 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:37:20 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:37:20 INFO Utils: Successfully started service 'sparkDriver' on port 63571.
20/11/18 17:37:20 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:37:20 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:37:20 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:37:20 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:37:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
20/11/18 17:37:21 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\blockmgr-80bf4e37-6aab-484c-ac0e-6e788c2642e9
20/11/18 17:37:21 INFO MemoryStore: MemoryStore started with capacity 912.3 MiB
20/11/18 17:37:21 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:37:21 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:37:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:37:21 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:37:21 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-3.0-2.12.jar at spark://127.0.0.1:63571/jars/sparklyr-3.0-2.12.jar with timestamp 1605735441887
20/11/18 17:37:22 INFO Executor: Starting executor ID driver on host 127.0.0.1
20/11/18 17:37:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63594.
20/11/18 17:37:22 INFO NettyBlockTransferService: Server created on 127.0.0.1:63594
20/11/18 17:37:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:37:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63594, None)
20/11/18 17:37:22 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63594 with 912.3 MiB RAM, BlockManagerId(driver, 127.0.0.1, 63594, None)
20/11/18 17:37:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63594, None)
20/11/18 17:37:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 63594, None)
20/11/18 17:37:23 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:37:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive').
20/11/18 17:37:23 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive'.
20/11/18 17:37:23 WARN SharedState: Not allowing to set spark.sql.warehouse.dir or hive.metastore.warehouse.dir in SparkSession's options, it should be set statically for cross-session usages
20/11/18 17:37:29 INFO HiveUtils: Initializing HiveMetastoreConnection version 2.3.7 using Spark classes.
20/11/18 17:37:29 INFO HiveConf: Found configuration file file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:37:30 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/782d361c-e0bc-4b5d-a64a-afe19aae81e9
20/11/18 17:37:30 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/782d361c-e0bc-4b5d-a64a-afe19aae81e9
20/11/18 17:37:30 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive/ALVARO/782d361c-e0bc-4b5d-a64a-afe19aae81e9/_tmp_space.db
20/11/18 17:37:30 INFO HiveClientImpl: Warehouse location for Hive client (version 2.3.7) is C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/hive
20/11/18 17:37:32 WARN HiveConf: HiveConf of name hive.stats.jdbc.timeout does not exist
20/11/18 17:37:32 WARN HiveConf: HiveConf of name hive.stats.retries.wait does not exist
20/11/18 17:37:32 INFO HiveMetaStore: 0: Opening raw store with implementation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:37:32 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:37:32 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:37:32 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:37:32 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped
20/11/18 17:37:35 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:37:38 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:37:38 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:37:38 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 2.3.0
20/11/18 17:37:38 WARN ObjectStore: setMetaStoreSchemaVersion called but recording version is disabled: version = 2.3.0, comment = Set by MetaStore UNKNOWN@192.168.1.223
20/11/18 17:37:38 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:37:39 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:37:39 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:37:39 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_all_functions
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_functions	
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:37:39 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:37:39 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:37:39 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:37:41 INFO CodeGenerator: Code generated in 482.6661 ms
20/11/18 17:37:41 INFO CodeGenerator: Code generated in 21.3227 ms
20/11/18 17:37:41 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:37:41 INFO DAGScheduler: Registering RDD 2 (count at utils.scala:114) as input to shuffle 0
20/11/18 17:37:41 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:37:41 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:37:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:37:41 INFO DAGScheduler: Missing parents: List()
20/11/18 17:37:41 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114), which has no missing parents
20/11/18 17:37:42 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 10.1 KiB, free 912.3 MiB)
20/11/18 17:37:42 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 5.0 KiB, free 912.3 MiB)
20/11/18 17:37:42 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:63594 (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:37:42 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1200
20/11/18 17:37:42 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:37:42 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:37:42 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 0, 127.0.0.1, executor driver, partition 0, PROCESS_LOCAL, 7325 bytes)
20/11/18 17:37:42 INFO Executor: Running task 0.0 in stage 1.0 (TID 0)
20/11/18 17:37:42 INFO Executor: Fetching spark://127.0.0.1:63571/jars/sparklyr-3.0-2.12.jar with timestamp 1605735441887
20/11/18 17:37:42 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:63571 after 78 ms (0 ms spent in bootstraps)
20/11/18 17:37:42 INFO Utils: Fetching spark://127.0.0.1:63571/jars/sparklyr-3.0-2.12.jar to C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557\fetchFileTemp5395943803780117396.tmp
20/11/18 17:37:42 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-3.0.0-bin-hadoop2.7/tmp/local/spark-d7e441a9-2f2f-48d6-960e-2780780a32a3/userFiles-79035ef5-62d3-4671-9513-f2b6e147c557/sparklyr-3.0-2.12.jar to class loader
20/11/18 17:37:43 INFO ShuffleBlockFetcherIterator: Getting 0 (0.0 B) non-empty blocks including 0 (0.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
20/11/18 17:37:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 39 ms
20/11/18 17:37:43 INFO Executor: Finished task 0.0 in stage 1.0 (TID 0). 2684 bytes result sent to driver
20/11/18 17:37:43 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 0) in 1716 ms on 127.0.0.1 (executor driver) (1/1)
20/11/18 17:37:44 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:37:44 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 2,067 s
20/11/18 17:37:44 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
20/11/18 17:37:44 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
20/11/18 17:37:44 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,161052 s
20/11/18 17:37:51 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:63594 in memory (size: 5.0 KiB, free: 912.3 MiB)
20/11/18 17:38:13 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:38:13 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:38:13 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:38:13 INFO MemoryStore: MemoryStore cleared
20/11/18 17:38:13 INFO BlockManager: BlockManager stopped
20/11/18 17:38:13 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:38:13 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:38:13 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:105)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2006)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2006)
	at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:626)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:38:13 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:38:13 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:38:13 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3
20/11/18 17:38:13 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:38:13 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-3eac6ee9-4df5-4c2d-b9c0-25ac11095822
20/11/18 17:38:13 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557
20/11/18 17:38:13 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-3.0.0-bin-hadoop2.7\tmp\local\spark-d7e441a9-2f2f-48d6-960e-2780780a32a3\userFiles-79035ef5-62d3-4671-9513-f2b6e147c557\sparklyr-3.0-2.12.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1079)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$4$adapted(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
	at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)
	at org.apache.spark.util.ShutdownHookManager$.$anonfun$new$2(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.util.Try$.apply(Try.scala:213)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:38:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:38:51 INFO SparkContext: Running Spark version 2.4.3
20/11/18 17:38:51 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:38:51 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:38:51 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:38:51 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:38:51 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:38:51 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:38:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:38:52 INFO Utils: Successfully started service 'sparkDriver' on port 63696.
20/11/18 17:38:52 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:38:52 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:38:52 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:38:52 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:38:52 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-b000e812-830c-4366-b113-26490c7b2ce1
20/11/18 17:38:52 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/11/18 17:38:52 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:38:52 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:38:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:38:53 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:38:53 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:63696/jars/sparklyr-2.4-2.11.jar with timestamp 1605735533438
20/11/18 17:38:53 INFO Executor: Starting executor ID driver on host localhost
20/11/18 17:38:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63718.
20/11/18 17:38:54 INFO NettyBlockTransferService: Server created on 127.0.0.1:63718
20/11/18 17:38:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:38:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63718, None)
20/11/18 17:38:54 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63718 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 63718, None)
20/11/18 17:38:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63718, None)
20/11/18 17:38:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 63718, None)
20/11/18 17:38:55 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:38:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
20/11/18 17:38:55 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
20/11/18 17:38:58 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/11/18 17:39:06 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/11/18 17:39:08 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:39:08 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:39:09 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:39:09 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:39:11 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:39:13 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:39:13 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:39:14 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:39:14 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:39:15 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:39:15 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:39:15 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/11/18 17:39:15 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:39:15 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:39:15 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:39:15 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:39:17 INFO HiveMetaStore: 0: get_all_databases
20/11/18 17:39:17 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_databases	
20/11/18 17:39:17 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/11/18 17:39:17 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/11/18 17:39:17 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:39:26 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/Temp/8ccb2979-32df-4db0-b4b1-a9648fbfdee3_resources
20/11/18 17:39:27 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/8ccb2979-32df-4db0-b4b1-a9648fbfdee3
20/11/18 17:39:27 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/8ccb2979-32df-4db0-b4b1-a9648fbfdee3
20/11/18 17:39:27 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/8ccb2979-32df-4db0-b4b1-a9648fbfdee3/_tmp_space.db
20/11/18 17:39:27 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
20/11/18 17:39:27 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:39:27 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:39:27 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:39:27 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:39:27 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:39:27 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:39:27 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:39:27 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:39:27 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:39:27 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:39:27 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:39:30 INFO CodeGenerator: Code generated in 601.4835 ms
20/11/18 17:39:32 INFO CodeGenerator: Code generated in 33.5477 ms
20/11/18 17:39:32 INFO CodeGenerator: Code generated in 15.3882 ms
20/11/18 17:39:32 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:39:32 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:114)
20/11/18 17:39:32 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:39:32 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:39:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:39:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/18 17:39:32 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114), which has no missing parents
20/11/18 17:39:32 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
20/11/18 17:39:33 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
20/11/18 17:39:33 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:63718 (size: 4.2 KB, free: 912.3 MB)
20/11/18 17:39:33 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/11/18 17:39:33 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:39:33 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/11/18 17:39:33 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
20/11/18 17:39:33 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/18 17:39:33 INFO Executor: Fetching spark://127.0.0.1:63696/jars/sparklyr-2.4-2.11.jar with timestamp 1605735533438
20/11/18 17:39:33 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:63696 after 102 ms (0 ms spent in bootstraps)
20/11/18 17:39:33 INFO Utils: Fetching spark://127.0.0.1:63696/jars/sparklyr-2.4-2.11.jar to C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292\fetchFileTemp2990065034058667945.tmp
20/11/18 17:39:34 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-1e20fad8-5b36-4450-802e-e71630840310/userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292/sparklyr-2.4-2.11.jar to class loader
20/11/18 17:39:34 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1585 bytes result sent to driver
20/11/18 17:39:34 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1402 ms on localhost (executor driver) (1/1)
20/11/18 17:39:34 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/18 17:39:34 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:114) finished in 1,880 s
20/11/18 17:39:34 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:39:34 INFO DAGScheduler: running: Set()
20/11/18 17:39:34 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/18 17:39:34 INFO DAGScheduler: failed: Set()
20/11/18 17:39:34 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114), which has no missing parents
20/11/18 17:39:34 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
20/11/18 17:39:34 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
20/11/18 17:39:34 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:63718 (size: 3.8 KB, free: 912.3 MB)
20/11/18 17:39:34 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/11/18 17:39:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:39:34 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:39:34 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:39:34 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/18 17:39:34 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:39:34 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 34 ms
20/11/18 17:39:34 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1818 bytes result sent to driver
20/11/18 17:39:34 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 178 ms on localhost (executor driver) (1/1)
20/11/18 17:39:34 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0,206 s
20/11/18 17:39:34 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:39:34 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 2,229655 s
20/11/18 17:40:05 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:40:06 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:40:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:40:06 INFO MemoryStore: MemoryStore cleared
20/11/18 17:40:06 INFO BlockManager: BlockManager stopped
20/11/18 17:40:06 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:40:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:40:06 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:40:06 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:40:06 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:40:06 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292
20/11/18 17:40:06 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:40:06 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310
20/11/18 17:40:06 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-1e20fad8-5b36-4450-802e-e71630840310\userFiles-1057b016-cdd7-4e54-9961-ddbc39c75292\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:40:06 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-56570d00-b7bb-41fb-a67a-4e5c3e88921b
20/11/18 17:40:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:40:25 INFO SparkContext: Running Spark version 2.4.3
20/11/18 17:40:25 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:40:25 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:40:25 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:40:25 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:40:25 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:40:25 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:40:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:40:25 INFO Utils: Successfully started service 'sparkDriver' on port 63833.
20/11/18 17:40:25 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:40:25 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:40:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:40:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:40:25 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-d01e18fb-8e71-47ea-a9c6-85cedda2de20
20/11/18 17:40:26 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/11/18 17:40:26 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:40:26 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:40:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:40:26 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:40:26 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:63833/jars/sparklyr-2.4-2.11.jar with timestamp 1605735626675
20/11/18 17:40:26 INFO Executor: Starting executor ID driver on host localhost
20/11/18 17:40:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 63854.
20/11/18 17:40:27 INFO NettyBlockTransferService: Server created on 127.0.0.1:63854
20/11/18 17:40:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:40:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 63854, None)
20/11/18 17:40:27 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:63854 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 63854, None)
20/11/18 17:40:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 63854, None)
20/11/18 17:40:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 63854, None)
20/11/18 17:40:27 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:40:27 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
20/11/18 17:40:27 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
20/11/18 17:40:37 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/11/18 17:40:44 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/11/18 17:40:45 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:40:45 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:40:45 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:40:45 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:40:48 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:40:50 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:40:50 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:40:51 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:40:51 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:40:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:40:52 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:40:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/11/18 17:40:56 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:41:00 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:41:00 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:41:01 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:41:02 INFO HiveMetaStore: 0: get_all_databases
20/11/18 17:41:02 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_databases	
20/11/18 17:41:02 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/11/18 17:41:02 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/11/18 17:41:02 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:41:03 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/Temp/f1781ac6-11f4-4f92-adc1-6888d0157f8c_resources
20/11/18 17:41:03 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/f1781ac6-11f4-4f92-adc1-6888d0157f8c
20/11/18 17:41:03 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/f1781ac6-11f4-4f92-adc1-6888d0157f8c
20/11/18 17:41:03 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/f1781ac6-11f4-4f92-adc1-6888d0157f8c/_tmp_space.db
20/11/18 17:41:03 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
20/11/18 17:41:03 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:41:03 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:41:03 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:41:03 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:41:03 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:41:03 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:41:03 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:41:03 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:41:03 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:41:03 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:41:03 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:41:05 INFO CodeGenerator: Code generated in 461.0792 ms
20/11/18 17:41:05 INFO CodeGenerator: Code generated in 34.7328 ms
20/11/18 17:41:05 INFO CodeGenerator: Code generated in 18.7799 ms
20/11/18 17:41:06 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:41:06 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:114)
20/11/18 17:41:06 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:41:06 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:41:06 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:41:06 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/18 17:41:06 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114), which has no missing parents
20/11/18 17:41:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
20/11/18 17:41:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
20/11/18 17:41:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:63854 (size: 4.2 KB, free: 912.3 MB)
20/11/18 17:41:06 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/11/18 17:41:06 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:41:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/11/18 17:41:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
20/11/18 17:41:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/18 17:41:06 INFO Executor: Fetching spark://127.0.0.1:63833/jars/sparklyr-2.4-2.11.jar with timestamp 1605735626675
20/11/18 17:41:06 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:63833 after 51 ms (0 ms spent in bootstraps)
20/11/18 17:41:06 INFO Utils: Fetching spark://127.0.0.1:63833/jars/sparklyr-2.4-2.11.jar to C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a\fetchFileTemp870947757992487760.tmp
20/11/18 17:41:07 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-d11511b9-a79d-4b1c-897f-58c6e9f41927/userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a/sparklyr-2.4-2.11.jar to class loader
20/11/18 17:41:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver
20/11/18 17:41:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 874 ms on localhost (executor driver) (1/1)
20/11/18 17:41:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/18 17:41:07 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:114) finished in 1,236 s
20/11/18 17:41:07 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:41:07 INFO DAGScheduler: running: Set()
20/11/18 17:41:07 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/18 17:41:07 INFO DAGScheduler: failed: Set()
20/11/18 17:41:07 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114), which has no missing parents
20/11/18 17:41:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
20/11/18 17:41:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
20/11/18 17:41:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:63854 (size: 3.8 KB, free: 912.3 MB)
20/11/18 17:41:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/11/18 17:41:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:41:07 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:41:07 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:41:07 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/18 17:41:07 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:41:07 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 35 ms
20/11/18 17:41:07 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1775 bytes result sent to driver
20/11/18 17:41:07 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 130 ms on localhost (executor driver) (1/1)
20/11/18 17:41:07 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0,152 s
20/11/18 17:41:07 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:41:07 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 1,543137 s
20/11/18 17:48:45 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:48:45 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:48:45 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:48:45 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:48:45 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:48:45 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:48:45 INFO SparkContext: Starting job: collect at utils.scala:44
20/11/18 17:48:45 INFO DAGScheduler: Got job 1 (collect at utils.scala:44) with 1 output partitions
20/11/18 17:48:45 INFO DAGScheduler: Final stage: ResultStage 2 (collect at utils.scala:44)
20/11/18 17:48:45 INFO DAGScheduler: Parents of final stage: List()
20/11/18 17:48:45 INFO DAGScheduler: Missing parents: List()
20/11/18 17:48:45 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[11] at map at utils.scala:41), which has no missing parents
20/11/18 17:48:45 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 6.0 KB, free 912.3 MB)
20/11/18 17:48:45 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 3.4 KB, free 912.3 MB)
20/11/18 17:48:46 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 127.0.0.1:63854 (size: 3.4 KB, free: 912.3 MB)
20/11/18 17:48:46 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1161
20/11/18 17:48:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[11] at map at utils.scala:41) (first 15 tasks are for partitions Vector(0))
20/11/18 17:48:46 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
20/11/18 17:48:46 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, executor driver, partition 0, PROCESS_LOCAL, 7893 bytes)
20/11/18 17:48:46 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
20/11/18 17:48:46 INFO CodeGenerator: Code generated in 21.1685 ms
20/11/18 17:48:46 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 927 bytes result sent to driver
20/11/18 17:48:46 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 74 ms on localhost (executor driver) (1/1)
20/11/18 17:48:46 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
20/11/18 17:48:46 INFO DAGScheduler: ResultStage 2 (collect at utils.scala:44) finished in 0,090 s
20/11/18 17:48:46 INFO DAGScheduler: Job 1 finished: collect at utils.scala:44, took 0,095871 s
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 81
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 82
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 85
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 52
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 38
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 72
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 58
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 22
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 41
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 80
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 63
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 1
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 67
20/11/18 17:48:58 INFO ContextCleaner: Cleaned shuffle 0
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 25
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 37
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 32
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 55
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 64
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 9
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 27
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 76
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 8
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 44
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 45
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 24
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 17
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 87
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 90
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 21
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 79
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 56
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 20
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 59
20/11/18 17:48:58 INFO ContextCleaner: Cleaned accumulator 18
20/11/18 17:48:58 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 127.0.0.1:63854 in memory (size: 3.4 KB, free: 912.3 MB)
20/11/18 17:48:59 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 36
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 14
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 50
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 7
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 84
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 3
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 48
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 86
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 33
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 35
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 40
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 51
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 69
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 16
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 2
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 31
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 47
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 68
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 28
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 74
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 39
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 4
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 66
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 49
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 83
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 71
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 60
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 61
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 42
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 26
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 46
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 10
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 12
20/11/18 17:48:59 INFO ContextCleaner: Cleaned accumulator 19
20/11/18 17:48:59 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 127.0.0.1:63854 in memory (size: 3.8 KB, free: 912.3 MB)
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 57
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 11
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 62
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 91
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 30
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 29
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 70
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 43
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 53
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 15
20/11/18 17:49:00 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 127.0.0.1:63854 in memory (size: 4.2 KB, free: 912.3 MB)
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 75
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 5
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 88
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 6
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 77
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 13
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 78
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 65
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 23
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 54
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 34
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 89
20/11/18 17:49:00 INFO ContextCleaner: Cleaned accumulator 73
20/11/18 17:49:00 INFO SparkContext: Starting job: sql at <unknown>:0
20/11/18 17:49:00 INFO DAGScheduler: Registering RDD 21 (sql at <unknown>:0)
20/11/18 17:49:00 INFO DAGScheduler: Got job 2 (sql at <unknown>:0) with 1 output partitions
20/11/18 17:49:00 INFO DAGScheduler: Final stage: ResultStage 4 (sql at <unknown>:0)
20/11/18 17:49:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
20/11/18 17:49:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
20/11/18 17:49:00 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[21] at sql at <unknown>:0), which has no missing parents
20/11/18 17:49:00 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 917.0 KB, free 911.4 MB)
20/11/18 17:49:00 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 261.9 KB, free 911.1 MB)
20/11/18 17:49:00 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 127.0.0.1:63854 (size: 261.9 KB, free: 912.0 MB)
20/11/18 17:49:00 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:00 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[21] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:00 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
20/11/18 17:49:01 INFO ContextCleaner: Cleaned accumulator 93
20/11/18 17:49:01 INFO ContextCleaner: Cleaned accumulator 0
20/11/18 17:49:01 WARN TaskSetManager: Stage 3 contains a task of very large size (17748 KB). The maximum recommended task size is 100 KB.
20/11/18 17:49:01 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, executor driver, partition 0, PROCESS_LOCAL, 18174057 bytes)
20/11/18 17:49:01 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
20/11/18 17:49:03 INFO CodeGenerator: Code generated in 1130.7617 ms
20/11/18 17:49:11 INFO CodeGenerator: Code generated in 6898.2214 ms
20/11/18 17:49:15 INFO MemoryStore: Block rdd_16_0 stored as values in memory (estimated size 7.9 MB, free 903.3 MB)
20/11/18 17:49:15 INFO BlockManagerInfo: Added rdd_16_0 in memory on 127.0.0.1:63854 (size: 7.9 MB, free: 904.2 MB)
20/11/18 17:49:15 INFO CodeGenerator: Code generated in 9.8532 ms
20/11/18 17:49:15 INFO CodeGenerator: Code generated in 21.3157 ms
20/11/18 17:49:15 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 1738 bytes result sent to driver
20/11/18 17:49:15 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 14839 ms on localhost (executor driver) (1/1)
20/11/18 17:49:15 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
20/11/18 17:49:15 INFO DAGScheduler: ShuffleMapStage 3 (sql at <unknown>:0) finished in 14,987 s
20/11/18 17:49:15 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:49:15 INFO DAGScheduler: running: Set()
20/11/18 17:49:15 INFO DAGScheduler: waiting: Set(ResultStage 4)
20/11/18 17:49:15 INFO DAGScheduler: failed: Set()
20/11/18 17:49:15 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[24] at sql at <unknown>:0), which has no missing parents
20/11/18 17:49:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 7.1 KB, free 903.3 MB)
20/11/18 17:49:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 3.8 KB, free 903.2 MB)
20/11/18 17:49:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 127.0.0.1:63854 (size: 3.8 KB, free: 904.1 MB)
20/11/18 17:49:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[24] at sql at <unknown>:0) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:15 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
20/11/18 17:49:15 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:49:15 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
20/11/18 17:49:15 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:49:15 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/11/18 17:49:15 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 1739 bytes result sent to driver
20/11/18 17:49:15 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 16 ms on localhost (executor driver) (1/1)
20/11/18 17:49:15 INFO DAGScheduler: ResultStage 4 (sql at <unknown>:0) finished in 0,024 s
20/11/18 17:49:15 INFO DAGScheduler: Job 2 finished: sql at <unknown>:0, took 15,028720 s
20/11/18 17:49:15 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
20/11/18 17:49:15 INFO CodeGenerator: Code generated in 6.8957 ms
20/11/18 17:49:15 INFO SparkContext: Starting job: collect at utils.scala:114
20/11/18 17:49:15 INFO DAGScheduler: Registering RDD 29 (collect at utils.scala:114)
20/11/18 17:49:15 INFO DAGScheduler: Got job 3 (collect at utils.scala:114) with 1 output partitions
20/11/18 17:49:15 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:114)
20/11/18 17:49:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
20/11/18 17:49:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
20/11/18 17:49:15 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[29] at collect at utils.scala:114), which has no missing parents
20/11/18 17:49:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 917.0 KB, free 902.4 MB)
20/11/18 17:49:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 261.8 KB, free 902.1 MB)
20/11/18 17:49:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 127.0.0.1:63854 (size: 261.8 KB, free: 903.9 MB)
20/11/18 17:49:15 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:15 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[29] at collect at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:15 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
20/11/18 17:49:15 WARN TaskSetManager: Stage 5 contains a task of very large size (17748 KB). The maximum recommended task size is 100 KB.
20/11/18 17:49:15 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, executor driver, partition 0, PROCESS_LOCAL, 18174057 bytes)
20/11/18 17:49:15 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
20/11/18 17:49:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 127.0.0.1:63854 in memory (size: 3.8 KB, free: 903.9 MB)
20/11/18 17:49:15 INFO ContextCleaner: Cleaned accumulator 160
20/11/18 17:49:16 INFO BlockManager: Found block rdd_16_0 locally
20/11/18 17:49:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1781 bytes result sent to driver
20/11/18 17:49:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 393 ms on localhost (executor driver) (1/1)
20/11/18 17:49:16 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:114) finished in 0,465 s
20/11/18 17:49:16 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:49:16 INFO DAGScheduler: running: Set()
20/11/18 17:49:16 INFO DAGScheduler: waiting: Set(ResultStage 6)
20/11/18 17:49:16 INFO DAGScheduler: failed: Set()
20/11/18 17:49:16 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[32] at collect at utils.scala:114), which has no missing parents
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 7.1 KB, free 902.1 MB)
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 3.8 KB, free 902.1 MB)
20/11/18 17:49:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
20/11/18 17:49:16 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 127.0.0.1:63854 (size: 3.8 KB, free: 903.9 MB)
20/11/18 17:49:16 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[32] at collect at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:16 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
20/11/18 17:49:16 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:49:16 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
20/11/18 17:49:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:49:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/11/18 17:49:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 1782 bytes result sent to driver
20/11/18 17:49:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 15 ms on localhost (executor driver) (1/1)
20/11/18 17:49:16 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:114) finished in 0,025 s
20/11/18 17:49:16 INFO DAGScheduler: Job 3 finished: collect at utils.scala:114, took 0,497384 s
20/11/18 17:49:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
20/11/18 17:49:16 INFO CodeGenerator: Code generated in 14.8511 ms
20/11/18 17:49:16 INFO CodeGenerator: Code generated in 8.4407 ms
20/11/18 17:49:16 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:49:16 INFO DAGScheduler: Registering RDD 37 (count at utils.scala:114)
20/11/18 17:49:16 INFO DAGScheduler: Got job 4 (count at utils.scala:114) with 1 output partitions
20/11/18 17:49:16 INFO DAGScheduler: Final stage: ResultStage 8 (count at utils.scala:114)
20/11/18 17:49:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)
20/11/18 17:49:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 7)
20/11/18 17:49:16 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[37] at count at utils.scala:114), which has no missing parents
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 916.4 KB, free 901.2 MB)
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 261.8 KB, free 900.9 MB)
20/11/18 17:49:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 127.0.0.1:63854 (size: 261.8 KB, free: 903.6 MB)
20/11/18 17:49:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[37] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
20/11/18 17:49:16 WARN TaskSetManager: Stage 7 contains a task of very large size (17748 KB). The maximum recommended task size is 100 KB.
20/11/18 17:49:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, executor driver, partition 0, PROCESS_LOCAL, 18174057 bytes)
20/11/18 17:49:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
20/11/18 17:49:16 INFO BlockManager: Found block rdd_16_0 locally
20/11/18 17:49:16 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 1781 bytes result sent to driver
20/11/18 17:49:16 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 204 ms on localhost (executor driver) (1/1)
20/11/18 17:49:16 INFO DAGScheduler: ShuffleMapStage 7 (count at utils.scala:114) finished in 0,264 s
20/11/18 17:49:16 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:49:16 INFO DAGScheduler: running: Set()
20/11/18 17:49:16 INFO DAGScheduler: waiting: Set(ResultStage 8)
20/11/18 17:49:16 INFO DAGScheduler: failed: Set()
20/11/18 17:49:16 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[40] at count at utils.scala:114), which has no missing parents
20/11/18 17:49:16 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 9.1 KB, free 900.9 MB)
20/11/18 17:49:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 4.1 KB, free 900.9 MB)
20/11/18 17:49:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 127.0.0.1:63854 (size: 4.1 KB, free: 903.6 MB)
20/11/18 17:49:16 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[40] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:16 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
20/11/18 17:49:16 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:49:16 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
20/11/18 17:49:16 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:49:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 1 ms
20/11/18 17:49:16 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 2055 bytes result sent to driver
20/11/18 17:49:16 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 21 ms on localhost (executor driver) (1/1)
20/11/18 17:49:16 INFO DAGScheduler: ResultStage 8 (count at utils.scala:114) finished in 0,031 s
20/11/18 17:49:16 INFO DAGScheduler: Job 4 finished: count at utils.scala:114, took 0,308336 s
20/11/18 17:49:16 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
20/11/18 17:49:17 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:49:17 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:49:17 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:49:17 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:49:17 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:49:17 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:49:17 INFO CodeGenerator: Code generated in 7.3024 ms
20/11/18 17:49:17 INFO CodeGenerator: Code generated in 5.3602 ms
20/11/18 17:49:17 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:49:17 INFO DAGScheduler: Registering RDD 44 (count at utils.scala:114)
20/11/18 17:49:17 INFO DAGScheduler: Got job 5 (count at utils.scala:114) with 1 output partitions
20/11/18 17:49:17 INFO DAGScheduler: Final stage: ResultStage 10 (count at utils.scala:114)
20/11/18 17:49:17 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
20/11/18 17:49:17 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
20/11/18 17:49:17 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[44] at count at utils.scala:114), which has no missing parents
20/11/18 17:49:17 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 7.9 KB, free 900.9 MB)
20/11/18 17:49:17 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.2 KB, free 900.9 MB)
20/11/18 17:49:17 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 127.0.0.1:63854 (size: 4.2 KB, free: 903.6 MB)
20/11/18 17:49:17 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:17 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[44] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:17 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
20/11/18 17:49:17 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, executor driver, partition 0, PROCESS_LOCAL, 8017 bytes)
20/11/18 17:49:17 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
20/11/18 17:49:17 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 1456 bytes result sent to driver
20/11/18 17:49:17 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 18 ms on localhost (executor driver) (1/1)
20/11/18 17:49:17 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
20/11/18 17:49:17 INFO DAGScheduler: ShuffleMapStage 9 (count at utils.scala:114) finished in 0,029 s
20/11/18 17:49:17 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:49:17 INFO DAGScheduler: running: Set()
20/11/18 17:49:17 INFO DAGScheduler: waiting: Set(ResultStage 10)
20/11/18 17:49:17 INFO DAGScheduler: failed: Set()
20/11/18 17:49:17 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[47] at count at utils.scala:114), which has no missing parents
20/11/18 17:49:17 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 7.1 KB, free 900.9 MB)
20/11/18 17:49:17 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.8 KB, free 900.9 MB)
20/11/18 17:49:17 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 127.0.0.1:63854 (size: 3.8 KB, free: 903.6 MB)
20/11/18 17:49:17 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1161
20/11/18 17:49:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[47] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:49:17 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
20/11/18 17:49:17 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:49:17 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
20/11/18 17:49:17 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:49:17 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
20/11/18 17:49:17 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 1696 bytes result sent to driver
20/11/18 17:49:17 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 9 ms on localhost (executor driver) (1/1)
20/11/18 17:49:17 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
20/11/18 17:49:17 INFO DAGScheduler: ResultStage 10 (count at utils.scala:114) finished in 0,017 s
20/11/18 17:49:17 INFO DAGScheduler: Job 5 finished: count at utils.scala:114, took 0,051348 s
20/11/18 17:52:17 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:52:17 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:52:17 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:52:17 INFO MemoryStore: MemoryStore cleared
20/11/18 17:52:17 INFO BlockManager: BlockManager stopped
20/11/18 17:52:17 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:52:17 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:52:17 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:52:17 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:52:17 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:52:17 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-7575b26f-181d-4f6e-b95f-b877767364e7
20/11/18 17:52:17 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927
20/11/18 17:52:17 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:52:17 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a
20/11/18 17:52:17 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-d11511b9-a79d-4b1c-897f-58c6e9f41927\userFiles-2036ab74-5f0b-48ce-a576-1001a765c78a\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:52:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/11/18 17:52:32 INFO SparkContext: Running Spark version 2.4.3
20/11/18 17:52:32 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
20/11/18 17:52:32 INFO SparkContext: Submitted application: sparklyr
20/11/18 17:52:32 INFO SecurityManager: Changing view acls to: ALVARO
20/11/18 17:52:32 INFO SecurityManager: Changing modify acls to: ALVARO
20/11/18 17:52:32 INFO SecurityManager: Changing view acls groups to: 
20/11/18 17:52:32 INFO SecurityManager: Changing modify acls groups to: 
20/11/18 17:52:33 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ALVARO); groups with view permissions: Set(); users  with modify permissions: Set(ALVARO); groups with modify permissions: Set()
20/11/18 17:52:33 INFO Utils: Successfully started service 'sparkDriver' on port 64081.
20/11/18 17:52:33 INFO SparkEnv: Registering MapOutputTracker
20/11/18 17:52:33 INFO SparkEnv: Registering BlockManagerMaster
20/11/18 17:52:33 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
20/11/18 17:52:33 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
20/11/18 17:52:33 INFO DiskBlockManager: Created local directory at C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\blockmgr-64f1d2ca-7db1-494a-b499-4bd5901a6e9b
20/11/18 17:52:33 INFO MemoryStore: MemoryStore started with capacity 912.3 MB
20/11/18 17:52:33 INFO SparkEnv: Registering OutputCommitCoordinator
20/11/18 17:52:33 WARN Utils: The configured local directories are not expected to be URIs; however, got suspicious values [C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local]. Please check your configured local directories.
20/11/18 17:52:33 INFO Utils: Successfully started service 'SparkUI' on port 4040.
20/11/18 17:52:33 INFO SparkUI: Bound SparkUI to 127.0.0.1, and started at http://127.0.0.1:4040
20/11/18 17:52:33 INFO SparkContext: Added JAR file:/C:/Users/ALVARO/Documents/R/win-library/4.0/sparklyr/java/sparklyr-2.4-2.11.jar at spark://127.0.0.1:64081/jars/sparklyr-2.4-2.11.jar with timestamp 1605736353948
20/11/18 17:52:34 INFO Executor: Starting executor ID driver on host localhost
20/11/18 17:52:34 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 64102.
20/11/18 17:52:34 INFO NettyBlockTransferService: Server created on 127.0.0.1:64102
20/11/18 17:52:34 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
20/11/18 17:52:34 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 127.0.0.1, 64102, None)
20/11/18 17:52:34 INFO BlockManagerMasterEndpoint: Registering block manager 127.0.0.1:64102 with 912.3 MB RAM, BlockManagerId(driver, 127.0.0.1, 64102, None)
20/11/18 17:52:34 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 127.0.0.1, 64102, None)
20/11/18 17:52:34 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 127.0.0.1, 64102, None)
20/11/18 17:52:34 INFO SharedState: loading hive config file: file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/conf/hive-site.xml
20/11/18 17:52:34 INFO SharedState: Setting hive.metastore.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive') to the value of spark.sql.warehouse.dir ('C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive').
20/11/18 17:52:34 INFO SharedState: Warehouse path is 'C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive'.
20/11/18 17:52:36 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint
20/11/18 17:52:40 INFO HiveUtils: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
20/11/18 17:52:42 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
20/11/18 17:52:42 INFO ObjectStore: ObjectStore, initialize called
20/11/18 17:52:42 INFO Persistence: Propiedad hive.metastore.integral.jdo.pushdown desconocida - vamos a ignorarla
20/11/18 17:52:42 INFO Persistence: Propiedad datanucleus.cache.level2 desconocida - vamos a ignorarla
20/11/18 17:52:44 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
20/11/18 17:52:51 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:52:51 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:52:52 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MFieldSchema" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:52:52 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MOrder" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:52:52 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
20/11/18 17:52:52 INFO ObjectStore: Initialized ObjectStore
20/11/18 17:52:53 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/11/18 17:52:53 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/11/18 17:52:53 INFO HiveMetaStore: Added admin role in metastore
20/11/18 17:52:53 INFO HiveMetaStore: Added public role in metastore
20/11/18 17:52:53 INFO HiveMetaStore: No user is added in admin role, since config is empty
20/11/18 17:52:53 INFO HiveMetaStore: 0: get_all_databases
20/11/18 17:52:53 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_all_databases	
20/11/18 17:52:53 INFO HiveMetaStore: 0: get_functions: db=default pat=*
20/11/18 17:52:53 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
20/11/18 17:52:53 INFO Datastore: La clase "org.apache.hadoop.hive.metastore.model.MResourceUri" es "embedded-only" asi que no tiene su propia tabla.
20/11/18 17:52:54 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/Temp/4ade0746-8939-401e-b267-96cbe4fac309_resources
20/11/18 17:52:54 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/4ade0746-8939-401e-b267-96cbe4fac309
20/11/18 17:52:54 INFO SessionState: Created local directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/4ade0746-8939-401e-b267-96cbe4fac309
20/11/18 17:52:54 INFO SessionState: Created HDFS directory: C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive/ALVARO/4ade0746-8939-401e-b267-96cbe4fac309/_tmp_space.db
20/11/18 17:52:54 INFO HiveClientImpl: Warehouse location for Hive client (version 1.2.2) is C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/hive
20/11/18 17:52:54 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:52:54 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:52:54 INFO HiveMetaStore: 0: get_database: global_temp
20/11/18 17:52:54 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: global_temp	
20/11/18 17:52:54 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
20/11/18 17:52:54 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:52:54 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:52:54 INFO HiveMetaStore: 0: get_database: default
20/11/18 17:52:54 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_database: default	
20/11/18 17:52:54 INFO HiveMetaStore: 0: get_tables: db=default pat=*
20/11/18 17:52:54 INFO audit: ugi=ALVARO	ip=unknown-ip-addr	cmd=get_tables: db=default pat=*	
20/11/18 17:52:55 INFO CodeGenerator: Code generated in 470.4441 ms
20/11/18 17:52:56 INFO ContextCleaner: Cleaned accumulator 1
20/11/18 17:52:56 INFO CodeGenerator: Code generated in 35.0294 ms
20/11/18 17:52:56 INFO CodeGenerator: Code generated in 19.3026 ms
20/11/18 17:52:57 INFO SparkContext: Starting job: count at utils.scala:114
20/11/18 17:52:57 INFO DAGScheduler: Registering RDD 3 (count at utils.scala:114)
20/11/18 17:52:57 INFO DAGScheduler: Got job 0 (count at utils.scala:114) with 1 output partitions
20/11/18 17:52:57 INFO DAGScheduler: Final stage: ResultStage 1 (count at utils.scala:114)
20/11/18 17:52:57 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 0)
20/11/18 17:52:57 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 0)
20/11/18 17:52:57 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114), which has no missing parents
20/11/18 17:52:57 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 7.9 KB, free 912.3 MB)
20/11/18 17:52:57 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 4.2 KB, free 912.3 MB)
20/11/18 17:52:57 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 127.0.0.1:64102 (size: 4.2 KB, free: 912.3 MB)
20/11/18 17:52:57 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1161
20/11/18 17:52:57 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[3] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:52:57 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
20/11/18 17:52:57 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, PROCESS_LOCAL, 7882 bytes)
20/11/18 17:52:57 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
20/11/18 17:52:57 INFO Executor: Fetching spark://127.0.0.1:64081/jars/sparklyr-2.4-2.11.jar with timestamp 1605736353948
20/11/18 17:52:57 INFO TransportClientFactory: Successfully created connection to /127.0.0.1:64081 after 72 ms (0 ms spent in bootstraps)
20/11/18 17:52:58 INFO Utils: Fetching spark://127.0.0.1:64081/jars/sparklyr-2.4-2.11.jar to C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d\fetchFileTemp2846747065749892534.tmp
20/11/18 17:52:58 INFO Executor: Adding file:/C:/Users/ALVARO/AppData/Local/spark/spark-2.4.3-bin-hadoop2.7/tmp/local/spark-00d411c5-06eb-48f5-a527-110c94d617d8/userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d/sparklyr-2.4-2.11.jar to class loader
20/11/18 17:52:58 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1542 bytes result sent to driver
20/11/18 17:52:58 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 990 ms on localhost (executor driver) (1/1)
20/11/18 17:52:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
20/11/18 17:52:58 INFO DAGScheduler: ShuffleMapStage 0 (count at utils.scala:114) finished in 1,561 s
20/11/18 17:52:58 INFO DAGScheduler: looking for newly runnable stages
20/11/18 17:52:58 INFO DAGScheduler: running: Set()
20/11/18 17:52:58 INFO DAGScheduler: waiting: Set(ResultStage 1)
20/11/18 17:52:58 INFO DAGScheduler: failed: Set()
20/11/18 17:52:58 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114), which has no missing parents
20/11/18 17:52:58 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.1 KB, free 912.3 MB)
20/11/18 17:52:58 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 3.8 KB, free 912.3 MB)
20/11/18 17:52:58 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 127.0.0.1:64102 (size: 3.8 KB, free: 912.3 MB)
20/11/18 17:52:58 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1161
20/11/18 17:52:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at count at utils.scala:114) (first 15 tasks are for partitions Vector(0))
20/11/18 17:52:58 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
20/11/18 17:52:58 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, executor driver, partition 0, ANY, 7767 bytes)
20/11/18 17:52:58 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
20/11/18 17:52:58 INFO ShuffleBlockFetcherIterator: Getting 1 non-empty blocks including 1 local blocks and 0 remote blocks
20/11/18 17:52:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 21 ms
20/11/18 17:52:58 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1775 bytes result sent to driver
20/11/18 17:52:58 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 129 ms on localhost (executor driver) (1/1)
20/11/18 17:52:58 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
20/11/18 17:52:58 INFO DAGScheduler: ResultStage 1 (count at utils.scala:114) finished in 0,154 s
20/11/18 17:52:58 INFO DAGScheduler: Job 0 finished: count at utils.scala:114, took 1,864335 s
20/11/18 17:52:59 INFO SparkContext: Invoking stop() from shutdown hook
20/11/18 17:52:59 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
20/11/18 17:52:59 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
20/11/18 17:52:59 INFO MemoryStore: MemoryStore cleared
20/11/18 17:52:59 INFO BlockManager: BlockManager stopped
20/11/18 17:52:59 INFO BlockManagerMaster: BlockManagerMaster stopped
20/11/18 17:52:59 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
20/11/18 17:52:59 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:103)
	at org.apache.spark.SparkContext$$anonfun$stop$11.apply$mcV$sp(SparkContext.scala:1974)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1340)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1973)
	at org.apache.spark.SparkContext$$anonfun$2.apply$mcV$sp(SparkContext.scala:575)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:52:59 INFO SparkContext: Successfully stopped SparkContext
20/11/18 17:52:59 INFO ShutdownHookManager: Shutdown hook called
20/11/18 17:52:59 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\Temp\spark-9aababd9-88ed-447b-8d58-ee0ae5ef0e1a
20/11/18 17:52:59 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8
20/11/18 17:52:59 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
20/11/18 17:52:59 INFO ShutdownHookManager: Deleting directory C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d
20/11/18 17:52:59 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d
java.io.IOException: Failed to delete: C:\Users\ALVARO\AppData\Local\spark\spark-2.4.3-bin-hadoop2.7\tmp\local\spark-00d411c5-06eb-48f5-a527-110c94d617d8\userFiles-1a01caf0-4ecb-4f9a-856d-5ce97173224d\sparklyr-2.4-2.11.jar
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:144)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursivelyUsingJavaIO(JavaUtils.java:128)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:118)
	at org.apache.spark.network.util.JavaUtils.deleteRecursively(JavaUtils.java:91)
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:1062)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:216)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1945)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:188)
	at scala.util.Try$.apply(Try.scala:192)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
